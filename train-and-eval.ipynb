{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Library imports and data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camer\\PycharmProjects\\CS598-DL4H-Final-Project\\venv\\lib\\site-packages\\pyhealth\\trainer.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    },
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x158cbff25b0>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyhealth.data import Visit\n",
    "from pyhealth.datasets import split_by_patient, get_dataloader\n",
    "from pyhealth.trainer import Trainer\n",
    "from pyhealth.medcode import InnerMap\n",
    "from pyhealth.metrics.binary import binary_metrics_fn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from typing import List, Dict\n",
    "from model.bitenet import BiteNet\n",
    "from model.baseline import RNN, BRNN, RETAIN, Deepr\n",
    "import pickle\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "BATCH_SIZE = 32\n",
    "KS = list(range(5, 31, 5))\n",
    "SEQ_LENS = list(range(6, 17, 2))\n",
    "N_TRIALS=10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(RANDOM_SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-08T17:24:43.014186Z",
     "end_time": "2023-05-08T17:24:44.804184Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with open(\"mimic3_dataset.pkl\", \"rb\") as dataset_file:\n",
    "    mimic3_ds = pickle.load(dataset_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-08T17:24:44.805183Z",
     "end_time": "2023-05-08T17:24:46.258185Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fgrnO7KkWDBY",
    "ExecuteTime": {
     "start_time": "2023-05-08T17:24:46.264184Z",
     "end_time": "2023-05-08T17:24:46.304182Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the tasks\n",
    "\n",
    "DIAGNOSES_KEY = \"conditions\"\n",
    "PROCEDURES_KEY = \"procedures\"\n",
    "INTERVAL_DAYS_KEY = \"days_since_first_visit\"\n",
    "\n",
    "icd9cm = InnerMap.load(\"ICD9CM\")\n",
    "\n",
    "def flatten(l: List):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def patient_level_readmission_prediction(patient, time_window: int = 30, max_length_visits: int = None):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    sorted_visits = sorted(patient, key=lambda visit: visit.encounter_time)\n",
    "\n",
    "    # Clip the patient visits to the most recent max_length_visits + 1 if max_length_visits is not None\n",
    "    if max_length_visits is not None:\n",
    "        n_visits = len(sorted_visits)\n",
    "        if n_visits > max_length_visits + 1:\n",
    "            sorted_visits = sorted_visits[n_visits - (max_length_visits + 1):]\n",
    "\n",
    "    feature_visits: List[Visit] = sorted_visits[:-1]\n",
    "    last_visit: Visit = sorted_visits[-1]\n",
    "    second_to_last_visit: Visit = feature_visits[-1]\n",
    "    first_visit: Visit = feature_visits[0]\n",
    "\n",
    "    # step 1 a: define readmission label\n",
    "    time_diff = (last_visit.encounter_time - second_to_last_visit.encounter_time).days\n",
    "    readmission_label = 1 if time_diff <= time_window else 0\n",
    "\n",
    "    # step 1 b: define diagnosis prediction label\n",
    "    diagnosis_label = list(set([icd9cm.get_ancestors(code)[1] for code in last_visit.get_code_list(\"DIAGNOSES_ICD\")]))\n",
    "\n",
    "    # step 2: obtain features\n",
    "    visits_diagnoses = []\n",
    "    visits_procedures = []\n",
    "    visits_intervals = []\n",
    "    for idx, visit in enumerate(feature_visits):\n",
    "        diagnoses = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        time_diff_from_first_visit = (visit.encounter_time - first_visit.encounter_time).days\n",
    "\n",
    "        # Exclude visits that are missing either diagnoses or procedures.\n",
    "        # BiteNet can handle missing procedures, but other PyHealth models like RNN\n",
    "        # require all features have a length greater than 0.\n",
    "        if len(diagnoses) == 0:\n",
    "            continue\n",
    "\n",
    "        visits_diagnoses.append(diagnoses)\n",
    "        visits_procedures.append(procedures)\n",
    "        visits_intervals.append([str(time_diff_from_first_visit)])\n",
    "\n",
    "    unique_diagnoses = list(set(flatten(visits_diagnoses)))\n",
    "\n",
    "    # step 3: exclusion criteria\n",
    "    if len(unique_diagnoses) == 0:\n",
    "        return []\n",
    "\n",
    "    # step 4: assemble the sample\n",
    "    samples.append(\n",
    "        {\n",
    "            \"patient_id\": patient.patient_id,\n",
    "            \"visit_id\": visit.visit_id,\n",
    "            \"diagnoses\": visits_diagnoses,\n",
    "            \"procedures\": visits_procedures,\n",
    "            \"intervals\": visits_intervals,\n",
    "            \"readmission_label\": readmission_label,\n",
    "            \"diagnosis_label\": diagnosis_label\n",
    "        }\n",
    "    )\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "RESULTS_FILE = \"./results/baseline_comparison.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-08T17:24:46.305184Z",
     "end_time": "2023-05-08T17:24:46.320179Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def train_and_inference(model, train_loader, val_loader, test_loader, lr=0.0005, monitor=\"pr_auc\", optim = torch.optim.Adam):\n",
    "    trainer = Trainer(model=model, device=device)\n",
    "    trainer.train(\n",
    "        train_dataloader=train_loader,\n",
    "        val_dataloader=val_loader,\n",
    "        epochs=10,\n",
    "        monitor=monitor,\n",
    "        optimizer_class=optim,\n",
    "        optimizer_params = {\"lr\" : lr},\n",
    "        load_best_model_at_last=False\n",
    "    )\n",
    "\n",
    "    return trainer.inference(test_loader)\n",
    "\n",
    "def precision_at_k(y_true: np.ndarray, y_prob: np.ndarray):\n",
    "\n",
    "    y_pred: np.ndarray = (y_prob > 0.5).astype(int)\n",
    "    desc_idx: np.ndarray = np.flip(np.argsort(y_prob, axis=-1), axis=-1)\n",
    "\n",
    "    y_true = np.take_along_axis(y_true, desc_idx, axis=-1).astype(int)\n",
    "    y_pred = np.take_along_axis(y_pred, desc_idx, axis=-1)\n",
    "\n",
    "    num_cat_labels = y_true.sum(axis=-1)\n",
    "\n",
    "    precisions: List[float] = []\n",
    "    for k in KS:\n",
    "        y_true_k = y_true[:, :k]\n",
    "        y_pred_k = y_pred[:, :k]\n",
    "\n",
    "        num_correct_preds_k = np.logical_and(y_true_k, y_pred_k).astype(int).sum(axis=-1)\n",
    "        ks = np.repeat(k, num_correct_preds_k.shape[0])\n",
    "        denominator = np.minimum(num_cat_labels, ks).astype(float)\n",
    "        precision_k = num_correct_preds_k.astype(float) / denominator\n",
    "        precisions.append(precision_k.mean())\n",
    "\n",
    "    precisions: Dict[str, float] = {\n",
    "        f\"precision@{k}\": p for k, p in zip(KS, precisions)\n",
    "    }\n",
    "    return precisions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-08T17:24:46.325181Z",
     "end_time": "2023-05-08T17:24:46.335362Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-08T17:24:46.338362Z",
     "end_time": "2023-05-08T17:24:46.351185Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_record_metrics(model_readm, model_diag, df, row_fields, train_loader, val_loader, test_loader, lr=0.0005, readm_optim=torch.optim.Adam, diag_optim=torch.optim.Adam):\n",
    "    y_true, y_prob, _ = train_and_inference(\n",
    "        model_readm,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        test_loader,\n",
    "        lr=lr,\n",
    "        optim=readm_optim\n",
    "    )\n",
    "    binary_metrics = binary_metrics_fn(y_true, y_prob, metrics=[\"pr_auc\"])\n",
    "\n",
    "    y_true, y_prob, _ = train_and_inference(\n",
    "        model_diag,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        test_loader,\n",
    "        lr=lr,\n",
    "        monitor=\"pr_auc_samples\",\n",
    "        optim=diag_optim\n",
    "    )\n",
    "    precisions = precision_at_k(y_true, y_prob)\n",
    "\n",
    "    row = binary_metrics | precisions | row_fields\n",
    "    row = {\n",
    "        k: [v] for k, v in row.items()\n",
    "    }\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame.from_dict(row)], ignore_index=True)\n",
    "\n",
    "    # Save df for checkpoint\n",
    "    df.to_csv(RESULTS_FILE, index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# %%capture\n",
    "#\n",
    "# # Compare BiteNet performance to baselines\n",
    "#\n",
    "# metrics_df = pd.DataFrame(columns=['model_name', 'feature_set', 'seq_len', 'trial', 'pr_auc'] + [f\"precision@{k}\" for k in KS])\n",
    "#\n",
    "# for seq_len in SEQ_LENS:\n",
    "#\n",
    "#     dataset = mimic3_ds.set_task(\n",
    "#         task_fn=lambda p: patient_level_readmission_prediction(p, max_length_visits=seq_len)\n",
    "#     )\n",
    "#\n",
    "#     for trial in range(1, N_TRIALS+1):\n",
    "#\n",
    "#         train, val, test = split_by_patient(dataset, [0.8, 0.1, 0.1])\n",
    "#\n",
    "#         train_loader = get_dataloader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "#         val_loader = get_dataloader(val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "#         test_loader = get_dataloader(test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "#\n",
    "#         #################### BITENET ####################\n",
    "#         metrics_df = train_and_record_metrics(\n",
    "#             model_readm=BiteNet(\n",
    "#                 dataset = dataset,\n",
    "#                 feature_keys = [\"diagnoses\", \"procedures\", \"intervals\"],\n",
    "#                 label_key = \"readmission_label\",\n",
    "#                 mode = \"binary\",\n",
    "#             ).to(device),\n",
    "#             model_diag=BiteNet(\n",
    "#                 dataset = dataset,\n",
    "#                 feature_keys = [\"diagnoses\", \"procedures\", \"intervals\"],\n",
    "#                 label_key = \"diagnosis_label\",\n",
    "#                 mode = \"multilabel\",\n",
    "#             ).to(device),\n",
    "#             df=metrics_df,\n",
    "#             row_fields={\n",
    "#                 \"model_name\": \"bitenet\",\n",
    "#                 \"feature_set\": \"dxtx\",\n",
    "#                 \"seq_len\": seq_len,\n",
    "#                 \"trial\": trial\n",
    "#             },\n",
    "#             train_loader=train_loader,\n",
    "#             val_loader=val_loader,\n",
    "#             test_loader=test_loader,\n",
    "#         )\n",
    "#\n",
    "#         metrics_df = train_and_record_metrics(\n",
    "#             model_readm=BiteNet(\n",
    "#                 dataset = dataset,\n",
    "#                 feature_keys = [\"diagnoses\", \"intervals\"],\n",
    "#                 label_key = \"readmission_label\",\n",
    "#                 mode = \"binary\",\n",
    "#             ).to(device),\n",
    "#             model_diag=BiteNet(\n",
    "#                 dataset = dataset,\n",
    "#                 feature_keys = [\"diagnoses\", \"intervals\"],\n",
    "#                 label_key = \"diagnosis_label\",\n",
    "#                 mode = \"multilabel\",\n",
    "#             ).to(device),\n",
    "#             df=metrics_df,\n",
    "#             row_fields={\n",
    "#                 \"model_name\": \"bitenet\",\n",
    "#                 \"feature_set\": \"dx\",\n",
    "#                 \"seq_len\": seq_len,\n",
    "#                 \"trial\": trial\n",
    "#             },\n",
    "#             train_loader=train_loader,\n",
    "#             val_loader=val_loader,\n",
    "#             test_loader=test_loader\n",
    "#         )\n",
    "#\n",
    "#         #################### RNN ####################\n",
    "#         metrics_df = train_and_record_metrics(\n",
    "#                 model_readm=RNN(\n",
    "#                 dataset = dataset,\n",
    "#                 feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "#                 label_key = \"readmission_label\",\n",
    "#                 mode = \"binary\",\n",
    "#             ).to(device),\n",
    "#             model_diag=RNN(\n",
    "#                 dataset = dataset,\n",
    "#                 feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "#                 label_key = \"diagnosis_label\",\n",
    "#                 mode = \"multilabel\",\n",
    "#             ).to(device),\n",
    "#             df=metrics_df,\n",
    "#             row_fields={\n",
    "#                 \"model_name\": \"rnn\",\n",
    "#                 \"feature_set\": \"dxtx\",\n",
    "#                 \"seq_len\": seq_len,\n",
    "#                 \"trial\": trial\n",
    "#             },\n",
    "#             train_loader=train_loader,\n",
    "#             val_loader=val_loader,\n",
    "#             test_loader=test_loader\n",
    "#         )\n",
    "#\n",
    "#         metrics_df = train_and_record_metrics(\n",
    "#                 model_readm=RNN(\n",
    "#                 dataset = dataset,\n",
    "#                 feature_keys = [\"diagnoses\"],\n",
    "#                 label_key = \"readmission_label\",\n",
    "#                 mode = \"binary\",\n",
    "#             ).to(device),\n",
    "#             model_diag=RNN(\n",
    "#                 dataset = dataset,\n",
    "#                 feature_keys = [\"diagnoses\"],\n",
    "#                 label_key = \"diagnosis_label\",\n",
    "#                 mode = \"multilabel\",\n",
    "#             ).to(device),\n",
    "#             df=metrics_df,\n",
    "#             row_fields={\n",
    "#                 \"model_name\": \"rnn\",\n",
    "#                 \"feature_set\": \"dx\",\n",
    "#                 \"seq_len\": seq_len,\n",
    "#                 \"trial\": trial\n",
    "#             },\n",
    "#             train_loader=train_loader,\n",
    "#             val_loader=val_loader,\n",
    "#             test_loader=test_loader,\n",
    "#         )\n",
    "#\n",
    "#         #################### BRNN ####################\n",
    "#         metrics_df = train_and_record_metrics(\n",
    "#                 model_readm=BRNN(\n",
    "#                 dataset = dataset,\n",
    "#                 feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "#                 label_key = \"readmission_label\",\n",
    "#                 mode = \"binary\",\n",
    "#                 bidirectional=True\n",
    "#             ).to(device),\n",
    "#             model_diag=RNN(\n",
    "#                 dataset = dataset,\n",
    "#                 feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "#                 label_key = \"diagnosis_label\",\n",
    "#                 mode = \"multilabel\",\n",
    "#                 bidirectional=True\n",
    "#             ).to(device),\n",
    "#             df=metrics_df,\n",
    "#             row_fields={\n",
    "#                 \"model_name\": \"brnn\",\n",
    "#                 \"feature_set\": \"dxtx\",\n",
    "#                 \"seq_len\": seq_len,\n",
    "#                 \"trial\": trial\n",
    "#             },\n",
    "#             train_loader=train_loader,\n",
    "#             val_loader=val_loader,\n",
    "#             test_loader=test_loader\n",
    "#         )\n",
    "#\n",
    "#         metrics_df = train_and_record_metrics(\n",
    "#                 model_readm=BRNN(\n",
    "#                 dataset = dataset,\n",
    "#                 feature_keys = [\"diagnoses\"],\n",
    "#                 label_key = \"readmission_label\",\n",
    "#                 mode = \"binary\",\n",
    "#                 bidirectional=True\n",
    "#             ).to(device),\n",
    "#             model_diag=RNN(\n",
    "#                 dataset = dataset,\n",
    "#                 feature_keys = [\"diagnoses\"],\n",
    "#                 label_key = \"diagnosis_label\",\n",
    "#                 mode = \"multilabel\",\n",
    "#                 bidirectional=True\n",
    "#             ).to(device),\n",
    "#             df=metrics_df,\n",
    "#             row_fields={\n",
    "#                 \"model_name\": \"brnn\",\n",
    "#                 \"feature_set\": \"dx\",\n",
    "#                 \"seq_len\": seq_len,\n",
    "#                 \"trial\": trial\n",
    "#             },\n",
    "#             train_loader=train_loader,\n",
    "#             val_loader=val_loader,\n",
    "#             test_loader=test_loader\n",
    "#         )\n",
    "#\n",
    "#         #################### RETAIN ####################\n",
    "#         metrics_df = train_and_record_metrics(\n",
    "#                 model_readm=RETAIN(\n",
    "#                 dataset = dataset,\n",
    "#                 feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "#                 label_key = \"readmission_label\",\n",
    "#                 mode = \"binary\"\n",
    "#             ).to(device),\n",
    "#             model_diag=RETAIN(\n",
    "#                 dataset = dataset,\n",
    "#                 feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "#                 label_key = \"diagnosis_label\",\n",
    "#                 mode = \"multilabel\"\n",
    "#             ).to(device),\n",
    "#             df=metrics_df,\n",
    "#             row_fields={\n",
    "#                 \"model_name\": \"retain\",\n",
    "#                 \"feature_set\": \"dxtx\",\n",
    "#                 \"seq_len\": seq_len,\n",
    "#                 \"trial\": trial\n",
    "#             },\n",
    "#             train_loader=train_loader,\n",
    "#             val_loader=val_loader,\n",
    "#             test_loader=test_loader,\n",
    "#         )\n",
    "#\n",
    "#         metrics_df = train_and_record_metrics(\n",
    "#                 model_readm=RETAIN(\n",
    "#                 dataset = dataset,\n",
    "#                 feature_keys = [\"diagnoses\"],\n",
    "#                 label_key = \"readmission_label\",\n",
    "#                 mode = \"binary\"\n",
    "#             ).to(device),\n",
    "#             model_diag=RETAIN(\n",
    "#                 dataset = dataset,\n",
    "#                 feature_keys = [\"diagnoses\"],\n",
    "#                 label_key = \"diagnosis_label\",\n",
    "#                 mode = \"multilabel\"\n",
    "#             ).to(device),\n",
    "#             df=metrics_df,\n",
    "#             row_fields={\n",
    "#                 \"model_name\": \"retain\",\n",
    "#                 \"feature_set\": \"dx\",\n",
    "#                 \"seq_len\": seq_len,\n",
    "#                 \"trial\": trial\n",
    "#             },\n",
    "#             train_loader=train_loader,\n",
    "#             val_loader=val_loader,\n",
    "#             test_loader=test_loader\n",
    "#         )\n",
    "#\n",
    "#         #################### Deepr ####################\n",
    "#         metrics_df = train_and_record_metrics(\n",
    "#                 model_readm=Deepr(\n",
    "#                 dataset = dataset,\n",
    "#                 feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "#                 label_key = \"readmission_label\",\n",
    "#                 mode = \"binary\"\n",
    "#             ).to(device),\n",
    "#             model_diag=Deepr(\n",
    "#                 dataset = dataset,\n",
    "#                 feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "#                 label_key = \"diagnosis_label\",\n",
    "#                 mode = \"multilabel\"\n",
    "#             ).to(device),\n",
    "#             df=metrics_df,\n",
    "#             row_fields={\n",
    "#                 \"model_name\": \"deepr\",\n",
    "#                 \"feature_set\": \"dxtx\",\n",
    "#                 \"seq_len\": seq_len,\n",
    "#                 \"trial\": trial\n",
    "#             },\n",
    "#             train_loader=train_loader,\n",
    "#             val_loader=val_loader,\n",
    "#             test_loader=test_loader\n",
    "#         )\n",
    "#\n",
    "#         metrics_df = train_and_record_metrics(\n",
    "#                 model_readm=Deepr(\n",
    "#                 dataset = dataset,\n",
    "#                 feature_keys = [\"diagnoses\"],\n",
    "#                 label_key = \"readmission_label\",\n",
    "#                 mode = \"binary\"\n",
    "#             ).to(device),\n",
    "#             model_diag=Deepr(\n",
    "#                 dataset = dataset,\n",
    "#                 feature_keys = [\"diagnoses\"],\n",
    "#                 label_key = \"diagnosis_label\",\n",
    "#                 mode = \"multilabel\"\n",
    "#             ).to(device),\n",
    "#             df=metrics_df,\n",
    "#             row_fields={\n",
    "#                 \"model_name\": \"deepr\",\n",
    "#                 \"feature_set\": \"dx\",\n",
    "#                 \"seq_len\": seq_len,\n",
    "#                 \"trial\": trial\n",
    "#             },\n",
    "#             train_loader=train_loader,\n",
    "#             val_loader=val_loader,\n",
    "#             test_loader=test_loader\n",
    "#         )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-08T17:24:46.357185Z",
     "end_time": "2023-05-08T17:24:46.368184Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3432, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1359, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1726, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x00000158887AF4C0>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5258\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.3746\n",
      "roc_auc: 0.6196\n",
      "f1: 0.2143\n",
      "loss: 0.4642\n",
      "New best pr_auc score (0.3746) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.4738\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3754\n",
      "roc_auc: 0.6214\n",
      "f1: 0.2156\n",
      "loss: 0.4575\n",
      "New best pr_auc score (0.3754) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.4357\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3905\n",
      "roc_auc: 0.6366\n",
      "f1: 0.2567\n",
      "loss: 0.4730\n",
      "New best pr_auc score (0.3905) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.3964\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3797\n",
      "roc_auc: 0.6338\n",
      "f1: 0.2636\n",
      "loss: 0.4968\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.3385\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3894\n",
      "roc_auc: 0.6423\n",
      "f1: 0.2694\n",
      "loss: 0.5105\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.2615\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3797\n",
      "roc_auc: 0.6256\n",
      "f1: 0.3213\n",
      "loss: 0.6076\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.1816\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3934\n",
      "roc_auc: 0.6292\n",
      "f1: 0.2844\n",
      "loss: 0.7017\n",
      "New best pr_auc score (0.3934) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.1136\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3966\n",
      "roc_auc: 0.6419\n",
      "f1: 0.2623\n",
      "loss: 0.8340\n",
      "New best pr_auc score (0.3966) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0721\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3812\n",
      "roc_auc: 0.6249\n",
      "f1: 0.3321\n",
      "loss: 1.0469\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0474\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3808\n",
      "roc_auc: 0.6356\n",
      "f1: 0.2797\n",
      "loss: 1.1706\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3432, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1359, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1726, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x00000158887AF4C0>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.1492\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.3425\n",
      "loss: 0.0856\n",
      "New best pr_auc_samples score (0.3425) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.0849\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.3649\n",
      "loss: 0.0828\n",
      "New best pr_auc_samples score (0.3649) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0820\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.3995\n",
      "loss: 0.0807\n",
      "New best pr_auc_samples score (0.3995) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0798\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.4177\n",
      "loss: 0.0796\n",
      "New best pr_auc_samples score (0.4177) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0779\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.4294\n",
      "loss: 0.0787\n",
      "New best pr_auc_samples score (0.4294) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0760\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.4413\n",
      "loss: 0.0778\n",
      "New best pr_auc_samples score (0.4413) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0744\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.4460\n",
      "loss: 0.0775\n",
      "New best pr_auc_samples score (0.4460) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0729\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.4541\n",
      "loss: 0.0778\n",
      "New best pr_auc_samples score (0.4541) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0712\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.4563\n",
      "loss: 0.0772\n",
      "New best pr_auc_samples score (0.4563) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0698\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.4581\n",
      "loss: 0.0774\n",
      "New best pr_auc_samples score (0.4581) at epoch-9, step-1880\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3432, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1359, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1726, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x00000158887AF4C0>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5294\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.2030\n",
      "roc_auc: 0.5220\n",
      "f1: 0.0000\n",
      "loss: 0.5131\n",
      "New best pr_auc score (0.2030) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.5251\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.2240\n",
      "roc_auc: 0.5510\n",
      "f1: 0.0000\n",
      "loss: 0.4965\n",
      "New best pr_auc score (0.2240) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.4972\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3537\n",
      "roc_auc: 0.6177\n",
      "f1: 0.1939\n",
      "loss: 0.4604\n",
      "New best pr_auc score (0.3537) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.4719\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3526\n",
      "roc_auc: 0.5974\n",
      "f1: 0.2130\n",
      "loss: 0.4627\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.4466\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3730\n",
      "roc_auc: 0.6094\n",
      "f1: 0.2500\n",
      "loss: 0.4973\n",
      "New best pr_auc score (0.3730) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.4171\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3720\n",
      "roc_auc: 0.6023\n",
      "f1: 0.2088\n",
      "loss: 0.4883\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.3807\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3775\n",
      "roc_auc: 0.6100\n",
      "f1: 0.2654\n",
      "loss: 0.5626\n",
      "New best pr_auc score (0.3775) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.3323\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3679\n",
      "roc_auc: 0.6049\n",
      "f1: 0.2807\n",
      "loss: 0.5964\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.2887\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3734\n",
      "roc_auc: 0.6106\n",
      "f1: 0.2955\n",
      "loss: 0.6126\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.2554\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3563\n",
      "roc_auc: 0.6001\n",
      "f1: 0.2846\n",
      "loss: 0.7618\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3432, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1359, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1726, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x00000158887AF4C0>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.1698\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.3250\n",
      "loss: 0.1160\n",
      "New best pr_auc_samples score (0.3250) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.1069\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.3246\n",
      "loss: 0.0983\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0930\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.3360\n",
      "loss: 0.0887\n",
      "New best pr_auc_samples score (0.3360) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0870\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.3436\n",
      "loss: 0.0851\n",
      "New best pr_auc_samples score (0.3436) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0841\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.3607\n",
      "loss: 0.0830\n",
      "New best pr_auc_samples score (0.3607) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0823\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.3722\n",
      "loss: 0.0819\n",
      "New best pr_auc_samples score (0.3722) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0812\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.3785\n",
      "loss: 0.0809\n",
      "New best pr_auc_samples score (0.3785) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0801\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.3864\n",
      "loss: 0.0807\n",
      "New best pr_auc_samples score (0.3864) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0792\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.3931\n",
      "loss: 0.0801\n",
      "New best pr_auc_samples score (0.3931) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0785\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.3986\n",
      "loss: 0.0801\n",
      "New best pr_auc_samples score (0.3986) at epoch-9, step-1880\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3432, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1359, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x00000158887AF4C0>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5300\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.2208\n",
      "roc_auc: 0.5458\n",
      "f1: 0.0000\n",
      "loss: 0.5091\n",
      "New best pr_auc score (0.2208) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.5244\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.1915\n",
      "roc_auc: 0.4849\n",
      "f1: 0.0000\n",
      "loss: 0.5079\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.5238\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.1995\n",
      "roc_auc: 0.5103\n",
      "f1: 0.0000\n",
      "loss: 0.4969\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.5192\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.1971\n",
      "roc_auc: 0.4996\n",
      "f1: 0.0000\n",
      "loss: 0.4995\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.5015\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3333\n",
      "roc_auc: 0.5843\n",
      "f1: 0.1718\n",
      "loss: 0.4693\n",
      "New best pr_auc score (0.3333) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.4716\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3476\n",
      "roc_auc: 0.5953\n",
      "f1: 0.2011\n",
      "loss: 0.4718\n",
      "New best pr_auc score (0.3476) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.4487\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3658\n",
      "roc_auc: 0.6120\n",
      "f1: 0.2593\n",
      "loss: 0.4975\n",
      "New best pr_auc score (0.3658) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.4246\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3628\n",
      "roc_auc: 0.6050\n",
      "f1: 0.2198\n",
      "loss: 0.4761\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.3954\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3676\n",
      "roc_auc: 0.6074\n",
      "f1: 0.2587\n",
      "loss: 0.5182\n",
      "New best pr_auc score (0.3676) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.3639\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3766\n",
      "roc_auc: 0.6269\n",
      "f1: 0.2500\n",
      "loss: 0.5343\n",
      "New best pr_auc score (0.3766) at epoch-9, step-1880\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3432, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1359, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x00000158887AF4C0>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.1676\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.3185\n",
      "loss: 0.1153\n",
      "New best pr_auc_samples score (0.3185) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.1054\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.3323\n",
      "loss: 0.0971\n",
      "New best pr_auc_samples score (0.3323) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0920\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.3392\n",
      "loss: 0.0875\n",
      "New best pr_auc_samples score (0.3392) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0860\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.3612\n",
      "loss: 0.0838\n",
      "New best pr_auc_samples score (0.3612) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0832\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.3689\n",
      "loss: 0.0825\n",
      "New best pr_auc_samples score (0.3689) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0817\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.3759\n",
      "loss: 0.0814\n",
      "New best pr_auc_samples score (0.3759) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0807\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.3804\n",
      "loss: 0.0808\n",
      "New best pr_auc_samples score (0.3804) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0799\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.3824\n",
      "loss: 0.0806\n",
      "New best pr_auc_samples score (0.3824) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0794\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.3846\n",
      "loss: 0.0807\n",
      "New best pr_auc_samples score (0.3846) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0789\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.3918\n",
      "loss: 0.0803\n",
      "New best pr_auc_samples score (0.3918) at epoch-9, step-1880\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3432, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1359, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1726, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x00000158887AF4C0>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5270\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.1932\n",
      "roc_auc: 0.4894\n",
      "f1: 0.0000\n",
      "loss: 0.5027\n",
      "New best pr_auc score (0.1932) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.5229\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.2252\n",
      "roc_auc: 0.5690\n",
      "f1: 0.0000\n",
      "loss: 0.5021\n",
      "New best pr_auc score (0.2252) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.5070\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3494\n",
      "roc_auc: 0.6256\n",
      "f1: 0.1939\n",
      "loss: 0.4776\n",
      "New best pr_auc score (0.3494) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.4726\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3553\n",
      "roc_auc: 0.6158\n",
      "f1: 0.2151\n",
      "loss: 0.4815\n",
      "New best pr_auc score (0.3553) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.4492\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3662\n",
      "roc_auc: 0.6304\n",
      "f1: 0.2174\n",
      "loss: 0.4674\n",
      "New best pr_auc score (0.3662) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.4223\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3839\n",
      "roc_auc: 0.6507\n",
      "f1: 0.2727\n",
      "loss: 0.4936\n",
      "New best pr_auc score (0.3839) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.3877\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3865\n",
      "roc_auc: 0.6533\n",
      "f1: 0.2353\n",
      "loss: 0.5104\n",
      "New best pr_auc score (0.3865) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.3513\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3854\n",
      "roc_auc: 0.6599\n",
      "f1: 0.3091\n",
      "loss: 0.5336\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.3149\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3702\n",
      "roc_auc: 0.6490\n",
      "f1: 0.3158\n",
      "loss: 0.6211\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.2616\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3370\n",
      "roc_auc: 0.6320\n",
      "f1: 0.3429\n",
      "loss: 0.6772\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3432, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1359, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1726, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x00000158887AF4C0>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.1686\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.3281\n",
      "loss: 0.1149\n",
      "New best pr_auc_samples score (0.3281) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.1057\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.3285\n",
      "loss: 0.0975\n",
      "New best pr_auc_samples score (0.3285) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0924\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.3347\n",
      "loss: 0.0880\n",
      "New best pr_auc_samples score (0.3347) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0867\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.3431\n",
      "loss: 0.0847\n",
      "New best pr_auc_samples score (0.3431) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0839\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.3617\n",
      "loss: 0.0824\n",
      "New best pr_auc_samples score (0.3617) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0821\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.3723\n",
      "loss: 0.0814\n",
      "New best pr_auc_samples score (0.3723) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0809\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.3777\n",
      "loss: 0.0811\n",
      "New best pr_auc_samples score (0.3777) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0801\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.3774\n",
      "loss: 0.0815\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0793\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.3849\n",
      "loss: 0.0811\n",
      "New best pr_auc_samples score (0.3849) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0788\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.3897\n",
      "loss: 0.0811\n",
      "New best pr_auc_samples score (0.3897) at epoch-9, step-1880\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3432, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1726, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x00000158887AF4C0>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5182\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.3661\n",
      "roc_auc: 0.6101\n",
      "f1: 0.2036\n",
      "loss: 0.4635\n",
      "New best pr_auc score (0.3661) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.4704\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3773\n",
      "roc_auc: 0.6280\n",
      "f1: 0.2130\n",
      "loss: 0.4557\n",
      "New best pr_auc score (0.3773) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.4338\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3727\n",
      "roc_auc: 0.6121\n",
      "f1: 0.2222\n",
      "loss: 0.4694\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.3857\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3675\n",
      "roc_auc: 0.6286\n",
      "f1: 0.2424\n",
      "loss: 0.4907\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.3270\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3738\n",
      "roc_auc: 0.6194\n",
      "f1: 0.2478\n",
      "loss: 0.5405\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.2515\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3727\n",
      "roc_auc: 0.6030\n",
      "f1: 0.2857\n",
      "loss: 0.6465\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.1768\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3537\n",
      "roc_auc: 0.5890\n",
      "f1: 0.2597\n",
      "loss: 0.7955\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.1051\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3542\n",
      "roc_auc: 0.5928\n",
      "f1: 0.2892\n",
      "loss: 1.0290\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0649\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3579\n",
      "roc_auc: 0.5839\n",
      "f1: 0.2698\n",
      "loss: 1.2955\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0446\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3477\n",
      "roc_auc: 0.5762\n",
      "f1: 0.2797\n",
      "loss: 1.4641\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3432, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1726, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x00000158887AF4C0>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.1511\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.3440\n",
      "loss: 0.0853\n",
      "New best pr_auc_samples score (0.3440) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.0844\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.3747\n",
      "loss: 0.0816\n",
      "New best pr_auc_samples score (0.3747) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0818\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.3962\n",
      "loss: 0.0803\n",
      "New best pr_auc_samples score (0.3962) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0799\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.4172\n",
      "loss: 0.0793\n",
      "New best pr_auc_samples score (0.4172) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0778\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.4350\n",
      "loss: 0.0784\n",
      "New best pr_auc_samples score (0.4350) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0763\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.4394\n",
      "loss: 0.0778\n",
      "New best pr_auc_samples score (0.4394) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0744\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.4469\n",
      "loss: 0.0773\n",
      "New best pr_auc_samples score (0.4469) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0732\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.4537\n",
      "loss: 0.0771\n",
      "New best pr_auc_samples score (0.4537) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0716\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.4574\n",
      "loss: 0.0767\n",
      "New best pr_auc_samples score (0.4574) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0701\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.4606\n",
      "loss: 0.0768\n",
      "New best pr_auc_samples score (0.4606) at epoch-9, step-1880\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3432, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1726, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x00000158887AF4C0>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5291\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.1880\n",
      "roc_auc: 0.4665\n",
      "f1: 0.0000\n",
      "loss: 0.5063\n",
      "New best pr_auc score (0.1880) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.5244\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.2108\n",
      "roc_auc: 0.5323\n",
      "f1: 0.0000\n",
      "loss: 0.4968\n",
      "New best pr_auc score (0.2108) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.5190\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.2470\n",
      "roc_auc: 0.5895\n",
      "f1: 0.0000\n",
      "loss: 0.4897\n",
      "New best pr_auc score (0.2470) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.4900\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3684\n",
      "roc_auc: 0.6168\n",
      "f1: 0.1829\n",
      "loss: 0.4656\n",
      "New best pr_auc score (0.3684) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.4604\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3822\n",
      "roc_auc: 0.6366\n",
      "f1: 0.2260\n",
      "loss: 0.4657\n",
      "New best pr_auc score (0.3822) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.4208\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3753\n",
      "roc_auc: 0.6430\n",
      "f1: 0.2627\n",
      "loss: 0.5107\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.3735\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3946\n",
      "roc_auc: 0.6589\n",
      "f1: 0.2647\n",
      "loss: 0.5100\n",
      "New best pr_auc score (0.3946) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.3277\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3850\n",
      "roc_auc: 0.6317\n",
      "f1: 0.3220\n",
      "loss: 0.6274\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.2823\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3523\n",
      "roc_auc: 0.6303\n",
      "f1: 0.2600\n",
      "loss: 0.6412\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.2473\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3623\n",
      "roc_auc: 0.6271\n",
      "f1: 0.2920\n",
      "loss: 0.7114\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3432, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1726, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x00000158887AF4C0>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.1693\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.3225\n",
      "loss: 0.1155\n",
      "New best pr_auc_samples score (0.3225) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.1064\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.3333\n",
      "loss: 0.0991\n",
      "New best pr_auc_samples score (0.3333) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0936\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.3364\n",
      "loss: 0.0893\n",
      "New best pr_auc_samples score (0.3364) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0871\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.3491\n",
      "loss: 0.0849\n",
      "New best pr_auc_samples score (0.3491) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0841\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.3639\n",
      "loss: 0.0829\n",
      "New best pr_auc_samples score (0.3639) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0822\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.3740\n",
      "loss: 0.0815\n",
      "New best pr_auc_samples score (0.3740) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0811\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.3787\n",
      "loss: 0.0811\n",
      "New best pr_auc_samples score (0.3787) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0802\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.3838\n",
      "loss: 0.0807\n",
      "New best pr_auc_samples score (0.3838) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0792\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.3917\n",
      "loss: 0.0804\n",
      "New best pr_auc_samples score (0.3917) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0785\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.3967\n",
      "loss: 0.0802\n",
      "New best pr_auc_samples score (0.3967) at epoch-9, step-1880\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3432, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x00000158887AF4C0>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5310\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.1885\n",
      "roc_auc: 0.4712\n",
      "f1: 0.0000\n",
      "loss: 0.5074\n",
      "New best pr_auc score (0.1885) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.5227\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.2233\n",
      "roc_auc: 0.5591\n",
      "f1: 0.0000\n",
      "loss: 0.5047\n",
      "New best pr_auc score (0.2233) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.5206\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.2201\n",
      "roc_auc: 0.5646\n",
      "f1: 0.0000\n",
      "loss: 0.4966\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "\n",
    "dataset = mimic3_ds.set_task(\n",
    "    task_fn=lambda p: patient_level_readmission_prediction(p, max_length_visits=8)\n",
    ")\n",
    "\n",
    "train, val, test = split_by_patient(dataset, [0.8, 0.1, 0.1])\n",
    "\n",
    "train_loader = get_dataloader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = get_dataloader(val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = get_dataloader(test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# # Evaluate BiteNet performance as number of MaskEnc layers changes\n",
    "# RESULTS_FILE = \"./results/changing_n_layers.csv\"\n",
    "# n_layers_df = pd.DataFrame(columns=['model_name', 'feature_set', 'n_layers', 'pr_auc'] + [f\"precision@{k}\" for k in KS])\n",
    "#\n",
    "# for n_layers in range(9):\n",
    "#\n",
    "#     n_layers_df = train_and_record_metrics(\n",
    "#         model_readm=BiteNet(\n",
    "#             dataset = dataset,\n",
    "#             feature_keys = [\"diagnoses\", \"procedures\", \"intervals\"],\n",
    "#             label_key = \"readmission_label\",\n",
    "#             mode = \"binary\",\n",
    "#             n_mask_enc_layers=n_layers\n",
    "#         ).to(device),\n",
    "#         model_diag=BiteNet(\n",
    "#             dataset = dataset,\n",
    "#             feature_keys = [\"diagnoses\", \"procedures\", \"intervals\"],\n",
    "#             label_key = \"diagnosis_label\",\n",
    "#             mode = \"multilabel\",\n",
    "#             n_mask_enc_layers=n_layers\n",
    "#         ).to(device),\n",
    "#         df=n_layers_df,\n",
    "#         row_fields={\n",
    "#             \"model_name\": \"bitenet\",\n",
    "#             \"feature_set\": \"dxtx\",\n",
    "#             \"n_layers\": n_layers,\n",
    "#         },\n",
    "#         train_loader=train_loader,\n",
    "#         val_loader=val_loader,\n",
    "#         test_loader=test_loader,\n",
    "#     )\n",
    "#\n",
    "#     n_layers_df = train_and_record_metrics(\n",
    "#         model_readm=BiteNet(\n",
    "#             dataset = dataset,\n",
    "#             feature_keys = [\"diagnoses\", \"intervals\"],\n",
    "#             label_key = \"readmission_label\",\n",
    "#             mode = \"binary\",\n",
    "#             n_mask_enc_layers=n_layers\n",
    "#         ).to(device),\n",
    "#         model_diag=BiteNet(\n",
    "#             dataset = dataset,\n",
    "#             feature_keys = [\"diagnoses\", \"intervals\"],\n",
    "#             label_key = \"diagnosis_label\",\n",
    "#             mode = \"multilabel\",\n",
    "#             n_mask_enc_layers=n_layers\n",
    "#         ).to(device),\n",
    "#         df=n_layers_df,\n",
    "#         row_fields={\n",
    "#             \"model_name\": \"bitenet\",\n",
    "#             \"feature_set\": \"dx\",\n",
    "#             \"n_layers\": n_layers,\n",
    "#         },\n",
    "#         train_loader=train_loader,\n",
    "#         val_loader=val_loader,\n",
    "#         test_loader=test_loader\n",
    "#     )\n",
    "#\n",
    "# # Evaluate BiteNet performance as number of attention heads in MaskEnc layers changes\n",
    "# RESULTS_FILE = \"./results/changing_n_heads.csv\"\n",
    "# n_heads_df = pd.DataFrame(columns=['model_name', 'feature_set', 'seq_len', 'n_heads', 'trial', 'pr_auc'] + [f\"precision@{k}\" for k in KS])\n",
    "#\n",
    "# for n_heads in [4, 8, 16, 32]:\n",
    "#\n",
    "#     n_heads_df = train_and_record_metrics(\n",
    "#         model_readm=BiteNet(\n",
    "#             dataset = dataset,\n",
    "#             feature_keys = [\"diagnoses\", \"procedures\", \"intervals\"],\n",
    "#             label_key = \"readmission_label\",\n",
    "#             mode = \"binary\",\n",
    "#             n_heads=n_heads\n",
    "#         ).to(device),\n",
    "#         model_diag=BiteNet(\n",
    "#             dataset = dataset,\n",
    "#             feature_keys = [\"diagnoses\", \"procedures\", \"intervals\"],\n",
    "#             label_key = \"diagnosis_label\",\n",
    "#             mode = \"multilabel\",\n",
    "#             n_heads=n_heads\n",
    "#         ).to(device),\n",
    "#         df=n_heads_df,\n",
    "#         row_fields={\n",
    "#             \"model_name\": \"bitenet\",\n",
    "#             \"feature_set\": \"dxtx\",\n",
    "#             \"n_heads\": n_heads,\n",
    "#         },\n",
    "#         train_loader=train_loader,\n",
    "#         val_loader=val_loader,\n",
    "#         test_loader=test_loader,\n",
    "#     )\n",
    "#\n",
    "#     n_heads_df = train_and_record_metrics(\n",
    "#         model_readm=BiteNet(\n",
    "#             dataset = dataset,\n",
    "#             feature_keys = [\"diagnoses\", \"intervals\"],\n",
    "#             label_key = \"readmission_label\",\n",
    "#             mode = \"binary\",\n",
    "#             n_heads=n_heads\n",
    "#         ).to(device),\n",
    "#         model_diag=BiteNet(\n",
    "#             dataset = dataset,\n",
    "#             feature_keys = [\"diagnoses\", \"intervals\"],\n",
    "#             label_key = \"diagnosis_label\",\n",
    "#             mode = \"multilabel\",\n",
    "#             n_heads=n_heads\n",
    "#         ).to(device),\n",
    "#         df=n_heads_df,\n",
    "#         row_fields={\n",
    "#             \"model_name\": \"bitenet\",\n",
    "#             \"feature_set\": \"dx\",\n",
    "#             \"n_heads\": n_heads,\n",
    "#         },\n",
    "#         train_loader=train_loader,\n",
    "#         val_loader=val_loader,\n",
    "#         test_loader=test_loader\n",
    "#     )\n",
    "\n",
    "\n",
    "# Ablations\n",
    "\n",
    "RESULTS_FILE = \"./results/ablations.csv\"\n",
    "ablations_df = pd.DataFrame(columns=['model_name', 'feature_set', 'ablation', 'pr_auc'] + [f\"precision@{k}\" for k in KS])\n",
    "\n",
    "#################### BITENET DXTX ####################\n",
    "ablations_df = train_and_record_metrics(\n",
    "    model_readm=BiteNet(\n",
    "        dataset = dataset,\n",
    "        feature_keys = [\"diagnoses\", \"procedures\", \"intervals\"],\n",
    "        label_key = \"readmission_label\",\n",
    "        mode = \"binary\",\n",
    "        use_attn_pooling=False\n",
    "    ).to(device),\n",
    "    model_diag=BiteNet(\n",
    "        dataset = dataset,\n",
    "        feature_keys = [\"diagnoses\", \"procedures\", \"intervals\"],\n",
    "        label_key = \"diagnosis_label\",\n",
    "        mode = \"multilabel\",\n",
    "        use_attn_pooling=False\n",
    "    ).to(device),\n",
    "    df=ablations_df,\n",
    "    row_fields={\n",
    "        \"model_name\": \"bitenet\",\n",
    "        \"feature_set\": \"dxtx\",\n",
    "        \"ablation\": 'Attention',\n",
    "    },\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    ")\n",
    "\n",
    "ablations_df = train_and_record_metrics(\n",
    "    model_readm=BiteNet(\n",
    "        dataset = dataset,\n",
    "        feature_keys = [\"diagnoses\", \"procedures\", \"intervals\"],\n",
    "        label_key = \"readmission_label\",\n",
    "        mode = \"binary\",\n",
    "        use_dir_masks=False\n",
    "    ).to(device),\n",
    "    model_diag=BiteNet(\n",
    "        dataset = dataset,\n",
    "        feature_keys = [\"diagnoses\", \"procedures\", \"intervals\"],\n",
    "        label_key = \"diagnosis_label\",\n",
    "        mode = \"multilabel\",\n",
    "        use_dir_masks=False\n",
    "    ).to(device),\n",
    "    df=ablations_df,\n",
    "    row_fields={\n",
    "        \"model_name\": \"bitenet\",\n",
    "        \"feature_set\": \"dxtx\",\n",
    "        \"ablation\": 'DireMask',\n",
    "    },\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    ")\n",
    "\n",
    "ablations_df = train_and_record_metrics(\n",
    "    model_readm=BiteNet(\n",
    "        dataset = dataset,\n",
    "        feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "        label_key = \"readmission_label\",\n",
    "        mode = \"binary\",\n",
    "    ).to(device),\n",
    "    model_diag=BiteNet(\n",
    "        dataset = dataset,\n",
    "        feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "        label_key = \"diagnosis_label\",\n",
    "        mode = \"multilabel\",\n",
    "    ).to(device),\n",
    "    df=ablations_df,\n",
    "    row_fields={\n",
    "        \"model_name\": \"bitenet\",\n",
    "        \"feature_set\": \"dxtx\",\n",
    "        \"ablation\": 'Interval',\n",
    "    },\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    ")\n",
    "\n",
    "ablations_df = train_and_record_metrics(\n",
    "    model_readm=BiteNet(\n",
    "        dataset = dataset,\n",
    "        feature_keys = [\"diagnoses\", \"procedures\", \"intervals\"],\n",
    "        label_key = \"readmission_label\",\n",
    "        mode = \"binary\",\n",
    "    ).to(device),\n",
    "    model_diag=BiteNet(\n",
    "        dataset = dataset,\n",
    "        feature_keys = [\"diagnoses\", \"procedures\", \"intervals\"],\n",
    "        label_key = \"diagnosis_label\",\n",
    "        mode = \"multilabel\",\n",
    "    ).to(device),\n",
    "    df=ablations_df,\n",
    "    row_fields={\n",
    "        \"model_name\": \"bitenet\",\n",
    "        \"feature_set\": \"dxtx\",\n",
    "        \"ablation\": 'BiteNet',\n",
    "    },\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#################### BITENET DX ####################\n",
    "\n",
    "ablations_df = train_and_record_metrics(\n",
    "    model_readm=BiteNet(\n",
    "        dataset = dataset,\n",
    "        feature_keys = [\"diagnoses\", \"intervals\"],\n",
    "        label_key = \"readmission_label\",\n",
    "        mode = \"binary\",\n",
    "        use_attn_pooling=False\n",
    "    ).to(device),\n",
    "    model_diag=BiteNet(\n",
    "        dataset = dataset,\n",
    "        feature_keys = [\"diagnoses\", \"intervals\"],\n",
    "        label_key = \"diagnosis_label\",\n",
    "        mode = \"multilabel\",\n",
    "        use_attn_pooling=False\n",
    "    ).to(device),\n",
    "    df=ablations_df,\n",
    "    row_fields={\n",
    "        \"model_name\": \"bitenet\",\n",
    "        \"feature_set\": \"dx\",\n",
    "        \"ablation\": 'Attention',\n",
    "    },\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    ")\n",
    "\n",
    "ablations_df = train_and_record_metrics(\n",
    "    model_readm=BiteNet(\n",
    "        dataset = dataset,\n",
    "        feature_keys = [\"diagnoses\", \"intervals\"],\n",
    "        label_key = \"readmission_label\",\n",
    "        mode = \"binary\",\n",
    "        use_dir_masks=False\n",
    "    ).to(device),\n",
    "    model_diag=BiteNet(\n",
    "        dataset = dataset,\n",
    "        feature_keys = [\"diagnoses\", \"intervals\"],\n",
    "        label_key = \"diagnosis_label\",\n",
    "        mode = \"multilabel\",\n",
    "        use_dir_masks=False\n",
    "    ).to(device),\n",
    "    df=ablations_df,\n",
    "    row_fields={\n",
    "        \"model_name\": \"bitenet\",\n",
    "        \"feature_set\": \"dx\",\n",
    "        \"ablation\": 'DireMask',\n",
    "    },\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    ")\n",
    "\n",
    "ablations_df = train_and_record_metrics(\n",
    "    model_readm=BiteNet(\n",
    "        dataset = dataset,\n",
    "        feature_keys = [\"diagnoses\"],\n",
    "        label_key = \"readmission_label\",\n",
    "        mode = \"binary\",\n",
    "    ).to(device),\n",
    "    model_diag=BiteNet(\n",
    "        dataset = dataset,\n",
    "        feature_keys = [\"diagnoses\"],\n",
    "        label_key = \"diagnosis_label\",\n",
    "        mode = \"multilabel\",\n",
    "    ).to(device),\n",
    "    df=ablations_df,\n",
    "    row_fields={\n",
    "        \"model_name\": \"bitenet\",\n",
    "        \"feature_set\": \"dx\",\n",
    "        \"ablation\": 'Interval',\n",
    "    },\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    ")\n",
    "\n",
    "ablations_df = train_and_record_metrics(\n",
    "    model_readm=BiteNet(\n",
    "        dataset = dataset,\n",
    "        feature_keys = [\"diagnoses\", \"intervals\"],\n",
    "        label_key = \"readmission_label\",\n",
    "        mode = \"binary\",\n",
    "    ).to(device),\n",
    "    model_diag=BiteNet(\n",
    "        dataset = dataset,\n",
    "        feature_keys = [\"diagnoses\", \"intervals\"],\n",
    "        label_key = \"diagnosis_label\",\n",
    "        mode = \"multilabel\",\n",
    "    ).to(device),\n",
    "    df=ablations_df,\n",
    "    row_fields={\n",
    "        \"model_name\": \"bitenet\",\n",
    "        \"feature_set\": \"dx\",\n",
    "        \"ablation\": 'BiteNet',\n",
    "    },\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-08T17:24:46.372186Z",
     "end_time": "2023-05-08T17:38:11.611650Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-08T17:24:43.009187Z",
     "end_time": "2023-05-08T17:38:11.627654Z"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
