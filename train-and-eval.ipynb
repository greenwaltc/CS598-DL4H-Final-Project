{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Library imports and data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-07T17:32:27.043189Z",
     "end_time": "2023-05-07T17:32:28.685189Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camer\\PycharmProjects\\CS598-DL4H-Final-Project\\venv\\lib\\site-packages\\pyhealth\\trainer.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    }
   ],
   "source": [
    "from pyhealth.data import Visit\n",
    "from pyhealth.datasets import split_by_patient, get_dataloader\n",
    "from pyhealth.trainer import Trainer\n",
    "from pyhealth.medcode import InnerMap\n",
    "from pyhealth.metrics.binary import binary_metrics_fn\n",
    "from sklearn.metrics import precision_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from typing import List, Dict\n",
    "from model.bitenet import BiteNet\n",
    "from model.baseline import RNN, BRNN, RETAIN, Deepr\n",
    "import pickle\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with open(\"mimic3_dataset.pkl\", \"rb\") as dataset_file:\n",
    "    mimic3_ds = pickle.load(dataset_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-07T17:32:29.473943Z",
     "end_time": "2023-05-07T17:32:31.007941Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fgrnO7KkWDBY",
    "ExecuteTime": {
     "start_time": "2023-05-07T17:32:31.843744Z",
     "end_time": "2023-05-07T17:32:31.893731Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the tasks\n",
    "\n",
    "DIAGNOSES_KEY = \"conditions\"\n",
    "PROCEDURES_KEY = \"procedures\"\n",
    "INTERVAL_DAYS_KEY = \"days_since_first_visit\"\n",
    "\n",
    "icd9cm = InnerMap.load(\"ICD9CM\")\n",
    "\n",
    "def flatten(l: List):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def patient_level_readmission_prediction(patient, time_window: int = 30, max_length_visits: int = None):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    sorted_visits = sorted(patient, key=lambda visit: visit.encounter_time)\n",
    "\n",
    "    # Clip the patient visits to the most recent max_length_visits + 1 if max_length_visits is not None\n",
    "    if max_length_visits is not None:\n",
    "        n_visits = len(sorted_visits)\n",
    "        if n_visits > max_length_visits + 1:\n",
    "            sorted_visits = sorted_visits[n_visits - (max_length_visits + 1):]\n",
    "\n",
    "    feature_visits: List[Visit] = sorted_visits[:-1]\n",
    "    last_visit: Visit = sorted_visits[-1]\n",
    "    second_to_last_visit: Visit = feature_visits[-1]\n",
    "    first_visit: Visit = feature_visits[0]\n",
    "\n",
    "    # step 1 a: define readmission label\n",
    "    time_diff = (last_visit.encounter_time - second_to_last_visit.encounter_time).days\n",
    "    readmission_label = 1 if time_diff <= time_window else 0\n",
    "\n",
    "    # step 1 b: define diagnosis prediction label\n",
    "    diagnosis_label = list(set([icd9cm.get_ancestors(code)[1] for code in last_visit.get_code_list(\"DIAGNOSES_ICD\")]))\n",
    "\n",
    "    # step 2: obtain features\n",
    "    visits_diagnoses = []\n",
    "    visits_procedures = []\n",
    "    visits_intervals = []\n",
    "    for idx, visit in enumerate(feature_visits):\n",
    "        diagnoses = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        time_diff_from_first_visit = (visit.encounter_time - first_visit.encounter_time).days\n",
    "\n",
    "        # Exclude visits that are missing either diagnoses or procedures.\n",
    "        # BiteNet can handle missing procedures, but other PyHealth models like RNN\n",
    "        # require all features have a length greater than 0.\n",
    "        if len(diagnoses) == 0:\n",
    "            continue\n",
    "\n",
    "        visits_diagnoses.append(diagnoses)\n",
    "        visits_procedures.append(procedures)\n",
    "        visits_intervals.append([str(time_diff_from_first_visit)])\n",
    "\n",
    "    unique_diagnoses = list(set(flatten(visits_diagnoses)))\n",
    "\n",
    "    # step 3: exclusion criteria\n",
    "    if len(unique_diagnoses) == 0:\n",
    "        return []\n",
    "\n",
    "    # step 4: assemble the sample\n",
    "    samples.append(\n",
    "        {\n",
    "            \"patient_id\": patient.patient_id,\n",
    "            \"visit_id\": visit.visit_id,\n",
    "            \"diagnoses\": visits_diagnoses,\n",
    "            \"procedures\": visits_procedures,\n",
    "            \"intervals\": visits_intervals,\n",
    "            \"readmission_label\": readmission_label,\n",
    "            \"diagnosis_label\": diagnosis_label\n",
    "        }\n",
    "    )\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "KS = list(range(5, 31, 5))\n",
    "N_TRIALS=5\n",
    "SEQ_LENS = list(range(6, 17, 2))\n",
    "RESULTS_FILE = \"baseline_comparison.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-07T17:32:33.512087Z",
     "end_time": "2023-05-07T17:32:33.540084Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def train_and_inference(model, train_loader, val_loader, test_loader, lr=0.0001, monitor=\"pr_auc\", optim = torch.optim.Adam):\n",
    "    trainer = Trainer(model=model, device=device)\n",
    "    trainer.train(\n",
    "        train_dataloader=train_loader,\n",
    "        val_dataloader=val_loader,\n",
    "        epochs=10,\n",
    "        monitor=monitor,\n",
    "        optimizer_class=optim,\n",
    "        optimizer_params = {\"lr\" : lr},\n",
    "        load_best_model_at_last=False\n",
    "    )\n",
    "\n",
    "    return trainer.inference(test_loader)\n",
    "\n",
    "def precision_at_k(y_true: np.ndarray, y_prob: np.ndarray):\n",
    "\n",
    "    y_pred: np.ndarray = (y_prob > 0.5).astype(int)\n",
    "    desc_idx: np.ndarray = np.flip(np.argsort(y_prob, axis=-1), axis=-1)\n",
    "\n",
    "    y_true = np.take(y_true, desc_idx).astype(int)\n",
    "    y_pred = np.take(y_pred, desc_idx)\n",
    "\n",
    "    precisions: List[float] = []\n",
    "    for k in KS:\n",
    "        precisions.append(\n",
    "            precision_score(y_true[:, :k].reshape(-1), y_pred[:, :k].reshape(-1))\n",
    "        )\n",
    "\n",
    "    precisions: Dict[str, float] = {\n",
    "        f\"precision@{k}\": p for k, p in zip(KS, precisions)\n",
    "    }\n",
    "    return precisions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-07T17:32:50.404006Z",
     "end_time": "2023-05-07T17:32:50.418050Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-07T17:32:51.465331Z",
     "end_time": "2023-05-07T17:32:51.491342Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_record_metrics(model_readm, model_diag, df, row_fields, train_loader, val_loader, test_loader, lr=0.0005, readm_optim=torch.optim.Adam, diag_optim=torch.optim.Adam):\n",
    "    y_true, y_prob, _ = train_and_inference(\n",
    "        model_readm,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        test_loader,\n",
    "        lr=lr,\n",
    "        optim=readm_optim\n",
    "    )\n",
    "    binary_metrics = binary_metrics_fn(y_true, y_prob, metrics=[\"pr_auc\", \"roc_auc\", \"f1\"])\n",
    "\n",
    "    y_true, y_prob, _ = train_and_inference(\n",
    "        model_diag,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        test_loader,\n",
    "        lr=lr,\n",
    "        monitor=\"pr_auc_samples\",\n",
    "        optim=diag_optim\n",
    "    )\n",
    "    precisions = precision_at_k(y_true, y_prob)\n",
    "\n",
    "    row = binary_metrics | precisions | row_fields\n",
    "    row = {\n",
    "        k: [v] for k, v in row.items()\n",
    "    }\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame.from_dict(row)], ignore_index=True)\n",
    "\n",
    "    # Save df for checkpoint\n",
    "    df.to_csv(RESULTS_FILE, index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x000001EC67A361C0>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5293\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.1952\n",
      "roc_auc: 0.5025\n",
      "f1: 0.0000\n",
      "loss: 0.5066\n",
      "New best pr_auc score (0.1952) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.5188\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3221\n",
      "roc_auc: 0.5969\n",
      "f1: 0.1481\n",
      "loss: 0.4828\n",
      "New best pr_auc score (0.3221) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.4779\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3495\n",
      "roc_auc: 0.6122\n",
      "f1: 0.1538\n",
      "loss: 0.4729\n",
      "New best pr_auc score (0.3495) at epoch-2, step-564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "\n",
    "# Compare BiteNet performance to baselines\n",
    "\n",
    "metrics_df = pd.DataFrame(columns=['model_name', 'feature_set', 'seq_len', 'trial', 'pr_auc', 'roc_auc', 'f1'] + [f\"precision@{k}\" for k in KS])\n",
    "\n",
    "for seq_len in SEQ_LENS:\n",
    "\n",
    "    dataset = mimic3_ds.set_task(\n",
    "        task_fn=lambda p: patient_level_readmission_prediction(p, max_length_visits=seq_len)\n",
    "    )\n",
    "\n",
    "    for trial in range(N_TRIALS):\n",
    "\n",
    "        train, val, test = split_by_patient(dataset, [0.8, 0.1, 0.1])\n",
    "\n",
    "        train_loader = get_dataloader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader = get_dataloader(val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        test_loader = get_dataloader(test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        #################### BITENET ####################\n",
    "        metrics_df = train_and_record_metrics(\n",
    "            model_readm=BiteNet(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\", \"procedures\", \"intervals\"],\n",
    "                label_key = \"readmission_label\",\n",
    "                mode = \"binary\",\n",
    "            ).to(device),\n",
    "            model_diag=BiteNet(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\", \"procedures\", \"intervals\"],\n",
    "                label_key = \"diagnosis_label\",\n",
    "                mode = \"multilabel\",\n",
    "            ).to(device),\n",
    "            df=metrics_df,\n",
    "            row_fields={\n",
    "                \"model_name\": \"bitenet\",\n",
    "                \"feature_set\": \"dxtx\",\n",
    "                \"seq_len\": seq_len,\n",
    "                \"trial\": trial\n",
    "            },\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader,\n",
    "        )\n",
    "\n",
    "        metrics_df = train_and_record_metrics(\n",
    "            model_readm=BiteNet(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\", \"intervals\"],\n",
    "                label_key = \"readmission_label\",\n",
    "                mode = \"binary\",\n",
    "            ).to(device),\n",
    "            model_diag=BiteNet(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\", \"intervals\"],\n",
    "                label_key = \"diagnosis_label\",\n",
    "                mode = \"multilabel\",\n",
    "            ).to(device),\n",
    "            df=metrics_df,\n",
    "            row_fields={\n",
    "                \"model_name\": \"bitenet\",\n",
    "                \"feature_set\": \"dx\",\n",
    "                \"seq_len\": seq_len,\n",
    "                \"trial\": trial\n",
    "            },\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader\n",
    "        )\n",
    "\n",
    "        #################### RNN ####################\n",
    "        metrics_df = train_and_record_metrics(\n",
    "                model_readm=RNN(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "                label_key = \"readmission_label\",\n",
    "                mode = \"binary\",\n",
    "            ).to(device),\n",
    "            model_diag=RNN(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "                label_key = \"diagnosis_label\",\n",
    "                mode = \"multilabel\",\n",
    "            ).to(device),\n",
    "            df=metrics_df,\n",
    "            row_fields={\n",
    "                \"model_name\": \"rnn\",\n",
    "                \"feature_set\": \"dxtx\",\n",
    "                \"seq_len\": seq_len,\n",
    "                \"trial\": trial\n",
    "            },\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader\n",
    "        )\n",
    "\n",
    "        metrics_df = train_and_record_metrics(\n",
    "                model_readm=RNN(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\"],\n",
    "                label_key = \"readmission_label\",\n",
    "                mode = \"binary\",\n",
    "            ).to(device),\n",
    "            model_diag=RNN(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\"],\n",
    "                label_key = \"diagnosis_label\",\n",
    "                mode = \"multilabel\",\n",
    "            ).to(device),\n",
    "            df=metrics_df,\n",
    "            row_fields={\n",
    "                \"model_name\": \"rnn\",\n",
    "                \"feature_set\": \"dx\",\n",
    "                \"seq_len\": seq_len,\n",
    "                \"trial\": trial\n",
    "            },\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader,\n",
    "        )\n",
    "\n",
    "        #################### BRNN ####################\n",
    "        metrics_df = train_and_record_metrics(\n",
    "                model_readm=BRNN(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "                label_key = \"readmission_label\",\n",
    "                mode = \"binary\",\n",
    "                bidirectional=True\n",
    "            ).to(device),\n",
    "            model_diag=RNN(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "                label_key = \"diagnosis_label\",\n",
    "                mode = \"multilabel\",\n",
    "                bidirectional=True\n",
    "            ).to(device),\n",
    "            df=metrics_df,\n",
    "            row_fields={\n",
    "                \"model_name\": \"brnn\",\n",
    "                \"feature_set\": \"dxtx\",\n",
    "                \"seq_len\": seq_len,\n",
    "                \"trial\": trial\n",
    "            },\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader\n",
    "        )\n",
    "\n",
    "        metrics_df = train_and_record_metrics(\n",
    "                model_readm=BRNN(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\"],\n",
    "                label_key = \"readmission_label\",\n",
    "                mode = \"binary\",\n",
    "                bidirectional=True\n",
    "            ).to(device),\n",
    "            model_diag=RNN(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\"],\n",
    "                label_key = \"diagnosis_label\",\n",
    "                mode = \"multilabel\",\n",
    "                bidirectional=True\n",
    "            ).to(device),\n",
    "            df=metrics_df,\n",
    "            row_fields={\n",
    "                \"model_name\": \"brnn\",\n",
    "                \"feature_set\": \"dx\",\n",
    "                \"seq_len\": seq_len,\n",
    "                \"trial\": trial\n",
    "            },\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader\n",
    "        )\n",
    "\n",
    "        #################### RETAIN ####################\n",
    "        metrics_df = train_and_record_metrics(\n",
    "                model_readm=RETAIN(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "                label_key = \"readmission_label\",\n",
    "                mode = \"binary\"\n",
    "            ).to(device),\n",
    "            model_diag=RETAIN(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "                label_key = \"diagnosis_label\",\n",
    "                mode = \"multilabel\"\n",
    "            ).to(device),\n",
    "            df=metrics_df,\n",
    "            row_fields={\n",
    "                \"model_name\": \"retain\",\n",
    "                \"feature_set\": \"dxtx\",\n",
    "                \"seq_len\": seq_len,\n",
    "                \"trial\": trial\n",
    "            },\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader,\n",
    "        )\n",
    "\n",
    "        metrics_df = train_and_record_metrics(\n",
    "                model_readm=RETAIN(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\"],\n",
    "                label_key = \"readmission_label\",\n",
    "                mode = \"binary\"\n",
    "            ).to(device),\n",
    "            model_diag=RETAIN(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\"],\n",
    "                label_key = \"diagnosis_label\",\n",
    "                mode = \"multilabel\"\n",
    "            ).to(device),\n",
    "            df=metrics_df,\n",
    "            row_fields={\n",
    "                \"model_name\": \"retain\",\n",
    "                \"feature_set\": \"dx\",\n",
    "                \"seq_len\": seq_len,\n",
    "                \"trial\": trial\n",
    "            },\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader\n",
    "        )\n",
    "\n",
    "        #################### Deepr ####################\n",
    "        metrics_df = train_and_record_metrics(\n",
    "                model_readm=Deepr(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "                label_key = \"readmission_label\",\n",
    "                mode = \"binary\"\n",
    "            ).to(device),\n",
    "            model_diag=Deepr(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "                label_key = \"diagnosis_label\",\n",
    "                mode = \"multilabel\"\n",
    "            ).to(device),\n",
    "            df=metrics_df,\n",
    "            row_fields={\n",
    "                \"model_name\": \"deepr\",\n",
    "                \"feature_set\": \"dxtx\",\n",
    "                \"seq_len\": seq_len,\n",
    "                \"trial\": trial\n",
    "            },\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader\n",
    "        )\n",
    "\n",
    "        metrics_df = train_and_record_metrics(\n",
    "                model_readm=Deepr(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\"],\n",
    "                label_key = \"readmission_label\",\n",
    "                mode = \"binary\"\n",
    "            ).to(device),\n",
    "            model_diag=Deepr(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\"],\n",
    "                label_key = \"diagnosis_label\",\n",
    "                mode = \"multilabel\"\n",
    "            ).to(device),\n",
    "            df=metrics_df,\n",
    "            row_fields={\n",
    "                \"model_name\": \"deepr\",\n",
    "                \"feature_set\": \"dx\",\n",
    "                \"seq_len\": seq_len,\n",
    "                \"trial\": trial\n",
    "            },\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T00:58:04.267799Z",
     "end_time": "2023-05-05T05:30:31.302906Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x00000268CA438370>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5356\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.2389\n",
      "roc_auc: 0.5195\n",
      "f1: 0.0000\n",
      "loss: 0.5313\n",
      "New best pr_auc score (0.2389) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.5188\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.2468\n",
      "roc_auc: 0.5338\n",
      "f1: 0.0000\n",
      "loss: 0.5410\n",
      "New best pr_auc score (0.2468) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.4958\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3195\n",
      "roc_auc: 0.5655\n",
      "f1: 0.1143\n",
      "loss: 0.5307\n",
      "New best pr_auc score (0.3195) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.4631\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3656\n",
      "roc_auc: 0.6278\n",
      "f1: 0.1474\n",
      "loss: 0.5043\n",
      "New best pr_auc score (0.3656) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.4342\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3949\n",
      "roc_auc: 0.6346\n",
      "f1: 0.2441\n",
      "loss: 0.5220\n",
      "New best pr_auc score (0.3949) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.4008\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3875\n",
      "roc_auc: 0.6385\n",
      "f1: 0.2620\n",
      "loss: 0.5291\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.3693\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.4039\n",
      "roc_auc: 0.6461\n",
      "f1: 0.3828\n",
      "loss: 0.5728\n",
      "New best pr_auc score (0.4039) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.3393\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3979\n",
      "roc_auc: 0.6345\n",
      "f1: 0.3413\n",
      "loss: 0.6296\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.3120\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3807\n",
      "roc_auc: 0.6259\n",
      "f1: 0.3077\n",
      "loss: 0.7112\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.3072\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3719\n",
      "roc_auc: 0.6185\n",
      "f1: 0.2890\n",
      "loss: 0.6867\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x00000268CA438370>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.1830\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.3339\n",
      "loss: 0.1159\n",
      "New best pr_auc_samples score (0.3339) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.1039\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.3389\n",
      "loss: 0.0977\n",
      "New best pr_auc_samples score (0.3389) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0913\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.3409\n",
      "loss: 0.0901\n",
      "New best pr_auc_samples score (0.3409) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0862\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.3508\n",
      "loss: 0.0864\n",
      "New best pr_auc_samples score (0.3508) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0836\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.3589\n",
      "loss: 0.0848\n",
      "New best pr_auc_samples score (0.3589) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0822\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.3715\n",
      "loss: 0.0837\n",
      "New best pr_auc_samples score (0.3715) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0809\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.3840\n",
      "loss: 0.0829\n",
      "New best pr_auc_samples score (0.3840) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0800\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.3954\n",
      "loss: 0.0828\n",
      "New best pr_auc_samples score (0.3954) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0792\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.4064\n",
      "loss: 0.0819\n",
      "New best pr_auc_samples score (0.4064) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0783\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.4139\n",
      "loss: 0.0811\n",
      "New best pr_auc_samples score (0.4139) at epoch-9, step-1880\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x00000268CA438370>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5362\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.2431\n",
      "roc_auc: 0.5212\n",
      "f1: 0.0000\n",
      "loss: 0.5295\n",
      "New best pr_auc score (0.2431) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.5083\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3255\n",
      "roc_auc: 0.5715\n",
      "f1: 0.1136\n",
      "loss: 0.5136\n",
      "New best pr_auc score (0.3255) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.4730\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3536\n",
      "roc_auc: 0.6035\n",
      "f1: 0.1333\n",
      "loss: 0.5049\n",
      "New best pr_auc score (0.3536) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.4565\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3676\n",
      "roc_auc: 0.6048\n",
      "f1: 0.1929\n",
      "loss: 0.5085\n",
      "New best pr_auc score (0.3676) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.4287\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3932\n",
      "roc_auc: 0.6197\n",
      "f1: 0.2636\n",
      "loss: 0.5204\n",
      "New best pr_auc score (0.3932) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.4020\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3814\n",
      "roc_auc: 0.6155\n",
      "f1: 0.2466\n",
      "loss: 0.5407\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.3746\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3652\n",
      "roc_auc: 0.5789\n",
      "f1: 0.2466\n",
      "loss: 0.6470\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.3472\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3447\n",
      "roc_auc: 0.5828\n",
      "f1: 0.3150\n",
      "loss: 0.6503\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.3266\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3638\n",
      "roc_auc: 0.6061\n",
      "f1: 0.3130\n",
      "loss: 0.6848\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.3119\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3501\n",
      "roc_auc: 0.5749\n",
      "f1: 0.3089\n",
      "loss: 0.7792\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x00000268CA438370>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.1854\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.3317\n",
      "loss: 0.1170\n",
      "New best pr_auc_samples score (0.3317) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.1052\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.3356\n",
      "loss: 0.0991\n",
      "New best pr_auc_samples score (0.3356) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0923\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.3394\n",
      "loss: 0.0907\n",
      "New best pr_auc_samples score (0.3394) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0870\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.3455\n",
      "loss: 0.0875\n",
      "New best pr_auc_samples score (0.3455) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0842\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.3545\n",
      "loss: 0.0854\n",
      "New best pr_auc_samples score (0.3545) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0827\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.3666\n",
      "loss: 0.0842\n",
      "New best pr_auc_samples score (0.3666) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0816\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.3807\n",
      "loss: 0.0834\n",
      "New best pr_auc_samples score (0.3807) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0806\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.3935\n",
      "loss: 0.0824\n",
      "New best pr_auc_samples score (0.3935) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0795\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.4014\n",
      "loss: 0.0817\n",
      "New best pr_auc_samples score (0.4014) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0785\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.4097\n",
      "loss: 0.0816\n",
      "New best pr_auc_samples score (0.4097) at epoch-9, step-1880\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x00000268C2FAB6A0>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5402\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.2522\n",
      "roc_auc: 0.5587\n",
      "f1: 0.0000\n",
      "loss: 0.5284\n",
      "New best pr_auc score (0.2522) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.5151\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3223\n",
      "roc_auc: 0.5848\n",
      "f1: 0.1371\n",
      "loss: 0.5078\n",
      "New best pr_auc score (0.3223) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.4842\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3378\n",
      "roc_auc: 0.5750\n",
      "f1: 0.1564\n",
      "loss: 0.5039\n",
      "New best pr_auc score (0.3378) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.4513\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3401\n",
      "roc_auc: 0.5659\n",
      "f1: 0.1858\n",
      "loss: 0.5473\n",
      "New best pr_auc score (0.3401) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.4237\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3485\n",
      "roc_auc: 0.5548\n",
      "f1: 0.1969\n",
      "loss: 0.5636\n",
      "New best pr_auc score (0.3485) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.3961\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3418\n",
      "roc_auc: 0.5445\n",
      "f1: 0.2157\n",
      "loss: 0.6102\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.3630\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3444\n",
      "roc_auc: 0.5495\n",
      "f1: 0.2370\n",
      "loss: 0.6183\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.3413\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3387\n",
      "roc_auc: 0.5457\n",
      "f1: 0.2383\n",
      "loss: 0.6740\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.3208\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3466\n",
      "roc_auc: 0.5477\n",
      "f1: 0.2553\n",
      "loss: 0.7835\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.2892\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3035\n",
      "roc_auc: 0.5421\n",
      "f1: 0.2724\n",
      "loss: 0.8041\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x00000268C2FAB6A0>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.1871\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.3280\n",
      "loss: 0.1181\n",
      "New best pr_auc_samples score (0.3280) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.1068\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.3350\n",
      "loss: 0.1008\n",
      "New best pr_auc_samples score (0.3350) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0933\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.3388\n",
      "loss: 0.0913\n",
      "New best pr_auc_samples score (0.3388) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0875\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.3444\n",
      "loss: 0.0875\n",
      "New best pr_auc_samples score (0.3444) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0843\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.3558\n",
      "loss: 0.0853\n",
      "New best pr_auc_samples score (0.3558) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0826\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.3673\n",
      "loss: 0.0842\n",
      "New best pr_auc_samples score (0.3673) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0815\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.3780\n",
      "loss: 0.0835\n",
      "New best pr_auc_samples score (0.3780) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0805\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.3929\n",
      "loss: 0.0825\n",
      "New best pr_auc_samples score (0.3929) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0795\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.3985\n",
      "loss: 0.0822\n",
      "New best pr_auc_samples score (0.3985) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0787\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.4090\n",
      "loss: 0.0816\n",
      "New best pr_auc_samples score (0.4090) at epoch-9, step-1880\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x00000268C2FAB6A0>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5347\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.2436\n",
      "roc_auc: 0.5377\n",
      "f1: 0.0000\n",
      "loss: 0.5328\n",
      "New best pr_auc score (0.2436) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.5024\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3577\n",
      "roc_auc: 0.5832\n",
      "f1: 0.1967\n",
      "loss: 0.5026\n",
      "New best pr_auc score (0.3577) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.4682\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3754\n",
      "roc_auc: 0.6011\n",
      "f1: 0.2065\n",
      "loss: 0.4935\n",
      "New best pr_auc score (0.3754) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.4441\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3550\n",
      "roc_auc: 0.5721\n",
      "f1: 0.2128\n",
      "loss: 0.5221\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.4212\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3769\n",
      "roc_auc: 0.5950\n",
      "f1: 0.2574\n",
      "loss: 0.5407\n",
      "New best pr_auc score (0.3769) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.3947\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3690\n",
      "roc_auc: 0.5780\n",
      "f1: 0.2500\n",
      "loss: 0.5666\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.3655\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3633\n",
      "roc_auc: 0.5694\n",
      "f1: 0.2407\n",
      "loss: 0.6290\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.3396\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3522\n",
      "roc_auc: 0.5546\n",
      "f1: 0.2489\n",
      "loss: 0.7175\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.3226\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3494\n",
      "roc_auc: 0.5478\n",
      "f1: 0.2656\n",
      "loss: 0.7601\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.3051\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3396\n",
      "roc_auc: 0.5406\n",
      "f1: 0.2625\n",
      "loss: 0.8764\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x00000268C2FAB6A0>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.1830\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.3290\n",
      "loss: 0.1177\n",
      "New best pr_auc_samples score (0.3290) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.1062\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.3361\n",
      "loss: 0.1002\n",
      "New best pr_auc_samples score (0.3361) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0928\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.3400\n",
      "loss: 0.0910\n",
      "New best pr_auc_samples score (0.3400) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0870\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.3452\n",
      "loss: 0.0872\n",
      "New best pr_auc_samples score (0.3452) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0842\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.3519\n",
      "loss: 0.0851\n",
      "New best pr_auc_samples score (0.3519) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0824\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.3728\n",
      "loss: 0.0844\n",
      "New best pr_auc_samples score (0.3728) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0812\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.3909\n",
      "loss: 0.0831\n",
      "New best pr_auc_samples score (0.3909) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0802\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.4021\n",
      "loss: 0.0819\n",
      "New best pr_auc_samples score (0.4021) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0793\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.4073\n",
      "loss: 0.0816\n",
      "New best pr_auc_samples score (0.4073) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0784\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.4110\n",
      "loss: 0.0814\n",
      "New best pr_auc_samples score (0.4110) at epoch-9, step-1880\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x00000268CAB2FE80>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5326\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.2100\n",
      "roc_auc: 0.5040\n",
      "f1: 0.0000\n",
      "loss: 0.5092\n",
      "New best pr_auc score (0.2100) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.5093\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.2921\n",
      "roc_auc: 0.5535\n",
      "f1: 0.1146\n",
      "loss: 0.4904\n",
      "New best pr_auc score (0.2921) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.4806\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3109\n",
      "roc_auc: 0.5622\n",
      "f1: 0.1258\n",
      "loss: 0.4955\n",
      "New best pr_auc score (0.3109) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.4559\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3303\n",
      "roc_auc: 0.5944\n",
      "f1: 0.1798\n",
      "loss: 0.4889\n",
      "New best pr_auc score (0.3303) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.4278\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3404\n",
      "roc_auc: 0.6070\n",
      "f1: 0.2578\n",
      "loss: 0.5168\n",
      "New best pr_auc score (0.3404) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.3992\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3424\n",
      "roc_auc: 0.5922\n",
      "f1: 0.2996\n",
      "loss: 0.5556\n",
      "New best pr_auc score (0.3424) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.3761\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3469\n",
      "roc_auc: 0.5736\n",
      "f1: 0.2871\n",
      "loss: 0.5772\n",
      "New best pr_auc score (0.3469) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.3415\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3473\n",
      "roc_auc: 0.5832\n",
      "f1: 0.3150\n",
      "loss: 0.6408\n",
      "New best pr_auc score (0.3473) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.3293\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3562\n",
      "roc_auc: 0.5930\n",
      "f1: 0.3102\n",
      "loss: 0.6897\n",
      "New best pr_auc score (0.3562) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.3034\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3619\n",
      "roc_auc: 0.5910\n",
      "f1: 0.3077\n",
      "loss: 0.7323\n",
      "New best pr_auc score (0.3619) at epoch-9, step-1880\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x00000268CAB2FE80>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.1859\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.3268\n",
      "loss: 0.1189\n",
      "New best pr_auc_samples score (0.3268) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.1077\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.3363\n",
      "loss: 0.1017\n",
      "New best pr_auc_samples score (0.3363) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0941\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.3396\n",
      "loss: 0.0920\n",
      "New best pr_auc_samples score (0.3396) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0879\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.3456\n",
      "loss: 0.0877\n",
      "New best pr_auc_samples score (0.3456) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0846\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.3554\n",
      "loss: 0.0854\n",
      "New best pr_auc_samples score (0.3554) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0829\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.3730\n",
      "loss: 0.0844\n",
      "New best pr_auc_samples score (0.3730) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0816\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.3875\n",
      "loss: 0.0831\n",
      "New best pr_auc_samples score (0.3875) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0806\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.4000\n",
      "loss: 0.0824\n",
      "New best pr_auc_samples score (0.4000) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0796\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.4088\n",
      "loss: 0.0819\n",
      "New best pr_auc_samples score (0.4088) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0788\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.4183\n",
      "loss: 0.0812\n",
      "New best pr_auc_samples score (0.4183) at epoch-9, step-1880\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x00000268CAB2FE80>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5374\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.2160\n",
      "roc_auc: 0.5157\n",
      "f1: 0.0000\n",
      "loss: 0.5126\n",
      "New best pr_auc score (0.2160) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.5113\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3326\n",
      "roc_auc: 0.6200\n",
      "f1: 0.1491\n",
      "loss: 0.4779\n",
      "New best pr_auc score (0.3326) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.4790\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3412\n",
      "roc_auc: 0.6248\n",
      "f1: 0.1605\n",
      "loss: 0.4734\n",
      "New best pr_auc score (0.3412) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.4569\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3520\n",
      "roc_auc: 0.6335\n",
      "f1: 0.1789\n",
      "loss: 0.4856\n",
      "New best pr_auc score (0.3520) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.4312\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3539\n",
      "roc_auc: 0.6254\n",
      "f1: 0.2341\n",
      "loss: 0.4980\n",
      "New best pr_auc score (0.3539) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.4070\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3503\n",
      "roc_auc: 0.6043\n",
      "f1: 0.2057\n",
      "loss: 0.5350\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.3822\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3430\n",
      "roc_auc: 0.6186\n",
      "f1: 0.2280\n",
      "loss: 0.5572\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.3658\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3420\n",
      "roc_auc: 0.5895\n",
      "f1: 0.2347\n",
      "loss: 0.6029\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.3410\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3414\n",
      "roc_auc: 0.5918\n",
      "f1: 0.2032\n",
      "loss: 0.7275\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.3188\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3516\n",
      "roc_auc: 0.5903\n",
      "f1: 0.2523\n",
      "loss: 0.7509\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x00000268CAB2FE80>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.1867\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.3318\n",
      "loss: 0.1163\n",
      "New best pr_auc_samples score (0.3318) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.1024\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.3365\n",
      "loss: 0.0959\n",
      "New best pr_auc_samples score (0.3365) at epoch-1, step-376\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 51\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m#################### BITENET ####################\u001B[39;00m\n\u001B[0;32m     23\u001B[0m train_and_record_metrics(\n\u001B[0;32m     24\u001B[0m     model_readm\u001B[38;5;241m=\u001B[39mBiteNet(\n\u001B[0;32m     25\u001B[0m         dataset \u001B[38;5;241m=\u001B[39m dataset,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     48\u001B[0m     test_loader\u001B[38;5;241m=\u001B[39mtest_loader,\n\u001B[0;32m     49\u001B[0m )\n\u001B[1;32m---> 51\u001B[0m \u001B[43mtrain_and_record_metrics\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     52\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_readm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mBiteNet\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     53\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     54\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfeature_keys\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdiagnoses\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mintervals\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabel_key\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mreadmission_label\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbinary\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     57\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_mask_enc_layers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_layers\u001B[49m\n\u001B[0;32m     58\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     59\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_diag\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mBiteNet\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     60\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfeature_keys\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdiagnoses\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mintervals\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     62\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabel_key\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdiagnosis_label\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     63\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmultilabel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     64\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_mask_enc_layers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_layers\u001B[49m\n\u001B[0;32m     65\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_layers_df\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     67\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrow_fields\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\n\u001B[0;32m     68\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel_name\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbitenet\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     69\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfeature_set\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdx\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     70\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseq_len\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseq_len\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     71\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mn_layers\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     72\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrial\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrial\u001B[49m\n\u001B[0;32m     73\u001B[0m \u001B[43m    \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     74\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     75\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     76\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_loader\u001B[49m\n\u001B[0;32m     77\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[6], line 12\u001B[0m, in \u001B[0;36mtrain_and_record_metrics\u001B[1;34m(model_readm, model_diag, df, row_fields, train_loader, val_loader, test_loader, lr, readm_optim, diag_optim)\u001B[0m\n\u001B[0;32m      2\u001B[0m y_true, y_prob, _ \u001B[38;5;241m=\u001B[39m train_and_inference(\n\u001B[0;32m      3\u001B[0m     model_readm,\n\u001B[0;32m      4\u001B[0m     train_loader,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      8\u001B[0m     optim\u001B[38;5;241m=\u001B[39mreadm_optim\n\u001B[0;32m      9\u001B[0m )\n\u001B[0;32m     10\u001B[0m binary_metrics \u001B[38;5;241m=\u001B[39m binary_metrics_fn(y_true, y_prob, metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpr_auc\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mroc_auc\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mf1\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m---> 12\u001B[0m y_true, y_prob, _ \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_and_inference\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_diag\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmonitor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpr_auc_samples\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdiag_optim\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m precisions \u001B[38;5;241m=\u001B[39m precision_at_k(y_true, y_prob)\n\u001B[0;32m     23\u001B[0m row \u001B[38;5;241m=\u001B[39m binary_metrics \u001B[38;5;241m|\u001B[39m precisions \u001B[38;5;241m|\u001B[39m row_fields\n",
      "Cell \u001B[1;32mIn[5], line 4\u001B[0m, in \u001B[0;36mtrain_and_inference\u001B[1;34m(model, train_loader, val_loader, test_loader, lr, monitor, optim)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_and_inference\u001B[39m(model, train_loader, val_loader, test_loader, lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m, monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpr_auc\u001B[39m\u001B[38;5;124m\"\u001B[39m, optim \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam):\n\u001B[0;32m      3\u001B[0m     trainer \u001B[38;5;241m=\u001B[39m Trainer(model\u001B[38;5;241m=\u001B[39mmodel, device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[1;32m----> 4\u001B[0m     \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m        \u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m        \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmonitor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmonitor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptimizer_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptim\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptimizer_params\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m        \u001B[49m\u001B[43mload_best_model_at_last\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[0;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39minference(test_loader)\n",
      "File \u001B[1;32m~\\PycharmProjects\\CS598-DL4H-Final-Project\\venv\\lib\\site-packages\\pyhealth\\trainer.py:199\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, train_dataloader, val_dataloader, test_dataloader, epochs, optimizer_class, optimizer_params, weight_decay, max_grad_norm, monitor, monitor_criterion, load_best_model_at_last)\u001B[0m\n\u001B[0;32m    197\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(data_iterator)\n\u001B[0;32m    198\u001B[0m \u001B[38;5;66;03m# forward\u001B[39;00m\n\u001B[1;32m--> 199\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdata)\n\u001B[0;32m    200\u001B[0m loss \u001B[38;5;241m=\u001B[39m output[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    201\u001B[0m \u001B[38;5;66;03m# backward\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\PycharmProjects\\CS598-DL4H-Final-Project\\model\\bitenet.py:381\u001B[0m, in \u001B[0;36mBiteNet.forward\u001B[1;34m(self, **kwargs)\u001B[0m\n\u001B[0;32m    378\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc(output)\n\u001B[0;32m    380\u001B[0m \u001B[38;5;66;03m# obtain y_true, loss, y_prob\u001B[39;00m\n\u001B[1;32m--> 381\u001B[0m y_true \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprepare_labels\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabel_key\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabel_tokenizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    382\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_loss_function()(logits, y_true)\n\u001B[0;32m    383\u001B[0m y_prob \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_y_prob(logits)\n",
      "File \u001B[1;32m~\\PycharmProjects\\CS598-DL4H-Final-Project\\venv\\lib\\site-packages\\pyhealth\\models\\base_model.py:283\u001B[0m, in \u001B[0;36mBaseModel.prepare_labels\u001B[1;34m(self, labels, label_tokenizer)\u001B[0m\n\u001B[0;32m    281\u001B[0m     \u001B[38;5;66;03m# convert to multihot\u001B[39;00m\n\u001B[0;32m    282\u001B[0m     num_labels \u001B[38;5;241m=\u001B[39m label_tokenizer\u001B[38;5;241m.\u001B[39mget_vocabulary_size()\n\u001B[1;32m--> 283\u001B[0m     labels \u001B[38;5;241m=\u001B[39m \u001B[43mbatch_to_multihot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_labels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    285\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\CS598-DL4H-Final-Project\\venv\\lib\\site-packages\\pyhealth\\models\\utils.py:18\u001B[0m, in \u001B[0;36mbatch_to_multihot\u001B[1;34m(label, num_labels)\u001B[0m\n\u001B[0;32m     16\u001B[0m multihot \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros((\u001B[38;5;28mlen\u001B[39m(label), num_labels))\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, l \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(label):\n\u001B[1;32m---> 18\u001B[0m     \u001B[43mmultihot\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ml\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m multihot\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "\n",
    "# Evaluate BiteNet performance as number of MaskEnc layers changes\n",
    "\n",
    "RESULTS_FILE = \"./results/changing_n_layers.csv\"\n",
    "n_layers_df = pd.DataFrame(columns=['model_name', 'feature_set', 'seq_len', 'n_layers', 'trial', 'pr_auc', 'roc_auc', 'f1'] + [f\"precision@{k}\" for k in KS])\n",
    "\n",
    "for seq_len in SEQ_LENS:\n",
    "\n",
    "    dataset = mimic3_ds.set_task(\n",
    "        task_fn=lambda p: patient_level_readmission_prediction(p, max_length_visits=seq_len)\n",
    "    )\n",
    "\n",
    "    for n_layers in range(9):\n",
    "\n",
    "        for trial in range(N_TRIALS):\n",
    "\n",
    "            train, val, test = split_by_patient(dataset, [0.8, 0.1, 0.1])\n",
    "\n",
    "            train_loader = get_dataloader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "            val_loader = get_dataloader(val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            test_loader = get_dataloader(test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            #################### BITENET ####################\n",
    "            n_layers_df = train_and_record_metrics(\n",
    "                model_readm=BiteNet(\n",
    "                    dataset = dataset,\n",
    "                    feature_keys = [\"diagnoses\", \"procedures\", \"intervals\"],\n",
    "                    label_key = \"readmission_label\",\n",
    "                    mode = \"binary\",\n",
    "                    n_mask_enc_layers=n_layers\n",
    "                ).to(device),\n",
    "                model_diag=BiteNet(\n",
    "                    dataset = dataset,\n",
    "                    feature_keys = [\"diagnoses\", \"procedures\", \"intervals\"],\n",
    "                    label_key = \"diagnosis_label\",\n",
    "                    mode = \"multilabel\",\n",
    "                    n_mask_enc_layers=n_layers\n",
    "                ).to(device),\n",
    "                df=n_layers_df,\n",
    "                row_fields={\n",
    "                    \"model_name\": \"bitenet\",\n",
    "                    \"feature_set\": \"dxtx\",\n",
    "                    \"seq_len\": seq_len,\n",
    "                    \"n_layers\": n_layers,\n",
    "                    \"trial\": trial\n",
    "                },\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                test_loader=test_loader,\n",
    "            )\n",
    "\n",
    "            n_layers_df = train_and_record_metrics(\n",
    "                model_readm=BiteNet(\n",
    "                    dataset = dataset,\n",
    "                    feature_keys = [\"diagnoses\", \"intervals\"],\n",
    "                    label_key = \"readmission_label\",\n",
    "                    mode = \"binary\",\n",
    "                    n_mask_enc_layers=n_layers\n",
    "                ).to(device),\n",
    "                model_diag=BiteNet(\n",
    "                    dataset = dataset,\n",
    "                    feature_keys = [\"diagnoses\", \"intervals\"],\n",
    "                    label_key = \"diagnosis_label\",\n",
    "                    mode = \"multilabel\",\n",
    "                    n_mask_enc_layers=n_layers\n",
    "                ).to(device),\n",
    "                df=n_layers_df,\n",
    "                row_fields={\n",
    "                    \"model_name\": \"bitenet\",\n",
    "                    \"feature_set\": \"dx\",\n",
    "                    \"seq_len\": seq_len,\n",
    "                    \"n_layers\": n_layers,\n",
    "                    \"trial\": trial\n",
    "                },\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                test_loader=test_loader\n",
    "            )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-07T17:25:46.063591Z",
     "end_time": "2023-05-07T17:29:52.636747Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Evaluate BiteNet performance as number of attention heads in MaskEnc layers changes\n",
    "\n",
    "RESULTS_FILE = \"./results/changing_n_heads.csv\"\n",
    "n_heads_df = pd.DataFrame(columns=['model_name', 'feature_set', 'seq_len', 'n_heads', 'trial', 'pr_auc', 'roc_auc', 'f1'] + [f\"precision@{k}\" for k in KS])\n",
    "\n",
    "for seq_len in SEQ_LENS:\n",
    "\n",
    "    dataset = mimic3_ds.set_task(\n",
    "        task_fn=lambda p: patient_level_readmission_prediction(p, max_length_visits=seq_len)\n",
    "    )\n",
    "\n",
    "    for n_heads in [4, 8, 16, 32]:\n",
    "\n",
    "        for trial in range(N_TRIALS):\n",
    "\n",
    "            train, val, test = split_by_patient(dataset, [0.8, 0.1, 0.1])\n",
    "\n",
    "            train_loader = get_dataloader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "            val_loader = get_dataloader(val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            test_loader = get_dataloader(test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            #################### BITENET ####################\n",
    "            n_heads_df = train_and_record_metrics(\n",
    "                model_readm=BiteNet(\n",
    "                    dataset = dataset,\n",
    "                    feature_keys = [\"diagnoses\", \"procedures\", \"intervals\"],\n",
    "                    label_key = \"readmission_label\",\n",
    "                    mode = \"binary\",\n",
    "                    n_heads=n_heads\n",
    "                ).to(device),\n",
    "                model_diag=BiteNet(\n",
    "                    dataset = dataset,\n",
    "                    feature_keys = [\"diagnoses\", \"procedures\", \"intervals\"],\n",
    "                    label_key = \"diagnosis_label\",\n",
    "                    mode = \"multilabel\",\n",
    "                    n_heads=n_heads\n",
    "                ).to(device),\n",
    "                df=n_heads_df,\n",
    "                row_fields={\n",
    "                    \"model_name\": \"bitenet\",\n",
    "                    \"feature_set\": \"dxtx\",\n",
    "                    \"seq_len\": seq_len,\n",
    "                    \"n_heads\": n_heads,\n",
    "                    \"trial\": trial\n",
    "                },\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                test_loader=test_loader,\n",
    "            )\n",
    "\n",
    "            n_heads_df = train_and_record_metrics(\n",
    "                model_readm=BiteNet(\n",
    "                    dataset = dataset,\n",
    "                    feature_keys = [\"diagnoses\", \"intervals\"],\n",
    "                    label_key = \"readmission_label\",\n",
    "                    mode = \"binary\",\n",
    "                    n_heads=n_heads\n",
    "                ).to(device),\n",
    "                model_diag=BiteNet(\n",
    "                    dataset = dataset,\n",
    "                    feature_keys = [\"diagnoses\", \"intervals\"],\n",
    "                    label_key = \"diagnosis_label\",\n",
    "                    mode = \"multilabel\",\n",
    "                    n_heads=n_heads\n",
    "                ).to(device),\n",
    "                df=n_heads_df,\n",
    "                row_fields={\n",
    "                    \"model_name\": \"bitenet\",\n",
    "                    \"feature_set\": \"dx\",\n",
    "                    \"seq_len\": seq_len,\n",
    "                    \"n_heads\": n_heads,\n",
    "                    \"trial\": trial\n",
    "                },\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                test_loader=test_loader\n",
    "            )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-06T12:33:11.500183Z",
     "end_time": "2023-05-06T12:33:25.329040Z"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
