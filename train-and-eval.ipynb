{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Library imports and data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-07T17:46:40.585581Z",
     "end_time": "2023-05-07T17:46:42.268578Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camer\\PycharmProjects\\CS598-DL4H-Final-Project\\venv\\lib\\site-packages\\pyhealth\\trainer.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    }
   ],
   "source": [
    "from pyhealth.data import Visit\n",
    "from pyhealth.datasets import split_by_patient, get_dataloader\n",
    "from pyhealth.trainer import Trainer\n",
    "from pyhealth.medcode import InnerMap\n",
    "from pyhealth.metrics.binary import binary_metrics_fn\n",
    "from sklearn.metrics import precision_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from typing import List, Dict\n",
    "from model.bitenet import BiteNet\n",
    "from model.baseline import RNN, BRNN, RETAIN, Deepr\n",
    "import pickle\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "RANDOM_SEED = 0\n",
    "BATCH_SIZE = 32\n",
    "KS = list(range(5, 31, 5))\n",
    "SEQ_LENS = list(range(6, 17, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with open(\"mimic3_dataset.pkl\", \"rb\") as dataset_file:\n",
    "    mimic3_ds = pickle.load(dataset_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-07T17:46:42.262578Z",
     "end_time": "2023-05-07T17:46:43.754582Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fgrnO7KkWDBY",
    "ExecuteTime": {
     "start_time": "2023-05-07T17:46:43.760579Z",
     "end_time": "2023-05-07T17:46:43.788582Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the tasks\n",
    "\n",
    "DIAGNOSES_KEY = \"conditions\"\n",
    "PROCEDURES_KEY = \"procedures\"\n",
    "INTERVAL_DAYS_KEY = \"days_since_first_visit\"\n",
    "\n",
    "icd9cm = InnerMap.load(\"ICD9CM\")\n",
    "\n",
    "def flatten(l: List):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def patient_level_readmission_prediction(patient, time_window: int = 30, max_length_visits: int = None):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    sorted_visits = sorted(patient, key=lambda visit: visit.encounter_time)\n",
    "\n",
    "    # Clip the patient visits to the most recent max_length_visits + 1 if max_length_visits is not None\n",
    "    if max_length_visits is not None:\n",
    "        n_visits = len(sorted_visits)\n",
    "        if n_visits > max_length_visits + 1:\n",
    "            sorted_visits = sorted_visits[n_visits - (max_length_visits + 1):]\n",
    "\n",
    "    feature_visits: List[Visit] = sorted_visits[:-1]\n",
    "    last_visit: Visit = sorted_visits[-1]\n",
    "    second_to_last_visit: Visit = feature_visits[-1]\n",
    "    first_visit: Visit = feature_visits[0]\n",
    "\n",
    "    # step 1 a: define readmission label\n",
    "    time_diff = (last_visit.encounter_time - second_to_last_visit.encounter_time).days\n",
    "    readmission_label = 1 if time_diff <= time_window else 0\n",
    "\n",
    "    # step 1 b: define diagnosis prediction label\n",
    "    diagnosis_label = list(set([icd9cm.get_ancestors(code)[1] for code in last_visit.get_code_list(\"DIAGNOSES_ICD\")]))\n",
    "\n",
    "    # step 2: obtain features\n",
    "    visits_diagnoses = []\n",
    "    visits_procedures = []\n",
    "    visits_intervals = []\n",
    "    for idx, visit in enumerate(feature_visits):\n",
    "        diagnoses = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        time_diff_from_first_visit = (visit.encounter_time - first_visit.encounter_time).days\n",
    "\n",
    "        # Exclude visits that are missing either diagnoses or procedures.\n",
    "        # BiteNet can handle missing procedures, but other PyHealth models like RNN\n",
    "        # require all features have a length greater than 0.\n",
    "        if len(diagnoses) == 0:\n",
    "            continue\n",
    "\n",
    "        visits_diagnoses.append(diagnoses)\n",
    "        visits_procedures.append(procedures)\n",
    "        visits_intervals.append([str(time_diff_from_first_visit)])\n",
    "\n",
    "    unique_diagnoses = list(set(flatten(visits_diagnoses)))\n",
    "\n",
    "    # step 3: exclusion criteria\n",
    "    if len(unique_diagnoses) == 0:\n",
    "        return []\n",
    "\n",
    "    # step 4: assemble the sample\n",
    "    samples.append(\n",
    "        {\n",
    "            \"patient_id\": patient.patient_id,\n",
    "            \"visit_id\": visit.visit_id,\n",
    "            \"diagnoses\": visits_diagnoses,\n",
    "            \"procedures\": visits_procedures,\n",
    "            \"intervals\": visits_intervals,\n",
    "            \"readmission_label\": readmission_label,\n",
    "            \"diagnosis_label\": diagnosis_label\n",
    "        }\n",
    "    )\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "RESULTS_FILE = \"baseline_comparison.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-07T17:46:43.787580Z",
     "end_time": "2023-05-07T17:46:43.800580Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def train_and_inference(model, train_loader, val_loader, test_loader, lr=0.0001, monitor=\"pr_auc\", optim = torch.optim.Adam):\n",
    "    trainer = Trainer(model=model, device=device)\n",
    "    trainer.train(\n",
    "        train_dataloader=train_loader,\n",
    "        val_dataloader=val_loader,\n",
    "        epochs=10,\n",
    "        monitor=monitor,\n",
    "        optimizer_class=optim,\n",
    "        optimizer_params = {\"lr\" : lr},\n",
    "        load_best_model_at_last=False\n",
    "    )\n",
    "\n",
    "    return trainer.inference(test_loader)\n",
    "\n",
    "def precision_at_k(y_true: np.ndarray, y_prob: np.ndarray):\n",
    "\n",
    "    y_pred: np.ndarray = (y_prob > 0.5).astype(int)\n",
    "    desc_idx: np.ndarray = np.flip(np.argsort(y_prob, axis=-1), axis=-1)\n",
    "\n",
    "    y_true = np.take(y_true, desc_idx).astype(int)\n",
    "    y_pred = np.take(y_pred, desc_idx)\n",
    "\n",
    "    precisions: List[float] = []\n",
    "    for k in KS:\n",
    "        precisions.append(\n",
    "            precision_score(y_true[:, :k].reshape(-1), y_pred[:, :k].reshape(-1))\n",
    "        )\n",
    "\n",
    "    precisions: Dict[str, float] = {\n",
    "        f\"precision@{k}\": p for k, p in zip(KS, precisions)\n",
    "    }\n",
    "    return precisions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-07T17:46:43.804578Z",
     "end_time": "2023-05-07T17:46:43.820603Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-07T17:46:43.820603Z",
     "end_time": "2023-05-07T17:46:43.832601Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_record_metrics(model_readm, model_diag, df, row_fields, train_loader, val_loader, test_loader, lr=0.0005, readm_optim=torch.optim.Adam, diag_optim=torch.optim.Adam):\n",
    "    y_true, y_prob, _ = train_and_inference(\n",
    "        model_readm,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        test_loader,\n",
    "        lr=lr,\n",
    "        optim=readm_optim\n",
    "    )\n",
    "    binary_metrics = binary_metrics_fn(y_true, y_prob, metrics=[\"pr_auc\", \"roc_auc\", \"f1\"])\n",
    "\n",
    "    y_true, y_prob, _ = train_and_inference(\n",
    "        model_diag,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        test_loader,\n",
    "        lr=lr,\n",
    "        monitor=\"pr_auc_samples\",\n",
    "        optim=diag_optim\n",
    "    )\n",
    "    precisions = precision_at_k(y_true, y_prob)\n",
    "\n",
    "    row = binary_metrics | precisions | row_fields\n",
    "    row = {\n",
    "        k: [v] for k, v in row.items()\n",
    "    }\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame.from_dict(row)], ignore_index=True)\n",
    "\n",
    "    # Save df for checkpoint\n",
    "    df.to_csv(RESULTS_FILE, index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (2): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (2): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (2): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000022A0AE10F10>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5292\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.2010\n",
      "roc_auc: 0.4879\n",
      "f1: 0.0000\n",
      "loss: 0.5238\n",
      "New best pr_auc score (0.2010) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.5244\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.2322\n",
      "roc_auc: 0.5570\n",
      "f1: 0.0000\n",
      "loss: 0.5294\n",
      "New best pr_auc score (0.2322) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.5180\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.2500\n",
      "roc_auc: 0.5641\n",
      "f1: 0.0000\n",
      "loss: 0.5116\n",
      "New best pr_auc score (0.2500) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.4983\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3119\n",
      "roc_auc: 0.6137\n",
      "f1: 0.0000\n",
      "loss: 0.5022\n",
      "New best pr_auc score (0.3119) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.4520\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3490\n",
      "roc_auc: 0.5955\n",
      "f1: 0.2065\n",
      "loss: 0.5032\n",
      "New best pr_auc score (0.3490) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.4067\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3593\n",
      "roc_auc: 0.6443\n",
      "f1: 0.3630\n",
      "loss: 0.5513\n",
      "New best pr_auc score (0.3593) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.3581\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3853\n",
      "roc_auc: 0.6413\n",
      "f1: 0.2537\n",
      "loss: 0.5180\n",
      "New best pr_auc score (0.3853) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.3224\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3828\n",
      "roc_auc: 0.6223\n",
      "f1: 0.3392\n",
      "loss: 0.6730\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.2796\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3826\n",
      "roc_auc: 0.6112\n",
      "f1: 0.3158\n",
      "loss: 0.6622\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.2441\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3637\n",
      "roc_auc: 0.6186\n",
      "f1: 0.3359\n",
      "loss: 0.7201\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (2): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (2): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (2): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000022A0AE10F10>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.1646\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.3156\n",
      "loss: 0.1155\n",
      "New best pr_auc_samples score (0.3156) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.1050\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.3200\n",
      "loss: 0.0981\n",
      "New best pr_auc_samples score (0.3200) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0923\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.3223\n",
      "loss: 0.0900\n",
      "New best pr_auc_samples score (0.3223) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0869\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.3263\n",
      "loss: 0.0867\n",
      "New best pr_auc_samples score (0.3263) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0842\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.3444\n",
      "loss: 0.0848\n",
      "New best pr_auc_samples score (0.3444) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0825\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.3510\n",
      "loss: 0.0841\n",
      "New best pr_auc_samples score (0.3510) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0812\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.3571\n",
      "loss: 0.0831\n",
      "New best pr_auc_samples score (0.3571) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0802\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.3619\n",
      "loss: 0.0827\n",
      "New best pr_auc_samples score (0.3619) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0794\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.3633\n",
      "loss: 0.0832\n",
      "New best pr_auc_samples score (0.3633) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0789\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.3688\n",
      "loss: 0.0826\n",
      "New best pr_auc_samples score (0.3688) at epoch-9, step-1880\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (2): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (2): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (2): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000022A0AE10F10>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5305\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.2004\n",
      "roc_auc: 0.4960\n",
      "f1: 0.0000\n",
      "loss: 0.5230\n",
      "New best pr_auc score (0.2004) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.5234\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.2430\n",
      "roc_auc: 0.5718\n",
      "f1: 0.0000\n",
      "loss: 0.5294\n",
      "New best pr_auc score (0.2430) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.5051\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3076\n",
      "roc_auc: 0.5535\n",
      "f1: 0.1429\n",
      "loss: 0.4968\n",
      "New best pr_auc score (0.3076) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.4835\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3397\n",
      "roc_auc: 0.5840\n",
      "f1: 0.1529\n",
      "loss: 0.4914\n",
      "New best pr_auc score (0.3397) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.4581\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3449\n",
      "roc_auc: 0.5859\n",
      "f1: 0.1429\n",
      "loss: 0.4956\n",
      "New best pr_auc score (0.3449) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.4332\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3619\n",
      "roc_auc: 0.6160\n",
      "f1: 0.2347\n",
      "loss: 0.5248\n",
      "New best pr_auc score (0.3619) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.4045\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.4073\n",
      "roc_auc: 0.6139\n",
      "f1: 0.2500\n",
      "loss: 0.5296\n",
      "New best pr_auc score (0.4073) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.3828\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3735\n",
      "roc_auc: 0.6093\n",
      "f1: 0.2000\n",
      "loss: 0.5444\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.3691\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3527\n",
      "roc_auc: 0.5832\n",
      "f1: 0.1889\n",
      "loss: 0.6051\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.3463\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.4140\n",
      "roc_auc: 0.6395\n",
      "f1: 0.3534\n",
      "loss: 0.5520\n",
      "New best pr_auc score (0.4140) at epoch-9, step-1880\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (2): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (2): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (2): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000022A0AE10F10>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.1702\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.3095\n",
      "loss: 0.1176\n",
      "New best pr_auc_samples score (0.3095) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.1076\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.3171\n",
      "loss: 0.1005\n",
      "New best pr_auc_samples score (0.3171) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0938\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.3240\n",
      "loss: 0.0905\n",
      "New best pr_auc_samples score (0.3240) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0872\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.3305\n",
      "loss: 0.0869\n",
      "New best pr_auc_samples score (0.3305) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0840\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.3482\n",
      "loss: 0.0847\n",
      "New best pr_auc_samples score (0.3482) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0820\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.3579\n",
      "loss: 0.0834\n",
      "New best pr_auc_samples score (0.3579) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0804\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.3640\n",
      "loss: 0.0831\n",
      "New best pr_auc_samples score (0.3640) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0793\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.3670\n",
      "loss: 0.0825\n",
      "New best pr_auc_samples score (0.3670) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0787\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.3707\n",
      "loss: 0.0831\n",
      "New best pr_auc_samples score (0.3707) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0779\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.3739\n",
      "loss: 0.0829\n",
      "New best pr_auc_samples score (0.3739) at epoch-9, step-1880\n",
      "RNN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (model): GRU(128, 128, batch_first=True, dropout=0.1)\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000022A0AE10F10>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5340\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.3490\n",
      "roc_auc: 0.6056\n",
      "f1: 0.1477\n",
      "loss: 0.4977\n",
      "New best pr_auc score (0.3490) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.4556\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3561\n",
      "roc_auc: 0.6236\n",
      "f1: 0.1547\n",
      "loss: 0.4879\n",
      "New best pr_auc score (0.3561) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.3939\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3586\n",
      "roc_auc: 0.6309\n",
      "f1: 0.1436\n",
      "loss: 0.4892\n",
      "New best pr_auc score (0.3586) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.3237\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3690\n",
      "roc_auc: 0.6477\n",
      "f1: 0.1622\n",
      "loss: 0.4900\n",
      "New best pr_auc score (0.3690) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.2476\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3499\n",
      "roc_auc: 0.6264\n",
      "f1: 0.1791\n",
      "loss: 0.5168\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.1835\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3495\n",
      "roc_auc: 0.6346\n",
      "f1: 0.1508\n",
      "loss: 0.5404\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.1296\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3421\n",
      "roc_auc: 0.6258\n",
      "f1: 0.1826\n",
      "loss: 0.5727\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0915\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3425\n",
      "roc_auc: 0.6233\n",
      "f1: 0.1600\n",
      "loss: 0.6266\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0650\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3391\n",
      "roc_auc: 0.6187\n",
      "f1: 0.1714\n",
      "loss: 0.6620\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0472\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3430\n",
      "roc_auc: 0.6234\n",
      "f1: 0.1650\n",
      "loss: 0.6822\n",
      "RNN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (model): GRU(128, 128, batch_first=True, dropout=0.1)\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000022A0AE10F10>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.2429\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.3217\n",
      "loss: 0.1067\n",
      "New best pr_auc_samples score (0.3217) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.0959\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.3227\n",
      "loss: 0.0972\n",
      "New best pr_auc_samples score (0.3227) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0899\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.3314\n",
      "loss: 0.0940\n",
      "New best pr_auc_samples score (0.3314) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0873\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.3343\n",
      "loss: 0.0921\n",
      "New best pr_auc_samples score (0.3343) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0857\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.3377\n",
      "loss: 0.0908\n",
      "New best pr_auc_samples score (0.3377) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0844\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.3454\n",
      "loss: 0.0900\n",
      "New best pr_auc_samples score (0.3454) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0834\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.3535\n",
      "loss: 0.0893\n",
      "New best pr_auc_samples score (0.3535) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0825\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.3595\n",
      "loss: 0.0887\n",
      "New best pr_auc_samples score (0.3595) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0816\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.3732\n",
      "loss: 0.0881\n",
      "New best pr_auc_samples score (0.3732) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0808\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.3783\n",
      "loss: 0.0878\n",
      "New best pr_auc_samples score (0.3783) at epoch-9, step-1880\n",
      "RNN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (model): GRU(128, 128, batch_first=True, dropout=0.1)\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000022A0AE10F10>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5415\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.3246\n",
      "roc_auc: 0.5575\n",
      "f1: 0.1469\n",
      "loss: 0.5113\n",
      "New best pr_auc score (0.3246) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.4637\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3345\n",
      "roc_auc: 0.5674\n",
      "f1: 0.1609\n",
      "loss: 0.5024\n",
      "New best pr_auc score (0.3345) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.4078\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3521\n",
      "roc_auc: 0.5940\n",
      "f1: 0.1676\n",
      "loss: 0.4962\n",
      "New best pr_auc score (0.3521) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.3444\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3631\n",
      "roc_auc: 0.6006\n",
      "f1: 0.2105\n",
      "loss: 0.5014\n",
      "New best pr_auc score (0.3631) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.2778\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3631\n",
      "roc_auc: 0.6024\n",
      "f1: 0.2051\n",
      "loss: 0.5172\n",
      "New best pr_auc score (0.3631) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.2140\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3772\n",
      "roc_auc: 0.6193\n",
      "f1: 0.2189\n",
      "loss: 0.5212\n",
      "New best pr_auc score (0.3772) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.1624\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3640\n",
      "roc_auc: 0.6090\n",
      "f1: 0.2547\n",
      "loss: 0.5597\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.1213\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3582\n",
      "roc_auc: 0.6008\n",
      "f1: 0.2275\n",
      "loss: 0.5959\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0904\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3697\n",
      "roc_auc: 0.6146\n",
      "f1: 0.2385\n",
      "loss: 0.6148\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0679\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3671\n",
      "roc_auc: 0.6095\n",
      "f1: 0.2330\n",
      "loss: 0.6642\n",
      "RNN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (model): GRU(128, 128, batch_first=True, dropout=0.1)\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000022A0AE10F10>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.2469\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.3199\n",
      "loss: 0.1108\n",
      "New best pr_auc_samples score (0.3199) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.0996\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.3249\n",
      "loss: 0.0992\n",
      "New best pr_auc_samples score (0.3249) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0920\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.3264\n",
      "loss: 0.0955\n",
      "New best pr_auc_samples score (0.3264) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0888\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.3355\n",
      "loss: 0.0935\n",
      "New best pr_auc_samples score (0.3355) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0867\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.3458\n",
      "loss: 0.0917\n",
      "New best pr_auc_samples score (0.3458) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0850\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.3557\n",
      "loss: 0.0903\n",
      "New best pr_auc_samples score (0.3557) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0836\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.3609\n",
      "loss: 0.0892\n",
      "New best pr_auc_samples score (0.3609) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0824\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.3680\n",
      "loss: 0.0884\n",
      "New best pr_auc_samples score (0.3680) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0815\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.3732\n",
      "loss: 0.0876\n",
      "New best pr_auc_samples score (0.3732) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0807\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.3779\n",
      "loss: 0.0871\n",
      "New best pr_auc_samples score (0.3779) at epoch-9, step-1880\n",
      "BRNN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (model): GRU(128, 128, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000022A0AE10F10>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5294\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.3467\n",
      "roc_auc: 0.5696\n",
      "f1: 0.1628\n",
      "loss: 0.5028\n",
      "New best pr_auc score (0.3467) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.4413\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3637\n",
      "roc_auc: 0.6043\n",
      "f1: 0.1818\n",
      "loss: 0.4904\n",
      "New best pr_auc score (0.3637) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.3576\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3763\n",
      "roc_auc: 0.6240\n",
      "f1: 0.2043\n",
      "loss: 0.4873\n",
      "New best pr_auc score (0.3763) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.2632\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3864\n",
      "roc_auc: 0.6224\n",
      "f1: 0.3000\n",
      "loss: 0.5029\n",
      "New best pr_auc score (0.3864) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.1769\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3808\n",
      "roc_auc: 0.6121\n",
      "f1: 0.3111\n",
      "loss: 0.5366\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.1133\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3848\n",
      "roc_auc: 0.6183\n",
      "f1: 0.2897\n",
      "loss: 0.5643\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0725\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3790\n",
      "roc_auc: 0.6094\n",
      "f1: 0.2844\n",
      "loss: 0.6100\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0475\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3913\n",
      "roc_auc: 0.6176\n",
      "f1: 0.3172\n",
      "loss: 0.6254\n",
      "New best pr_auc score (0.3913) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0314\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3882\n",
      "roc_auc: 0.6189\n",
      "f1: 0.3193\n",
      "loss: 0.6576\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0219\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3919\n",
      "roc_auc: 0.6200\n",
      "f1: 0.2920\n",
      "loss: 0.7123\n",
      "New best pr_auc score (0.3919) at epoch-9, step-1880\n",
      "RNN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (model): GRU(128, 128, batch_first=True, dropout=0.1)\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000022A0AE10F10>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.2387\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.3244\n",
      "loss: 0.1014\n",
      "New best pr_auc_samples score (0.3244) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.0943\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.3287\n",
      "loss: 0.0931\n",
      "New best pr_auc_samples score (0.3287) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0889\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.3352\n",
      "loss: 0.0902\n",
      "New best pr_auc_samples score (0.3352) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0864\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.3390\n",
      "loss: 0.0886\n",
      "New best pr_auc_samples score (0.3390) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0847\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.3446\n",
      "loss: 0.0873\n",
      "New best pr_auc_samples score (0.3446) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0834\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.3524\n",
      "loss: 0.0864\n",
      "New best pr_auc_samples score (0.3524) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0825\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.3618\n",
      "loss: 0.0857\n",
      "New best pr_auc_samples score (0.3618) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0816\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.3695\n",
      "loss: 0.0851\n",
      "New best pr_auc_samples score (0.3695) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0808\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.3754\n",
      "loss: 0.0845\n",
      "New best pr_auc_samples score (0.3754) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0801\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.3813\n",
      "loss: 0.0840\n",
      "New best pr_auc_samples score (0.3813) at epoch-9, step-1880\n",
      "BRNN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (model): GRU(128, 128, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000022A0AE10F10>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5308\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.3809\n",
      "roc_auc: 0.5865\n",
      "f1: 0.2099\n",
      "loss: 0.4974\n",
      "New best pr_auc score (0.3809) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.4410\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.4026\n",
      "roc_auc: 0.5988\n",
      "f1: 0.2186\n",
      "loss: 0.4915\n",
      "New best pr_auc score (0.4026) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.3657\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.4013\n",
      "roc_auc: 0.6122\n",
      "f1: 0.2404\n",
      "loss: 0.4899\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.2783\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.4154\n",
      "roc_auc: 0.6132\n",
      "f1: 0.2540\n",
      "loss: 0.5092\n",
      "New best pr_auc score (0.4154) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.1997\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.4100\n",
      "roc_auc: 0.6157\n",
      "f1: 0.3077\n",
      "loss: 0.5280\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.1381\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.4109\n",
      "roc_auc: 0.6141\n",
      "f1: 0.2843\n",
      "loss: 0.5721\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0944\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.4148\n",
      "roc_auc: 0.6184\n",
      "f1: 0.3363\n",
      "loss: 0.5832\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0649\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.4095\n",
      "roc_auc: 0.6209\n",
      "f1: 0.3208\n",
      "loss: 0.6503\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0458\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.4056\n",
      "roc_auc: 0.6156\n",
      "f1: 0.2938\n",
      "loss: 0.6732\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0328\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.4153\n",
      "roc_auc: 0.6198\n",
      "f1: 0.3286\n",
      "loss: 0.7122\n",
      "RNN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (model): GRU(128, 128, batch_first=True, dropout=0.1)\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000022A0AE10F10>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.2429\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.3253\n",
      "loss: 0.1126\n",
      "New best pr_auc_samples score (0.3253) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.0980\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.3305\n",
      "loss: 0.1002\n",
      "New best pr_auc_samples score (0.3305) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0904\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.3338\n",
      "loss: 0.0959\n",
      "New best pr_auc_samples score (0.3338) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0871\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.3380\n",
      "loss: 0.0935\n",
      "New best pr_auc_samples score (0.3380) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0851\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.3534\n",
      "loss: 0.0917\n",
      "New best pr_auc_samples score (0.3534) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0837\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.3615\n",
      "loss: 0.0905\n",
      "New best pr_auc_samples score (0.3615) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0826\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.3672\n",
      "loss: 0.0896\n",
      "New best pr_auc_samples score (0.3672) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0816\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.3707\n",
      "loss: 0.0890\n",
      "New best pr_auc_samples score (0.3707) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0808\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.3747\n",
      "loss: 0.0884\n",
      "New best pr_auc_samples score (0.3747) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0801\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.3799\n",
      "loss: 0.0880\n",
      "New best pr_auc_samples score (0.3799) at epoch-9, step-1880\n",
      "RETAIN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (model): RETAINLayer(\n",
      "    (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "    (alpha_gru): GRU(128, 128, batch_first=True)\n",
      "    (beta_gru): GRU(128, 128, batch_first=True)\n",
      "    (alpha_li): Linear(in_features=128, out_features=1, bias=True)\n",
      "    (beta_li): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000022A0AE10F10>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5897\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.2984\n",
      "roc_auc: 0.5677\n",
      "f1: 0.1176\n",
      "loss: 0.5346\n",
      "New best pr_auc score (0.2984) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.4538\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3189\n",
      "roc_auc: 0.5725\n",
      "f1: 0.1461\n",
      "loss: 0.5291\n",
      "New best pr_auc score (0.3189) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.3493\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3335\n",
      "roc_auc: 0.5821\n",
      "f1: 0.1837\n",
      "loss: 0.5485\n",
      "New best pr_auc score (0.3335) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.2486\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3344\n",
      "roc_auc: 0.5842\n",
      "f1: 0.1932\n",
      "loss: 0.5933\n",
      "New best pr_auc score (0.3344) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.1749\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3501\n",
      "roc_auc: 0.5944\n",
      "f1: 0.2304\n",
      "loss: 0.6428\n",
      "New best pr_auc score (0.3501) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.1195\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3505\n",
      "roc_auc: 0.5886\n",
      "f1: 0.2715\n",
      "loss: 0.7117\n",
      "New best pr_auc score (0.3505) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0830\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3476\n",
      "roc_auc: 0.5910\n",
      "f1: 0.2643\n",
      "loss: 0.7610\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0650\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3400\n",
      "roc_auc: 0.5874\n",
      "f1: 0.2358\n",
      "loss: 0.8260\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0536\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3350\n",
      "roc_auc: 0.5843\n",
      "f1: 0.2424\n",
      "loss: 0.8769\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0433\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3450\n",
      "roc_auc: 0.5886\n",
      "f1: 0.2445\n",
      "loss: 0.9213\n",
      "RETAIN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (model): RETAINLayer(\n",
      "    (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "    (alpha_gru): GRU(128, 128, batch_first=True)\n",
      "    (beta_gru): GRU(128, 128, batch_first=True)\n",
      "    (alpha_li): Linear(in_features=128, out_features=1, bias=True)\n",
      "    (beta_li): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000022A0AE10F10>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.2949\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.3068\n",
      "loss: 0.1308\n",
      "New best pr_auc_samples score (0.3068) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.1093\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.3421\n",
      "loss: 0.1053\n",
      "New best pr_auc_samples score (0.3421) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0928\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.3694\n",
      "loss: 0.0964\n",
      "New best pr_auc_samples score (0.3694) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0857\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.3825\n",
      "loss: 0.0920\n",
      "New best pr_auc_samples score (0.3825) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0817\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.3920\n",
      "loss: 0.0898\n",
      "New best pr_auc_samples score (0.3920) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0790\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.3959\n",
      "loss: 0.0884\n",
      "New best pr_auc_samples score (0.3959) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0770\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.4025\n",
      "loss: 0.0874\n",
      "New best pr_auc_samples score (0.4025) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0752\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.4052\n",
      "loss: 0.0868\n",
      "New best pr_auc_samples score (0.4052) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0738\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.4076\n",
      "loss: 0.0862\n",
      "New best pr_auc_samples score (0.4076) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0724\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.4110\n",
      "loss: 0.0859\n",
      "New best pr_auc_samples score (0.4110) at epoch-9, step-1880\n",
      "RETAIN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (model): RETAINLayer(\n",
      "    (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "    (alpha_gru): GRU(128, 128, batch_first=True)\n",
      "    (beta_gru): GRU(128, 128, batch_first=True)\n",
      "    (alpha_li): Linear(in_features=128, out_features=1, bias=True)\n",
      "    (beta_li): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000022A0AE10F10>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5870\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.3083\n",
      "roc_auc: 0.5398\n",
      "f1: 0.0854\n",
      "loss: 0.5445\n",
      "New best pr_auc score (0.3083) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.4639\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3251\n",
      "roc_auc: 0.5634\n",
      "f1: 0.1486\n",
      "loss: 0.5286\n",
      "New best pr_auc score (0.3251) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.3585\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3333\n",
      "roc_auc: 0.5778\n",
      "f1: 0.1613\n",
      "loss: 0.5449\n",
      "New best pr_auc score (0.3333) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.2620\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3387\n",
      "roc_auc: 0.5804\n",
      "f1: 0.2443\n",
      "loss: 0.5843\n",
      "New best pr_auc score (0.3387) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.1915\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3454\n",
      "roc_auc: 0.5932\n",
      "f1: 0.2047\n",
      "loss: 0.6157\n",
      "New best pr_auc score (0.3454) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.1369\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3482\n",
      "roc_auc: 0.5878\n",
      "f1: 0.2411\n",
      "loss: 0.6721\n",
      "New best pr_auc score (0.3482) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.1029\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3586\n",
      "roc_auc: 0.6009\n",
      "f1: 0.2297\n",
      "loss: 0.7173\n",
      "New best pr_auc score (0.3586) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0798\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3447\n",
      "roc_auc: 0.5895\n",
      "f1: 0.2489\n",
      "loss: 0.7794\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0608\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3411\n",
      "roc_auc: 0.5974\n",
      "f1: 0.2348\n",
      "loss: 0.8072\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0486\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3392\n",
      "roc_auc: 0.5903\n",
      "f1: 0.2432\n",
      "loss: 0.9054\n",
      "RETAIN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (model): RETAINLayer(\n",
      "    (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "    (alpha_gru): GRU(128, 128, batch_first=True)\n",
      "    (beta_gru): GRU(128, 128, batch_first=True)\n",
      "    (alpha_li): Linear(in_features=128, out_features=1, bias=True)\n",
      "    (beta_li): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000022A0AE10F10>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.3248\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.3149\n",
      "loss: 0.1288\n",
      "New best pr_auc_samples score (0.3149) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.1113\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.3493\n",
      "loss: 0.1049\n",
      "New best pr_auc_samples score (0.3493) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0957\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.3644\n",
      "loss: 0.0963\n",
      "New best pr_auc_samples score (0.3644) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0882\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.3821\n",
      "loss: 0.0916\n",
      "New best pr_auc_samples score (0.3821) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0836\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.3979\n",
      "loss: 0.0890\n",
      "New best pr_auc_samples score (0.3979) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0803\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.4031\n",
      "loss: 0.0874\n",
      "New best pr_auc_samples score (0.4031) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0780\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.4057\n",
      "loss: 0.0863\n",
      "New best pr_auc_samples score (0.4057) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0762\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.4117\n",
      "loss: 0.0855\n",
      "New best pr_auc_samples score (0.4117) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0745\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.4125\n",
      "loss: 0.0852\n",
      "New best pr_auc_samples score (0.4125) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0732\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.4172\n",
      "loss: 0.0848\n",
      "New best pr_auc_samples score (0.4172) at epoch-9, step-1880\n",
      "Deepr(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (model): DeeprLayer(\n",
      "    (conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000022A0AE10F10>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5418\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.3588\n",
      "roc_auc: 0.6125\n",
      "f1: 0.1657\n",
      "loss: 0.5033\n",
      "New best pr_auc score (0.3588) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.4446\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3343\n",
      "roc_auc: 0.6164\n",
      "f1: 0.2146\n",
      "loss: 0.5158\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.3812\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3505\n",
      "roc_auc: 0.5903\n",
      "f1: 0.2178\n",
      "loss: 0.5323\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.3202\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3446\n",
      "roc_auc: 0.5993\n",
      "f1: 0.2547\n",
      "loss: 0.5518\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.2674\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3568\n",
      "roc_auc: 0.5960\n",
      "f1: 0.3090\n",
      "loss: 0.5797\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.2213\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3689\n",
      "roc_auc: 0.5925\n",
      "f1: 0.2805\n",
      "loss: 0.6300\n",
      "New best pr_auc score (0.3689) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.1776\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3636\n",
      "roc_auc: 0.5963\n",
      "f1: 0.3004\n",
      "loss: 0.6512\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.1428\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3546\n",
      "roc_auc: 0.5994\n",
      "f1: 0.2918\n",
      "loss: 0.7049\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.1135\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3515\n",
      "roc_auc: 0.5909\n",
      "f1: 0.2655\n",
      "loss: 0.7576\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0892\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3637\n",
      "roc_auc: 0.5934\n",
      "f1: 0.2893\n",
      "loss: 0.8441\n",
      "Deepr(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (model): DeeprLayer(\n",
      "    (conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000022A0AE10F10>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.1821\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.3342\n",
      "loss: 0.1120\n",
      "New best pr_auc_samples score (0.3342) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.0986\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.3763\n",
      "loss: 0.0993\n",
      "New best pr_auc_samples score (0.3763) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0891\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.3913\n",
      "loss: 0.0933\n",
      "New best pr_auc_samples score (0.3913) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0842\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.4052\n",
      "loss: 0.0901\n",
      "New best pr_auc_samples score (0.4052) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0807\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.4102\n",
      "loss: 0.0881\n",
      "New best pr_auc_samples score (0.4102) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0781\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.4145\n",
      "loss: 0.0871\n",
      "New best pr_auc_samples score (0.4145) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0763\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.4185\n",
      "loss: 0.0863\n",
      "New best pr_auc_samples score (0.4185) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0745\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.4180\n",
      "loss: 0.0858\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0731\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.4209\n",
      "loss: 0.0854\n",
      "New best pr_auc_samples score (0.4209) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0718\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.4244\n",
      "loss: 0.0853\n",
      "New best pr_auc_samples score (0.4244) at epoch-9, step-1880\n",
      "Deepr(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (model): DeeprLayer(\n",
      "    (conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000022A0AE10F10>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5400\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.3458\n",
      "roc_auc: 0.5639\n",
      "f1: 0.1395\n",
      "loss: 0.5253\n",
      "New best pr_auc score (0.3458) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.4452\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3485\n",
      "roc_auc: 0.5715\n",
      "f1: 0.1724\n",
      "loss: 0.5306\n",
      "New best pr_auc score (0.3485) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.3963\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3625\n",
      "roc_auc: 0.6009\n",
      "f1: 0.2000\n",
      "loss: 0.5189\n",
      "New best pr_auc score (0.3625) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.3398\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3622\n",
      "roc_auc: 0.6100\n",
      "f1: 0.2439\n",
      "loss: 0.5407\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.2932\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3618\n",
      "roc_auc: 0.5854\n",
      "f1: 0.2312\n",
      "loss: 0.5679\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.2464\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3683\n",
      "roc_auc: 0.5968\n",
      "f1: 0.2176\n",
      "loss: 0.6075\n",
      "New best pr_auc score (0.3683) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.2050\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3560\n",
      "roc_auc: 0.5771\n",
      "f1: 0.2620\n",
      "loss: 0.6345\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.1692\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3528\n",
      "roc_auc: 0.5799\n",
      "f1: 0.2358\n",
      "loss: 0.6891\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.1381\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3705\n",
      "roc_auc: 0.5929\n",
      "f1: 0.2453\n",
      "loss: 0.7335\n",
      "New best pr_auc score (0.3705) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.1163\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3633\n",
      "roc_auc: 0.5802\n",
      "f1: 0.2545\n",
      "loss: 0.7982\n",
      "Deepr(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (model): DeeprLayer(\n",
      "    (conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000022A0AE10F10>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.1950\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.3399\n",
      "loss: 0.1118\n",
      "New best pr_auc_samples score (0.3399) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.1006\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.3745\n",
      "loss: 0.0992\n",
      "New best pr_auc_samples score (0.3745) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0903\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.3932\n",
      "loss: 0.0930\n",
      "New best pr_auc_samples score (0.3932) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0846\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.4031\n",
      "loss: 0.0900\n",
      "New best pr_auc_samples score (0.4031) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0812\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.4112\n",
      "loss: 0.0882\n",
      "New best pr_auc_samples score (0.4112) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0788\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.4156\n",
      "loss: 0.0872\n",
      "New best pr_auc_samples score (0.4156) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0770\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.4226\n",
      "loss: 0.0859\n",
      "New best pr_auc_samples score (0.4226) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0753\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.4239\n",
      "loss: 0.0853\n",
      "New best pr_auc_samples score (0.4239) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0738\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.4272\n",
      "loss: 0.0846\n",
      "New best pr_auc_samples score (0.4272) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0722\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.4271\n",
      "loss: 0.0847\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3432, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1359, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1726, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (2): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (2): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (2): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000022A09EF0130>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5300\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.2136\n",
      "roc_auc: 0.5113\n",
      "f1: 0.0000\n",
      "loss: 0.5220\n",
      "New best pr_auc score (0.2136) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.5238\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.2162\n",
      "roc_auc: 0.5162\n",
      "f1: 0.0000\n",
      "loss: 0.5207\n",
      "New best pr_auc score (0.2162) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.5209\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.2228\n",
      "roc_auc: 0.5383\n",
      "f1: 0.0000\n",
      "loss: 0.5160\n",
      "New best pr_auc score (0.2228) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.5058\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.2810\n",
      "roc_auc: 0.5147\n",
      "f1: 0.1628\n",
      "loss: 0.4971\n",
      "New best pr_auc score (0.2810) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.4762\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3061\n",
      "roc_auc: 0.5634\n",
      "f1: 0.2804\n",
      "loss: 0.5783\n",
      "New best pr_auc score (0.3061) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.4592\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3043\n",
      "roc_auc: 0.5599\n",
      "f1: 0.1848\n",
      "loss: 0.5131\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.4396\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3273\n",
      "roc_auc: 0.6030\n",
      "f1: 0.2000\n",
      "loss: 0.5172\n",
      "New best pr_auc score (0.3273) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.4116\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3433\n",
      "roc_auc: 0.5971\n",
      "f1: 0.2277\n",
      "loss: 0.5328\n",
      "New best pr_auc score (0.3433) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.3763\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3304\n",
      "roc_auc: 0.6123\n",
      "f1: 0.3071\n",
      "loss: 0.6579\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.3532\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3356\n",
      "roc_auc: 0.6370\n",
      "f1: 0.2739\n",
      "loss: 0.5731\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3432, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1359, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1726, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (2): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (2): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (2): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000022A09EF0130>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.1628\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.3149\n",
      "loss: 0.1147\n",
      "New best pr_auc_samples score (0.3149) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.1038\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.3196\n",
      "loss: 0.0967\n",
      "New best pr_auc_samples score (0.3196) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0909\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.3237\n",
      "loss: 0.0893\n",
      "New best pr_auc_samples score (0.3237) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0858\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.3345\n",
      "loss: 0.0858\n",
      "New best pr_auc_samples score (0.3345) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0832\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.3495\n",
      "loss: 0.0843\n",
      "New best pr_auc_samples score (0.3495) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0818\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.3555\n",
      "loss: 0.0834\n",
      "New best pr_auc_samples score (0.3555) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0807\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.3600\n",
      "loss: 0.0833\n",
      "New best pr_auc_samples score (0.3600) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0799\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.3633\n",
      "loss: 0.0830\n",
      "New best pr_auc_samples score (0.3633) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0792\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.3692\n",
      "loss: 0.0826\n",
      "New best pr_auc_samples score (0.3692) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0787\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.3711\n",
      "loss: 0.0826\n",
      "New best pr_auc_samples score (0.3711) at epoch-9, step-1880\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3432, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1726, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (2): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (2): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (2): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000022A09EF0130>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5329\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.2155\n",
      "roc_auc: 0.5097\n",
      "f1: 0.0000\n",
      "loss: 0.5226\n",
      "New best pr_auc score (0.2155) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.5224\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.2232\n",
      "roc_auc: 0.5351\n",
      "f1: 0.0000\n",
      "loss: 0.5169\n",
      "New best pr_auc score (0.2232) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.5238\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.2425\n",
      "roc_auc: 0.5596\n",
      "f1: 0.0000\n",
      "loss: 0.5162\n",
      "New best pr_auc score (0.2425) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.5050\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3302\n",
      "roc_auc: 0.5624\n",
      "f1: 0.2162\n",
      "loss: 0.4918\n",
      "New best pr_auc score (0.3302) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.4740\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3363\n",
      "roc_auc: 0.5612\n",
      "f1: 0.1868\n",
      "loss: 0.5041\n",
      "New best pr_auc score (0.3363) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.4447\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3565\n",
      "roc_auc: 0.5910\n",
      "f1: 0.2330\n",
      "loss: 0.5070\n",
      "New best pr_auc score (0.3565) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.4168\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3508\n",
      "roc_auc: 0.5678\n",
      "f1: 0.2054\n",
      "loss: 0.5689\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.4000\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3919\n",
      "roc_auc: 0.6034\n",
      "f1: 0.3070\n",
      "loss: 0.5126\n",
      "New best pr_auc score (0.3919) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.3756\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3649\n",
      "roc_auc: 0.5847\n",
      "f1: 0.3096\n",
      "loss: 0.5396\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.3595\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3704\n",
      "roc_auc: 0.5966\n",
      "f1: 0.2396\n",
      "loss: 0.5610\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3432, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1726, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (2): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (2): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (2): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0005}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000022A09EF0130>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.1640\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.3142\n",
      "loss: 0.1164\n",
      "New best pr_auc_samples score (0.3142) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.1057\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "\n",
    "# Compare BiteNet performance to baselines\n",
    "\n",
    "metrics_df = pd.DataFrame(columns=['model_name', 'feature_set', 'seq_len', 'pr_auc', 'roc_auc', 'f1'] + [f\"precision@{k}\" for k in KS])\n",
    "\n",
    "for seq_len in SEQ_LENS:\n",
    "\n",
    "    dataset = mimic3_ds.set_task(\n",
    "        task_fn=lambda p: patient_level_readmission_prediction(p, max_length_visits=seq_len)\n",
    "    )\n",
    "\n",
    "    train, val, test = split_by_patient(dataset, [0.8, 0.1, 0.1], seed=RANDOM_SEED)\n",
    "\n",
    "    train_loader = get_dataloader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = get_dataloader(val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = get_dataloader(test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    #################### BITENET ####################\n",
    "    metrics_df = train_and_record_metrics(\n",
    "        model_readm=BiteNet(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\", \"procedures\", \"intervals\"],\n",
    "            label_key = \"readmission_label\",\n",
    "            mode = \"binary\",\n",
    "        ).to(device),\n",
    "        model_diag=BiteNet(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\", \"procedures\", \"intervals\"],\n",
    "            label_key = \"diagnosis_label\",\n",
    "            mode = \"multilabel\",\n",
    "        ).to(device),\n",
    "        df=metrics_df,\n",
    "        row_fields={\n",
    "            \"model_name\": \"bitenet\",\n",
    "            \"feature_set\": \"dxtx\",\n",
    "            \"seq_len\": seq_len,\n",
    "        },\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "    )\n",
    "\n",
    "    metrics_df = train_and_record_metrics(\n",
    "        model_readm=BiteNet(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\", \"intervals\"],\n",
    "            label_key = \"readmission_label\",\n",
    "            mode = \"binary\",\n",
    "        ).to(device),\n",
    "        model_diag=BiteNet(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\", \"intervals\"],\n",
    "            label_key = \"diagnosis_label\",\n",
    "            mode = \"multilabel\",\n",
    "        ).to(device),\n",
    "        df=metrics_df,\n",
    "        row_fields={\n",
    "            \"model_name\": \"bitenet\",\n",
    "            \"feature_set\": \"dx\",\n",
    "            \"seq_len\": seq_len,\n",
    "        },\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader\n",
    "    )\n",
    "\n",
    "    #################### RNN ####################\n",
    "    metrics_df = train_and_record_metrics(\n",
    "            model_readm=RNN(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "            label_key = \"readmission_label\",\n",
    "            mode = \"binary\",\n",
    "        ).to(device),\n",
    "        model_diag=RNN(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "            label_key = \"diagnosis_label\",\n",
    "            mode = \"multilabel\",\n",
    "        ).to(device),\n",
    "        df=metrics_df,\n",
    "        row_fields={\n",
    "            \"model_name\": \"rnn\",\n",
    "            \"feature_set\": \"dxtx\",\n",
    "            \"seq_len\": seq_len,\n",
    "        },\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader\n",
    "    )\n",
    "\n",
    "    metrics_df = train_and_record_metrics(\n",
    "            model_readm=RNN(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\"],\n",
    "            label_key = \"readmission_label\",\n",
    "            mode = \"binary\",\n",
    "        ).to(device),\n",
    "        model_diag=RNN(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\"],\n",
    "            label_key = \"diagnosis_label\",\n",
    "            mode = \"multilabel\",\n",
    "        ).to(device),\n",
    "        df=metrics_df,\n",
    "        row_fields={\n",
    "            \"model_name\": \"rnn\",\n",
    "            \"feature_set\": \"dx\",\n",
    "            \"seq_len\": seq_len,\n",
    "        },\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "    )\n",
    "\n",
    "    #################### BRNN ####################\n",
    "    metrics_df = train_and_record_metrics(\n",
    "            model_readm=BRNN(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "            label_key = \"readmission_label\",\n",
    "            mode = \"binary\",\n",
    "            bidirectional=True\n",
    "        ).to(device),\n",
    "        model_diag=RNN(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "            label_key = \"diagnosis_label\",\n",
    "            mode = \"multilabel\",\n",
    "            bidirectional=True\n",
    "        ).to(device),\n",
    "        df=metrics_df,\n",
    "        row_fields={\n",
    "            \"model_name\": \"brnn\",\n",
    "            \"feature_set\": \"dxtx\",\n",
    "            \"seq_len\": seq_len,\n",
    "        },\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader\n",
    "    )\n",
    "\n",
    "    metrics_df = train_and_record_metrics(\n",
    "            model_readm=BRNN(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\"],\n",
    "            label_key = \"readmission_label\",\n",
    "            mode = \"binary\",\n",
    "            bidirectional=True\n",
    "        ).to(device),\n",
    "        model_diag=RNN(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\"],\n",
    "            label_key = \"diagnosis_label\",\n",
    "            mode = \"multilabel\",\n",
    "            bidirectional=True\n",
    "        ).to(device),\n",
    "        df=metrics_df,\n",
    "        row_fields={\n",
    "            \"model_name\": \"brnn\",\n",
    "            \"feature_set\": \"dx\",\n",
    "            \"seq_len\": seq_len,\n",
    "        },\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader\n",
    "    )\n",
    "\n",
    "    #################### RETAIN ####################\n",
    "    metrics_df = train_and_record_metrics(\n",
    "            model_readm=RETAIN(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "            label_key = \"readmission_label\",\n",
    "            mode = \"binary\"\n",
    "        ).to(device),\n",
    "        model_diag=RETAIN(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "            label_key = \"diagnosis_label\",\n",
    "            mode = \"multilabel\"\n",
    "        ).to(device),\n",
    "        df=metrics_df,\n",
    "        row_fields={\n",
    "            \"model_name\": \"retain\",\n",
    "            \"feature_set\": \"dxtx\",\n",
    "            \"seq_len\": seq_len,\n",
    "        },\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "    )\n",
    "\n",
    "    metrics_df = train_and_record_metrics(\n",
    "            model_readm=RETAIN(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\"],\n",
    "            label_key = \"readmission_label\",\n",
    "            mode = \"binary\"\n",
    "        ).to(device),\n",
    "        model_diag=RETAIN(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\"],\n",
    "            label_key = \"diagnosis_label\",\n",
    "            mode = \"multilabel\"\n",
    "        ).to(device),\n",
    "        df=metrics_df,\n",
    "        row_fields={\n",
    "            \"model_name\": \"retain\",\n",
    "            \"feature_set\": \"dx\",\n",
    "            \"seq_len\": seq_len,\n",
    "        },\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader\n",
    "    )\n",
    "\n",
    "    #################### Deepr ####################\n",
    "    metrics_df = train_and_record_metrics(\n",
    "            model_readm=Deepr(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "            label_key = \"readmission_label\",\n",
    "            mode = \"binary\"\n",
    "        ).to(device),\n",
    "        model_diag=Deepr(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "            label_key = \"diagnosis_label\",\n",
    "            mode = \"multilabel\"\n",
    "        ).to(device),\n",
    "        df=metrics_df,\n",
    "        row_fields={\n",
    "            \"model_name\": \"deepr\",\n",
    "            \"feature_set\": \"dxtx\",\n",
    "            \"seq_len\": seq_len,\n",
    "        },\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader\n",
    "    )\n",
    "\n",
    "    metrics_df = train_and_record_metrics(\n",
    "            model_readm=Deepr(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\"],\n",
    "            label_key = \"readmission_label\",\n",
    "            mode = \"binary\"\n",
    "        ).to(device),\n",
    "        model_diag=Deepr(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\"],\n",
    "            label_key = \"diagnosis_label\",\n",
    "            mode = \"multilabel\"\n",
    "        ).to(device),\n",
    "        df=metrics_df,\n",
    "        row_fields={\n",
    "            \"model_name\": \"deepr\",\n",
    "            \"feature_set\": \"dx\",\n",
    "            \"seq_len\": seq_len,\n",
    "        },\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-07T17:44:28.143689Z",
     "end_time": "2023-05-07T17:46:12.395877Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Evaluate BiteNet performance as number of MaskEnc layers changes\n",
    "\n",
    "RESULTS_FILE = \"./results/changing_n_layers.csv\"\n",
    "n_layers_df = pd.DataFrame(columns=['model_name', 'feature_set', 'seq_len', 'n_layers', 'trial', 'pr_auc', 'roc_auc', 'f1'] + [f\"precision@{k}\" for k in KS])\n",
    "\n",
    "for seq_len in SEQ_LENS:\n",
    "\n",
    "    dataset = mimic3_ds.set_task(\n",
    "        task_fn=lambda p: patient_level_readmission_prediction(p, max_length_visits=seq_len)\n",
    "    )\n",
    "\n",
    "    for n_layers in range(9):\n",
    "\n",
    "        train, val, test = split_by_patient(dataset, [0.8, 0.1, 0.1], seed=RANDOM_SEED)\n",
    "\n",
    "        train_loader = get_dataloader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader = get_dataloader(val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        test_loader = get_dataloader(test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        #################### BITENET ####################\n",
    "        n_layers_df = train_and_record_metrics(\n",
    "            model_readm=BiteNet(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\", \"procedures\", \"intervals\"],\n",
    "                label_key = \"readmission_label\",\n",
    "                mode = \"binary\",\n",
    "                n_mask_enc_layers=n_layers\n",
    "            ).to(device),\n",
    "            model_diag=BiteNet(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\", \"procedures\", \"intervals\"],\n",
    "                label_key = \"diagnosis_label\",\n",
    "                mode = \"multilabel\",\n",
    "                n_mask_enc_layers=n_layers\n",
    "            ).to(device),\n",
    "            df=n_layers_df,\n",
    "            row_fields={\n",
    "                \"model_name\": \"bitenet\",\n",
    "                \"feature_set\": \"dxtx\",\n",
    "                \"seq_len\": seq_len,\n",
    "                \"n_layers\": n_layers,\n",
    "            },\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader,\n",
    "        )\n",
    "\n",
    "        n_layers_df = train_and_record_metrics(\n",
    "            model_readm=BiteNet(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\", \"intervals\"],\n",
    "                label_key = \"readmission_label\",\n",
    "                mode = \"binary\",\n",
    "                n_mask_enc_layers=n_layers\n",
    "            ).to(device),\n",
    "            model_diag=BiteNet(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\", \"intervals\"],\n",
    "                label_key = \"diagnosis_label\",\n",
    "                mode = \"multilabel\",\n",
    "                n_mask_enc_layers=n_layers\n",
    "            ).to(device),\n",
    "            df=n_layers_df,\n",
    "            row_fields={\n",
    "                \"model_name\": \"bitenet\",\n",
    "                \"feature_set\": \"dx\",\n",
    "                \"seq_len\": seq_len,\n",
    "                \"n_layers\": n_layers,\n",
    "            },\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-07T17:25:46.063591Z",
     "end_time": "2023-05-07T17:29:52.636747Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Evaluate BiteNet performance as number of attention heads in MaskEnc layers changes\n",
    "\n",
    "RESULTS_FILE = \"./results/changing_n_heads.csv\"\n",
    "n_heads_df = pd.DataFrame(columns=['model_name', 'feature_set', 'seq_len', 'n_heads', 'trial', 'pr_auc', 'roc_auc', 'f1'] + [f\"precision@{k}\" for k in KS])\n",
    "\n",
    "for seq_len in SEQ_LENS:\n",
    "\n",
    "    dataset = mimic3_ds.set_task(\n",
    "        task_fn=lambda p: patient_level_readmission_prediction(p, max_length_visits=seq_len)\n",
    "    )\n",
    "\n",
    "    for n_heads in [4, 8, 16, 32]:\n",
    "\n",
    "        train, val, test = split_by_patient(dataset, [0.8, 0.1, 0.1], seed=RANDOM_SEED)\n",
    "\n",
    "        train_loader = get_dataloader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        val_loader = get_dataloader(val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        test_loader = get_dataloader(test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        #################### BITENET ####################\n",
    "        n_heads_df = train_and_record_metrics(\n",
    "            model_readm=BiteNet(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\", \"procedures\", \"intervals\"],\n",
    "                label_key = \"readmission_label\",\n",
    "                mode = \"binary\",\n",
    "                n_heads=n_heads\n",
    "            ).to(device),\n",
    "            model_diag=BiteNet(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\", \"procedures\", \"intervals\"],\n",
    "                label_key = \"diagnosis_label\",\n",
    "                mode = \"multilabel\",\n",
    "                n_heads=n_heads\n",
    "            ).to(device),\n",
    "            df=n_heads_df,\n",
    "            row_fields={\n",
    "                \"model_name\": \"bitenet\",\n",
    "                \"feature_set\": \"dxtx\",\n",
    "                \"seq_len\": seq_len,\n",
    "                \"n_heads\": n_heads,\n",
    "            },\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader,\n",
    "        )\n",
    "\n",
    "        n_heads_df = train_and_record_metrics(\n",
    "            model_readm=BiteNet(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\", \"intervals\"],\n",
    "                label_key = \"readmission_label\",\n",
    "                mode = \"binary\",\n",
    "                n_heads=n_heads\n",
    "            ).to(device),\n",
    "            model_diag=BiteNet(\n",
    "                dataset = dataset,\n",
    "                feature_keys = [\"diagnoses\", \"intervals\"],\n",
    "                label_key = \"diagnosis_label\",\n",
    "                mode = \"multilabel\",\n",
    "                n_heads=n_heads\n",
    "            ).to(device),\n",
    "            df=n_heads_df,\n",
    "            row_fields={\n",
    "                \"model_name\": \"bitenet\",\n",
    "                \"feature_set\": \"dx\",\n",
    "                \"seq_len\": seq_len,\n",
    "                \"n_heads\": n_heads,\n",
    "            },\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-06T12:33:11.500183Z",
     "end_time": "2023-05-06T12:33:25.329040Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Ablations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
