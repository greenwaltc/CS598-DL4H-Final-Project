{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "from google.cloud import storage\n",
    "from typing import List\n",
    "\n",
    "auth.authenticate_user()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "You are running on a Google Compute Engine virtual machine.\n",
      "It is recommended that you use service accounts for authentication.\n",
      "\n",
      "You can run:\n",
      "\n",
      "  $ gcloud config set account `ACCOUNT`\n",
      "\n",
      "to switch accounts if necessary.\n",
      "\n",
      "Your credentials may be visible to others with access to this\n",
      "virtual machine. Are you sure you want to authenticate with\n",
      "your personal account?\n",
      "\n",
      "Do you want to continue (Y/n)?  y\n",
      "\n",
      "Go to the following link in your browser:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=9selettxhNuAcOYNnV6ADAx5w9n0ED&prompt=consent&access_type=offline&code_challenge=C2xzR9dsPnWCE4uL_GGNAQbT5SIfSm-snga94csUH6I&code_challenge_method=S256\n",
      "\n",
      "Enter authorization code: 4/0AVHEtk5YJ53wZoAT_mzkFBHyyvDYWUc9Bd7zt1s0ZMgiQGPDkxWRzTPylrymepazxf3aZA\n",
      "\n",
      "You are now logged in as [camerongreenwalt@gmail.com].\n",
      "Your current project is [dl4h-final-project-383605].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n"
     ]
    }
   ],
   "source": [
    "! gcloud auth login"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! gcloud auth application-default login"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "! gcloud config set project dl4h-final-project-383605"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dG_aMKF4EYiQ",
    "outputId": "98c714c2-cea9-4353-e54e-5919147a623e"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "project_id = \"dl4h-final-project-383605\"\n",
    "\n",
    "storage_client = storage.Client(project=project_id)\n",
    "bucket = storage_client.bucket(\"mimiciii-1.4.physionet.org\")\n",
    "for blob in bucket.list_blobs():\n",
    "  if \"CHARTEVENTS\" in blob.name:\n",
    "    continue\n",
    "  blob.download_to_filename(blob.name)"
   ],
   "metadata": {
    "id": "pCHvVcifGcdr"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Extract all the files\n",
    "! gunzip *.gz"
   ],
   "metadata": {
    "id": "UcD9JqzFP91A"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyhealth in ./venv/lib/python3.8/site-packages (1.1.3)\r\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.8/site-packages (from pyhealth) (4.65.0)\r\n",
      "Requirement already satisfied: networkx>=2.6.3 in ./venv/lib/python3.8/site-packages (from pyhealth) (3.1)\r\n",
      "Requirement already satisfied: torch>=1.8.0 in ./venv/lib/python3.8/site-packages (from pyhealth) (2.0.0)\r\n",
      "Requirement already satisfied: pandas>=1.3.2 in ./venv/lib/python3.8/site-packages (from pyhealth) (2.0.0)\r\n",
      "Requirement already satisfied: scikit-learn>=0.24.2 in ./venv/lib/python3.8/site-packages (from pyhealth) (1.2.2)\r\n",
      "Requirement already satisfied: rdkit>=2022.03.4 in ./venv/lib/python3.8/site-packages (from pyhealth) (2022.9.5)\r\n",
      "Requirement already satisfied: numpy>=1.20.3 in ./venv/lib/python3.8/site-packages (from pandas>=1.3.2->pyhealth) (1.24.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.8/site-packages (from pandas>=1.3.2->pyhealth) (2023.3)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./venv/lib/python3.8/site-packages (from pandas>=1.3.2->pyhealth) (2023.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.8/site-packages (from pandas>=1.3.2->pyhealth) (2.8.2)\r\n",
      "Requirement already satisfied: Pillow in ./venv/lib/python3.8/site-packages (from rdkit>=2022.03.4->pyhealth) (9.5.0)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./venv/lib/python3.8/site-packages (from scikit-learn>=0.24.2->pyhealth) (1.2.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv/lib/python3.8/site-packages (from scikit-learn>=0.24.2->pyhealth) (3.1.0)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in ./venv/lib/python3.8/site-packages (from scikit-learn>=0.24.2->pyhealth) (1.10.1)\r\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.8/site-packages (from torch>=1.8.0->pyhealth) (3.11.0)\r\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.8/site-packages (from torch>=1.8.0->pyhealth) (3.1.2)\r\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.8/site-packages (from torch>=1.8.0->pyhealth) (1.11.1)\r\n",
      "Requirement already satisfied: typing-extensions in ./venv/lib/python3.8/site-packages (from torch>=1.8.0->pyhealth) (4.5.0)\r\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.2->pyhealth) (1.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.8/site-packages (from jinja2->torch>=1.8.0->pyhealth) (2.1.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.8/site-packages (from sympy->torch>=1.8.0->pyhealth) (1.3.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/pyhealth/trainer.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    }
   ],
   "source": [
    "! pip install pyhealth\n",
    "from pyhealth.datasets import MIMIC3Dataset\n",
    "from pyhealth.data import Patient, Visit\n",
    "import pandas as pd\n",
    "from pyhealth.datasets import split_by_patient, get_dataloader, BaseDataset\n",
    "from pyhealth.models import Transformer, BaseModel\n",
    "from pyhealth.trainer import Trainer\n",
    "from pyhealth.metrics.binary import binary_metrics_fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import functools\n",
    "\n",
    "data_root = \"/Users/cyg1122/Desktop/school/dl4h/mimic3/physionet.org/files/mimiciii/1.4/\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "mimic3_ds = MIMIC3Dataset(\n",
    "        root=data_root,\n",
    "        tables=[\"DIAGNOSES_ICD\", \"PROCEDURES_ICD\"],\n",
    "        dev=True\n",
    ")"
   ],
   "metadata": {
    "id": "s-VgeMdJDYYK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9cb9c08a-9003-4836-a3b2-a23e7cd45b55"
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing PATIENTS and ADMISSIONS: 100%|██████████| 1000/1000 [00:00<00:00, 2070.21it/s]\n",
      "Parsing DIAGNOSES_ICD: 100%|██████████| 58929/58929 [00:02<00:00, 27164.66it/s]\n",
      "Parsing PROCEDURES_ICD: 100%|██████████| 52243/52243 [00:01<00:00, 35764.72it/s]\n",
      "Mapping codes: 100%|██████████| 1000/1000 [00:00<00:00, 164160.63it/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "mimic3_ds.stat()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "HQdeU1TLU6m_",
    "outputId": "590623d6-2917-4155-bf0a-5d75a3979c25"
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics of base dataset (dev=True):\n",
      "\t- Dataset: MIMIC3Dataset\n",
      "\t- Number of patients: 1000\n",
      "\t- Number of visits: 1295\n",
      "\t- Number of visits per patient: 1.2950\n",
      "\t- Number of events per visit in DIAGNOSES_ICD: 9.3544\n",
      "\t- Number of events per visit in PROCEDURES_ICD: 4.3351\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nStatistics of base dataset (dev=True):\\n\\t- Dataset: MIMIC3Dataset\\n\\t- Number of patients: 1000\\n\\t- Number of visits: 1295\\n\\t- Number of visits per patient: 1.2950\\n\\t- Number of events per visit in DIAGNOSES_ICD: 9.3544\\n\\t- Number of events per visit in PROCEDURES_ICD: 4.3351\\n'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Find all diagnoses codes with less than five occurrences\n",
    "\n",
    "all_diag_codes = []\n",
    "all_proc_codes = []\n",
    "for patient_id, patient in mimic3_ds.patients.items():\n",
    "  for i in range(len(patient)):\n",
    "    visit: Visit = patient[i]\n",
    "    conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "    procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "    all_diag_codes.extend(conditions)\n",
    "    all_proc_codes.extend(procedures)\n",
    "\n",
    "codes = pd.Series(all_diag_codes)\n",
    "diag_code_counts = codes.value_counts()\n",
    "filtered_diag_codes = diag_code_counts[diag_code_counts > 4].index.values\n",
    "num_unique_diag_codes = len(filtered_diag_codes)\n",
    "\n",
    "unique_proc_codes = list(set(all_proc_codes))\n",
    "num_unique_proc_codes = len(unique_proc_codes)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2n_iXyWSWE-H",
    "outputId": "4f9561b5-c85e-48a7-b9e2-365f8afc241a"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "proc_code_to_index_map = {}\n",
    "diag_code_to_index_map = {}\n",
    "\n",
    "index = 0\n",
    "for proc_code in unique_proc_codes:\n",
    "    proc_code_to_index_map[proc_code] = index\n",
    "    index += 1\n",
    "\n",
    "index = 0\n",
    "for diag_code in filtered_diag_codes:\n",
    "    diag_code_to_index_map[diag_code] = index\n",
    "    index += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def proc_code_to_onehot(code, n_codes, code_to_index_map):\n",
    "    index = code_to_index_map[code]\n",
    "    vec = np.zeros(n_codes)\n",
    "    vec[index] = 1\n",
    "    return vec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the tasks\n",
    "\n",
    "def readmission_prediction_mimic3_fn_dx(patient: Patient, time_window=30):\n",
    "    if len(patient) < 2:\n",
    "        return []\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    # we will drop the last visit\n",
    "    for i in range(len(patient) - 1):\n",
    "        first_visit: Visit = patient[0]\n",
    "        current_visit: Visit = patient[i]\n",
    "        next_visit: Visit = patient[i + 1]\n",
    "\n",
    "        # get time difference between current visit and next visit\n",
    "        time_diff_from_last_visit = (next_visit.encounter_time - current_visit.encounter_time).days\n",
    "        time_diff_from_first_visit = (current_visit.encounter_time - first_visit.encounter_time).days\n",
    "        readmission_label = 1 if time_diff_from_last_visit < time_window else 0\n",
    "\n",
    "        # get num days since first visit\n",
    "\n",
    "        conditions = [c for c in current_visit.get_code_list(table=\"DIAGNOSES_ICD\") if c in filtered_diag_codes]\n",
    "        # exclude: visits without condition, procedure, or drug code\n",
    "        if len(conditions) == 0:\n",
    "            continue\n",
    "        samples.append(\n",
    "            {\n",
    "                \"visit_id\": current_visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                \"conditions\": conditions,\n",
    "                # \"days_since_first_visit\": [time_diff_from_first_visit],\n",
    "                \"label\": readmission_label,\n",
    "            }\n",
    "        )\n",
    "    # no cohort selection\n",
    "    return samples\n",
    "\n",
    "def readmission_prediction_mimic3_fn_dxtx(patient: Patient, time_window=30):\n",
    "    if len(patient) < 2:\n",
    "        return []\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    # we will drop the last visit\n",
    "    for i in range(len(patient) - 1):\n",
    "        first_visit: Visit = patient[0]\n",
    "        current_visit: Visit = patient[i]\n",
    "        next_visit: Visit = patient[i + 1]\n",
    "\n",
    "        # get time difference between current visit and next visit\n",
    "        time_diff = (next_visit.encounter_time - current_visit.encounter_time).days\n",
    "        time_diff_from_first_visit = (current_visit.encounter_time - first_visit.encounter_time).days\n",
    "        readmission_label = 1 if time_diff < time_window else 0\n",
    "\n",
    "        conditions = [c for c in current_visit.get_code_list(table=\"DIAGNOSES_ICD\") if c in filtered_diag_codes]\n",
    "        procedures = current_visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        # exclude: visits without condition, procedure, or drug code\n",
    "        if len(conditions) * len(procedures) == 0:\n",
    "            continue\n",
    "        samples.append(\n",
    "            {\n",
    "                \"visit_id\": current_visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                \"conditions\": conditions,\n",
    "                \"procedures\": procedures,\n",
    "                # \"days_since_first_visit\": [time_diff_from_first_visit],\n",
    "                \"label\": readmission_label,\n",
    "            }\n",
    "        )\n",
    "    # no cohort selection\n",
    "    return samples"
   ],
   "metadata": {
    "id": "fgrnO7KkWDBY"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "mimic3_dx = mimic3_ds.set_task(task_fn=readmission_prediction_mimic3_fn_dx)\n",
    "mimic3_dxtx = mimic3_ds.set_task(task_fn=readmission_prediction_mimic3_fn_dxtx)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "IMD0MvgA8e8Z",
    "outputId": "11b7f21f-9f3b-419c-ee21-69ec1e3e02fe"
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples for readmission_prediction_mimic3_fn_dx: 100%|██████████| 1000/1000 [00:00<00:00, 31526.88it/s]\n",
      "Generating samples for readmission_prediction_mimic3_fn_dxtx: 100%|██████████| 1000/1000 [00:00<00:00, 32165.89it/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of sample dataset:\n",
      "\t- Dataset: MIMIC3Dataset\n",
      "\t- Task: readmission_prediction_mimic3_fn_dx\n",
      "\t- Number of samples: 293\n",
      "\t- Number of patients: 168\n",
      "\t- Number of visits: 293\n",
      "\t- Number of visits per patient: 1.7440\n",
      "\t- conditions:\n",
      "\t\t- Number of conditions per sample: 10.5324\n",
      "\t\t- Number of unique conditions: 419\n",
      "\t\t- Distribution of conditions (Top-10): [('4019', 98), ('4280', 97), ('42731', 76), ('41401', 63), ('5849', 58), ('5856', 57), ('51881', 46), ('486', 43), ('5990', 42), ('25000', 42)]\n",
      "\t- label:\n",
      "\t\t- Number of label per sample: 1.0000\n",
      "\t\t- Number of unique label: 2\n",
      "\t\t- Distribution of label (Top-10): [(1, 149), (0, 144)]\n",
      "Statistics of sample dataset:\n",
      "\t- Dataset: MIMIC3Dataset\n",
      "\t- Task: readmission_prediction_mimic3_fn_dx\n",
      "\t- Number of samples: 293\n",
      "\t- Number of patients: 168\n",
      "\t- Number of visits: 293\n",
      "\t- Number of visits per patient: 1.7440\n",
      "\t- conditions:\n",
      "\t\t- Number of conditions per sample: 10.5324\n",
      "\t\t- Number of unique conditions: 419\n",
      "\t\t- Distribution of conditions (Top-10): [('4019', 98), ('4280', 97), ('42731', 76), ('41401', 63), ('5849', 58), ('5856', 57), ('51881', 46), ('486', 43), ('5990', 42), ('25000', 42)]\n",
      "\t- label:\n",
      "\t\t- Number of label per sample: 1.0000\n",
      "\t\t- Number of unique label: 2\n",
      "\t\t- Distribution of label (Top-10): [(1, 149), (0, 144)]\n"
     ]
    }
   ],
   "source": [
    "print(mimic3_dx.stat())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of sample dataset:\n",
      "\t- Dataset: MIMIC3Dataset\n",
      "\t- Task: readmission_prediction_mimic3_fn_dxtx\n",
      "\t- Number of samples: 262\n",
      "\t- Number of patients: 154\n",
      "\t- Number of visits: 262\n",
      "\t- Number of visits per patient: 1.7013\n",
      "\t- conditions:\n",
      "\t\t- Number of conditions per sample: 10.5878\n",
      "\t\t- Number of unique conditions: 412\n",
      "\t\t- Distribution of conditions (Top-10): [('4280', 86), ('4019', 85), ('42731', 69), ('41401', 60), ('5849', 53), ('5856', 51), ('51881', 45), ('25000', 38), ('486', 36), ('5990', 36)]\n",
      "\t- procedures:\n",
      "\t\t- Number of procedures per sample: 4.6908\n",
      "\t\t- Number of unique procedures: 273\n",
      "\t\t- Distribution of procedures (Top-10): [('3893', 101), ('9904', 65), ('3995', 63), ('9604', 51), ('966', 48), ('9672', 37), ('9671', 34), ('3891', 30), ('8856', 27), ('9915', 23)]\n",
      "\t- label:\n",
      "\t\t- Number of label per sample: 1.0000\n",
      "\t\t- Number of unique label: 2\n",
      "\t\t- Distribution of label (Top-10): [(1, 137), (0, 125)]\n",
      "Statistics of sample dataset:\n",
      "\t- Dataset: MIMIC3Dataset\n",
      "\t- Task: readmission_prediction_mimic3_fn_dxtx\n",
      "\t- Number of samples: 262\n",
      "\t- Number of patients: 154\n",
      "\t- Number of visits: 262\n",
      "\t- Number of visits per patient: 1.7013\n",
      "\t- conditions:\n",
      "\t\t- Number of conditions per sample: 10.5878\n",
      "\t\t- Number of unique conditions: 412\n",
      "\t\t- Distribution of conditions (Top-10): [('4280', 86), ('4019', 85), ('42731', 69), ('41401', 60), ('5849', 53), ('5856', 51), ('51881', 45), ('25000', 38), ('486', 36), ('5990', 36)]\n",
      "\t- procedures:\n",
      "\t\t- Number of procedures per sample: 4.6908\n",
      "\t\t- Number of unique procedures: 273\n",
      "\t\t- Distribution of procedures (Top-10): [('3893', 101), ('9904', 65), ('3995', 63), ('9604', 51), ('966', 48), ('9672', 37), ('9671', 34), ('3891', 30), ('8856', 27), ('9915', 23)]\n",
      "\t- label:\n",
      "\t\t- Number of label per sample: 1.0000\n",
      "\t\t- Number of unique label: 2\n",
      "\t\t- Distribution of label (Top-10): [(1, 137), (0, 125)]\n"
     ]
    }
   ],
   "source": [
    "print(mimic3_dxtx.stat())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# define the dataloaders\n",
    "def create_dataloaders(dataset, split=[0.8, 0.1, 0.1]):\n",
    "    train, val, test = split_by_patient(dataset, split)\n",
    "\n",
    "    # obtain train/val/test dataloader, they are <torch.data.DataLoader> object\n",
    "    train_loader = get_dataloader(train, batch_size=32, shuffle=True)\n",
    "    val_loader = get_dataloader(val, batch_size=32, shuffle=False)\n",
    "    test_loader = get_dataloader(test, batch_size=32, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader_dx, val_loader_dx, test_loader_dx = create_dataloaders(mimic3_dx)\n",
    "train_loader_dxtx, val_loader_dxtx, test_loader_dxtx = create_dataloaders(mimic3_dxtx)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# Define the models\n",
    "from enum import Enum\n",
    "\n",
    "class MaskDirection(Enum):\n",
    "    FORWARD = 'forward'\n",
    "    BACKWARD = 'backward'\n",
    "    DIAGONAL = 'diagonal'\n",
    "    NONE = 'none'\n",
    "\n",
    "class MaskedLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape: int):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.parameter.Parameter(torch.randn(normalized_shape))\n",
    "        self.beta = nn.parameter.Parameter(torch.randn(normalized_shape))\n",
    "        self.eps = 1e-5\n",
    "        self.normalized_shape = normalized_shape\n",
    "\n",
    "    def forward(self, x: torch.Tensor, key_padding_mask: Optional[torch.Tensor] = None):\n",
    "        print(x.shape)\n",
    "        print(key_padding_mask.shape)\n",
    "        n = torch.sum(torch.ones_like(x) * (~key_padding_mask).int().unsqueeze(-1).expand(-1, -1, self.normalized_shape), dim=-1)\n",
    "        print(n.shape)\n",
    "        print(n)\n",
    "\n",
    "        # expected_val = torch.nanmean(x, dim=1)\n",
    "        # variance = torch.nansum(())\n",
    "\n",
    "# todo: add temporal encoding\n",
    "# todo: implement layer normalization\n",
    "# todo: implement fully connected network structure and linear layer activation functions\n",
    "class MaskEnc(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, num_heads: int, dropout: float = 0.1, batch_first: bool = True, temporal_mask_direction: MaskDirection = MaskDirection.NONE):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.temporal_mask_direction = temporal_mask_direction\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=embedding_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=batch_first\n",
    "        )\n",
    "\n",
    "        # self.masked_layer_norm1 = MaskedLayerNorm(embedding_dim)\n",
    "        # self.masked_layer_norm2 = MaskedLayerNorm(embedding_dim)\n",
    "\n",
    "        self.fc = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, key_padding_mask: Optional[torch.Tensor] = None):\n",
    "        attn_mask = self._make_temporal_mask(x.shape[1])\n",
    "\n",
    "        attn_output, attn_output_weights = self.attention(x, x, x, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        attn_output = torch.nan_to_num(attn_output, nan=-1e9)\n",
    "\n",
    "        # x = self.masked_layer_norm1(x + attn_output, key_padding_mask)\n",
    "        # x = self.masked_layer_norm2(x + self.fc(x), key_padding_mask)\n",
    "\n",
    "        x = self.dropout(self.relu(self.fc(attn_output)))\n",
    "        return x\n",
    "\n",
    "    def _make_temporal_mask(self, n: int) -> Optional[torch.Tensor]:\n",
    "        if self.temporal_mask_direction == MaskDirection.NONE:\n",
    "            return None\n",
    "\n",
    "        mask = torch.ones(n,n)\n",
    "        if self.temporal_mask_direction == MaskDirection.FORWARD:\n",
    "            mask = torch.tril(mask)\n",
    "        if self.temporal_mask_direction == MaskDirection.BACKWARD:\n",
    "            mask = torch.triu(mask)\n",
    "        if self.temporal_mask_direction == MaskDirection.DIAGONAL:\n",
    "            mask = mask.fill_diagonal_(0)\n",
    "\n",
    "        return mask.bool()\n",
    "\n",
    "class BiteNet(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, output_dim: int, num_heads: int, dropout: float = 0.1, batch_first: bool = True):\n",
    "        super().__init__()\n",
    "\n",
    "        def _make_mask_enc_block(temporal_mask_direction: MaskDirection = MaskDirection.NONE):\n",
    "            return MaskEnc(\n",
    "                embedding_dim = embedding_dim,\n",
    "                num_heads = num_heads,\n",
    "                dropout = dropout,\n",
    "                batch_first = batch_first,\n",
    "                temporal_mask_direction = temporal_mask_direction\n",
    "            )\n",
    "\n",
    "        self.code_level_attn = _make_mask_enc_block(MaskDirection.DIAGONAL)\n",
    "        self.visit_level_attn_forward = _make_mask_enc_block(MaskDirection.FORWARD)\n",
    "        self.visit_level_attn_backward = _make_mask_enc_block(MaskDirection.BACKWARD)\n",
    "        self.fc1 = nn.Linear(2*embedding_dim, output_dim)\n",
    "        self.fc2 = nn.Linear(embedding_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        code_attn = self.code_level_attn(x, key_padding_mask)\n",
    "\n",
    "        u_fw = self.visit_level_attn_forward(code_attn, key_padding_mask)\n",
    "        u_fw = u_fw.nansum(dim=1).squeeze()\n",
    "\n",
    "        u_bw = self.visit_level_attn_backward(code_attn, key_padding_mask)\n",
    "        u_bw = u_bw.nansum(dim=1).squeeze()\n",
    "\n",
    "        u_bi = torch.cat([u_fw, u_bw], dim=-1)\n",
    "        u_bi = torch.nan_to_num(u_bi)\n",
    "        s = self.relu(self.fc1(u_bi))\n",
    "        # logits = self.fc2(s)\n",
    "        return s\n",
    "\n",
    "class PyHealthBiteNet(BaseModel):\n",
    "    def __init__(self, dataset: BaseDataset, feature_keys: List[str], label_key: str, mode: str,\n",
    "                 embedding_dim: int = 128, num_heads: int = 8, dropout: float = 0.1, batch_first: bool = True, **kwargs):\n",
    "        super().__init__(dataset, feature_keys, label_key, mode)\n",
    "\n",
    "        # Any BaseModel should have these attributes, as functions like add_feature_transform_layer uses them\n",
    "        self.feat_tokenizers = {}\n",
    "        self.embeddings = nn.ModuleDict()\n",
    "        self.linear_layers = nn.ModuleDict()\n",
    "        self.label_tokenizer = self.get_label_tokenizer()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # self.add_feature_transform_layer will create a transformation layer for each feature\n",
    "        for feature_key in self.feature_keys:\n",
    "            input_info = self.dataset.input_info[feature_key]\n",
    "            self.add_feature_transform_layer(\n",
    "                feature_key, input_info, special_tokens=[\"<pad>\", \"<unk>\", \"<gap>\"]\n",
    "            )\n",
    "\n",
    "        # final output layer\n",
    "        output_size = self.get_output_size(self.label_tokenizer)\n",
    "        self.bite_net = BiteNet(\n",
    "            embedding_dim = embedding_dim,\n",
    "            output_dim = output_size,\n",
    "            num_heads = num_heads,\n",
    "            dropout = dropout,\n",
    "            batch_first = batch_first\n",
    "        )\n",
    "\n",
    "        # self.fc = nn.Linear(len(self.feature_keys) * hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:\n",
    "        patient_emb = []\n",
    "        patient_mask = []\n",
    "        for feature_key in self.feature_keys:\n",
    "\n",
    "            feature_vals = kwargs[feature_key]\n",
    "            x = self.feat_tokenizers[feature_key].batch_encode_2d(feature_vals, padding=True, truncation=False)\n",
    "            x = torch.tensor(x, dtype=torch.long, device=self.device)\n",
    "            pad_idx = self.feat_tokenizers[feature_key].vocabulary(\"<pad>\")\n",
    "\n",
    "            #create the mask\n",
    "            mask = (x != pad_idx).bool()\n",
    "\n",
    "            embeds = self.embeddings[feature_key](x)\n",
    "            patient_emb.append(embeds)\n",
    "            patient_mask.append(mask)\n",
    "\n",
    "        # (patient, features * hidden_dim)\n",
    "        patient_emb = torch.cat(patient_emb, dim=1)\n",
    "        patient_mask = ~(torch.cat(patient_mask, dim=1))\n",
    "\n",
    "        # (patient, label_size)\n",
    "        logits = self.bite_net(patient_emb, patient_mask)\n",
    "        print(logits)\n",
    "        # obtain y_true, loss, y_prob\n",
    "        y_true = self.prepare_labels(kwargs[self.label_key], self.label_tokenizer)\n",
    "        loss = self.get_loss_function()(logits, y_true)\n",
    "        y_prob = self.prepare_y_prob(logits)\n",
    "        return {\"loss\": loss, \"y_prob\": y_prob, \"y_true\": y_true}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "model_dxtx = PyHealthBiteNet(\n",
    "    dataset = mimic3_dxtx,\n",
    "    feature_keys = [\"conditions\", \"procedures\"],\n",
    "    label_key = \"label\",\n",
    "    mode = \"binary\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [      0.],\n",
      "        [3010752.],\n",
      "        [      0.]], grad_fn=<ReluBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'loss': tensor(94086.6719, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n 'y_prob': tensor([[0.5000],\n         [0.5000],\n         [0.5000],\n         [0.5000],\n         [0.5000],\n         [0.5000],\n         [0.5000],\n         [0.5000],\n         [0.5000],\n         [0.5000],\n         [0.5000],\n         [0.5000],\n         [0.5000],\n         [0.5000],\n         [0.5000],\n         [0.5000],\n         [0.5000],\n         [0.5000],\n         [0.5000],\n         [0.5000],\n         [0.5000],\n         [0.5000],\n         [0.5000],\n         [0.5000],\n         [0.5000],\n         [0.5000],\n         [0.5000],\n         [0.5000],\n         [0.5000],\n         [0.5000],\n         [1.0000],\n         [0.5000]], grad_fn=<SigmoidBackward0>),\n 'y_true': tensor([[1.],\n         [0.],\n         [1.],\n         [0.],\n         [0.],\n         [0.],\n         [1.],\n         [0.],\n         [0.],\n         [1.],\n         [0.],\n         [0.],\n         [1.],\n         [1.],\n         [1.],\n         [1.],\n         [1.],\n         [1.],\n         [0.],\n         [1.],\n         [0.],\n         [1.],\n         [0.],\n         [1.],\n         [0.],\n         [1.],\n         [1.],\n         [0.],\n         [1.],\n         [1.],\n         [0.],\n         [0.]])}"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dxtx(**next(iter(train_loader_dxtx)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyHealthBiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (conditions): Embedding(415, 128, padding_idx=0)\n",
      "    (procedures): Embedding(276, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): BiteNet(\n",
      "    (code_level_attn): MaskEnc(\n",
      "      (attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (visit_level_attn_forward): MaskEnc(\n",
      "      (attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (visit_level_attn_backward): MaskEnc(\n",
      "      (attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (fc1): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    (sigmoid): Sigmoid()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (relu): ReLU()\n",
      "  )\n",
      ")\n",
      "Metrics: None\n",
      "Device: cpu\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x176ca30d0>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 5\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 0 / 5:   0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d43d720686e64605ac6b3a7813a40daa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-7 ---\n",
      "loss: 0.6901\n",
      "Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 112.97it/s]\n",
      "--- Eval epoch-0, step-7 ---\n",
      "pr_auc: 0.5417\n",
      "roc_auc: 0.5000\n",
      "f1: 0.7027\n",
      "loss: 0.6931\n",
      "New best pr_auc score (0.5417) at epoch-0, step-7\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 1 / 5:   0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c0afb1e20364056ae4da848a8380401"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "--- Train epoch-1, step-14 ---\n",
      "loss: 0.6931\n",
      "Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 98.91it/s]\n",
      "--- Eval epoch-1, step-14 ---\n",
      "pr_auc: 0.5417\n",
      "roc_auc: 0.5000\n",
      "f1: 0.7027\n",
      "loss: 0.6931\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 2 / 5:   0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a567428192f64ff993a8092ff091ae05"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "--- Train epoch-2, step-21 ---\n",
      "loss: 0.6931\n",
      "Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 114.17it/s]\n",
      "--- Eval epoch-2, step-21 ---\n",
      "pr_auc: 0.5417\n",
      "roc_auc: 0.5000\n",
      "f1: 0.7027\n",
      "loss: 0.6931\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 3 / 5:   0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b5739145a484765bdca267fda8a124e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "--- Train epoch-3, step-28 ---\n",
      "loss: 0.6931\n",
      "Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 98.69it/s]\n",
      "--- Eval epoch-3, step-28 ---\n",
      "pr_auc: 0.5417\n",
      "roc_auc: 0.5000\n",
      "f1: 0.7027\n",
      "loss: 0.6931\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 4 / 5:   0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5dc0ab2733714a189ee899f42e4bc548"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "--- Train epoch-4, step-35 ---\n",
      "loss: 0.6931\n",
      "Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 101.18it/s]\n",
      "--- Eval epoch-4, step-35 ---\n",
      "pr_auc: 0.5417\n",
      "roc_auc: 0.5000\n",
      "f1: 0.7027\n",
      "loss: 0.6931\n",
      "Loaded best model\n"
     ]
    }
   ],
   "source": [
    "trainer_dxtx = Trainer(model=model_dxtx)\n",
    "trainer_dxtx.train(\n",
    "    train_dataloader=train_loader_dxtx,\n",
    "    val_dataloader=val_loader_dxtx,\n",
    "    epochs=5,\n",
    "    monitor=\"pr_auc\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 33/33 [00:00<00:00, 367.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pr_auc': 0.726784508178901, 'roc_auc': 0.685080036129632, 'f1': 0.7090909090909092, 'loss': 0.6331828191424861}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 33/33 [00:00<00:00, 363.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'pr_auc': 0.726784508178901}"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# option 1: use our built-in evaluation metric\n",
    "score_dxtx = trainer_dxtx.evaluate(test_loader_dxtx)\n",
    "print (score_dxtx)\n",
    "\n",
    "# option 2: use our pyhealth.metrics to evaluate\n",
    "y_true_dxtx, y_prob_dxtx, loss_dxtx = trainer_dxtx.inference(test_loader_dxtx)\n",
    "binary_metrics_fn(y_true_dxtx, y_prob_dxtx, metrics=[\"pr_auc\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
