{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Google Cloud authentication and data download"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "from google.cloud import storage\n",
    "\n",
    "auth.authenticate_user()\n",
    "! gcloud auth login\n",
    "! gcloud auth application-default login\n",
    "! gcloud config set project dl4h-final-project-383605\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Replace these values with your project and bucket as needed\n",
    "project_id = \"dl4h-final-project-383605\"\n",
    "mimic3_bucket = \"mimiciii-1.4.physionet.org\"\n",
    "\n",
    "storage_client = storage.Client(project=project_id)\n",
    "bucket = storage_client.bucket(mimic3_bucket)\n",
    "for blob in bucket.list_blobs():\n",
    "  if \"CHARTEVENTS\" in blob.name:\n",
    "    continue\n",
    "  blob.download_to_filename(blob.name)\n",
    "\n",
    "# Extract all the files\n",
    "! gunzip *.gz"
   ],
   "metadata": {
    "id": "pCHvVcifGcdr"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Library imports and data preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyhealth in ./venv/lib/python3.8/site-packages (1.1.3)\r\n",
      "Requirement already satisfied: rdkit>=2022.03.4 in ./venv/lib/python3.8/site-packages (from pyhealth) (2022.9.5)\r\n",
      "Requirement already satisfied: scikit-learn>=0.24.2 in ./venv/lib/python3.8/site-packages (from pyhealth) (1.2.2)\r\n",
      "Requirement already satisfied: pandas>=1.3.2 in ./venv/lib/python3.8/site-packages (from pyhealth) (2.0.0)\r\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.8/site-packages (from pyhealth) (4.65.0)\r\n",
      "Requirement already satisfied: torch>=1.8.0 in ./venv/lib/python3.8/site-packages (from pyhealth) (2.0.0)\r\n",
      "Requirement already satisfied: networkx>=2.6.3 in ./venv/lib/python3.8/site-packages (from pyhealth) (3.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.8/site-packages (from pandas>=1.3.2->pyhealth) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.8/site-packages (from pandas>=1.3.2->pyhealth) (2023.3)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./venv/lib/python3.8/site-packages (from pandas>=1.3.2->pyhealth) (2023.3)\r\n",
      "Requirement already satisfied: numpy>=1.20.3 in ./venv/lib/python3.8/site-packages (from pandas>=1.3.2->pyhealth) (1.24.2)\r\n",
      "Requirement already satisfied: Pillow in ./venv/lib/python3.8/site-packages (from rdkit>=2022.03.4->pyhealth) (9.5.0)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./venv/lib/python3.8/site-packages (from scikit-learn>=0.24.2->pyhealth) (1.2.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv/lib/python3.8/site-packages (from scikit-learn>=0.24.2->pyhealth) (3.1.0)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in ./venv/lib/python3.8/site-packages (from scikit-learn>=0.24.2->pyhealth) (1.10.1)\r\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.8/site-packages (from torch>=1.8.0->pyhealth) (3.1.2)\r\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.8/site-packages (from torch>=1.8.0->pyhealth) (1.11.1)\r\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.8/site-packages (from torch>=1.8.0->pyhealth) (3.11.0)\r\n",
      "Requirement already satisfied: typing-extensions in ./venv/lib/python3.8/site-packages (from torch>=1.8.0->pyhealth) (4.5.0)\r\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.2->pyhealth) (1.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.8/site-packages (from jinja2->torch>=1.8.0->pyhealth) (2.1.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.8/site-packages (from sympy->torch>=1.8.0->pyhealth) (1.3.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/pyhealth/trainer.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    }
   ],
   "source": [
    "! pip install pyhealth\n",
    "from pyhealth.datasets import MIMIC3Dataset\n",
    "from pyhealth.data import Patient, Visit\n",
    "import pandas as pd\n",
    "from pyhealth.datasets import split_by_patient, get_dataloader, BaseDataset\n",
    "from pyhealth.models import BaseModel\n",
    "from pyhealth.trainer import Trainer\n",
    "from pyhealth.metrics.binary import binary_metrics_fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional\n",
    "from enum import Enum\n",
    "\n",
    "# Set this to the directory with all MIMIC-3 dataset files\n",
    "data_root = \"/Users/cyg1122/Desktop/school/dl4h/mimic3/physionet.org/files/mimiciii/1.4/\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the dataset\n",
    "\n",
    "mimic3_ds = MIMIC3Dataset(\n",
    "        root=data_root,\n",
    "        tables=[\"DIAGNOSES_ICD\", \"PROCEDURES_ICD\"],\n",
    "        dev=False\n",
    ")"
   ],
   "metadata": {
    "id": "s-VgeMdJDYYK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9cb9c08a-9003-4836-a3b2-a23e7cd45b55"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Print dataset statistics\n",
    "\n",
    "mimic3_ds.stat()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "HQdeU1TLU6m_",
    "outputId": "590623d6-2917-4155-bf0a-5d75a3979c25"
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics of base dataset (dev=False):\n",
      "\t- Dataset: MIMIC3Dataset\n",
      "\t- Number of patients: 46520\n",
      "\t- Number of visits: 58976\n",
      "\t- Number of visits per patient: 1.2678\n",
      "\t- Number of events per visit in DIAGNOSES_ICD: 11.0384\n",
      "\t- Number of events per visit in PROCEDURES_ICD: 4.0711\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nStatistics of base dataset (dev=False):\\n\\t- Dataset: MIMIC3Dataset\\n\\t- Number of patients: 46520\\n\\t- Number of visits: 58976\\n\\t- Number of visits per patient: 1.2678\\n\\t- Number of events per visit in DIAGNOSES_ICD: 11.0384\\n\\t- Number of events per visit in PROCEDURES_ICD: 4.0711\\n'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Find all diagnoses codes\n",
    "# Find all procedure codes\n",
    "# Remove diagnoses codes with fewer than 5 occurences in the dataset\n",
    "\n",
    "all_diag_codes = []\n",
    "all_proc_codes = []\n",
    "for patient_id, patient in mimic3_ds.patients.items():\n",
    "  for i in range(len(patient)):\n",
    "    visit: Visit = patient[i]\n",
    "    conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "    procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "    all_diag_codes.extend(conditions)\n",
    "    all_proc_codes.extend(procedures)\n",
    "\n",
    "codes = pd.Series(all_diag_codes)\n",
    "diag_code_counts = codes.value_counts()\n",
    "filtered_diag_codes = diag_code_counts[diag_code_counts > 4].index.values\n",
    "num_unique_diag_codes = len(filtered_diag_codes)\n",
    "\n",
    "unique_proc_codes = list(set(all_proc_codes))\n",
    "num_unique_proc_codes = len(unique_proc_codes)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2n_iXyWSWE-H",
    "outputId": "4f9561b5-c85e-48a7-b9e2-365f8afc241a"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the tasks\n",
    "\n",
    "DIAGNOSES_KEY = \"conditions\"\n",
    "PROCEDURES_KEY = \"procedures\"\n",
    "INTERVAL_DAYS_KEY = \"days_since_first_visit\"\n",
    "\n",
    "# Readmission prediction for dataset consisting of diagnoses and procedure codes\n",
    "def readmission_prediction_mimic3_fn(patient: Patient, time_window=30):\n",
    "    if len(patient) < 2:\n",
    "        return []\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    # we will drop the last visit\n",
    "    for i in range(len(patient) - 1):\n",
    "        first_visit: Visit = patient[0]\n",
    "        current_visit: Visit = patient[i]\n",
    "        next_visit: Visit = patient[i + 1]\n",
    "\n",
    "        # get time difference between current visit and next visit\n",
    "        time_diff = (next_visit.encounter_time - current_visit.encounter_time).days\n",
    "        time_diff_from_first_visit = (current_visit.encounter_time - first_visit.encounter_time).days\n",
    "        readmission_label = 1 if time_diff < time_window else 0\n",
    "\n",
    "        conditions = [c for c in current_visit.get_code_list(table=\"DIAGNOSES_ICD\") if c in filtered_diag_codes]\n",
    "        procedures = current_visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        # exclude: visits without condition, procedure, or drug code\n",
    "        if len(conditions) * len(procedures) == 0:\n",
    "            continue\n",
    "        samples.append(\n",
    "            {\n",
    "                \"visit_id\": current_visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                DIAGNOSES_KEY: conditions,\n",
    "                PROCEDURES_KEY: procedures,\n",
    "                INTERVAL_DAYS_KEY: [time_diff_from_first_visit],\n",
    "                \"label\": readmission_label,\n",
    "            }\n",
    "        )\n",
    "    # no cohort selection\n",
    "    return samples"
   ],
   "metadata": {
    "id": "fgrnO7KkWDBY"
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create the task datasets\n",
    "\n",
    "mimic3_dxtx = mimic3_ds.set_task(task_fn=readmission_prediction_mimic3_fn)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "IMD0MvgA8e8Z",
    "outputId": "11b7f21f-9f3b-419c-ee21-69ec1e3e02fe"
   },
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples for readmission_prediction_mimic3_fn: 100%|██████████| 46520/46520 [00:10<00:00, 4265.44it/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of sample dataset:\n",
      "\t- Dataset: MIMIC3Dataset\n",
      "\t- Task: readmission_prediction_mimic3_fn\n",
      "\t- Number of samples: 10746\n",
      "\t- Number of patients: 6819\n",
      "\t- Number of visits: 10746\n",
      "\t- Number of visits per patient: 1.5759\n",
      "\t- conditions:\n",
      "\t\t- Number of conditions per sample: 13.4028\n",
      "\t\t- Number of unique conditions: 3327\n",
      "\t\t- Distribution of conditions (Top-10): [('4019', 3734), ('4280', 3649), ('42731', 2896), ('5849', 2316), ('41401', 2308), ('25000', 2015), ('51881', 1817), ('2724', 1676), ('5990', 1642), ('53081', 1417)]\n",
      "\t- procedures:\n",
      "\t\t- Number of procedures per sample: 4.3769\n",
      "\t\t- Number of unique procedures: 1331\n",
      "\t\t- Distribution of procedures (Top-10): [('3893', 3632), ('9604', 2008), ('966', 1935), ('9904', 1793), ('9671', 1711), ('3995', 1427), ('9672', 1322), ('3891', 983), ('9915', 841), ('8856', 838)]\n",
      "\t- days_since_first_visit:\n",
      "\t\t- Number of days_since_first_visit per sample: 1.0000\n",
      "\t\t- Length of days_since_first_visit: 1\n",
      "\t- label:\n",
      "\t\t- Number of label per sample: 1.0000\n",
      "\t\t- Number of unique label: 2\n",
      "\t\t- Distribution of label (Top-10): [(1, 6065), (0, 4681)]\n",
      "Statistics of sample dataset:\n",
      "\t- Dataset: MIMIC3Dataset\n",
      "\t- Task: readmission_prediction_mimic3_fn\n",
      "\t- Number of samples: 10746\n",
      "\t- Number of patients: 6819\n",
      "\t- Number of visits: 10746\n",
      "\t- Number of visits per patient: 1.5759\n",
      "\t- conditions:\n",
      "\t\t- Number of conditions per sample: 13.4028\n",
      "\t\t- Number of unique conditions: 3327\n",
      "\t\t- Distribution of conditions (Top-10): [('4019', 3734), ('4280', 3649), ('42731', 2896), ('5849', 2316), ('41401', 2308), ('25000', 2015), ('51881', 1817), ('2724', 1676), ('5990', 1642), ('53081', 1417)]\n",
      "\t- procedures:\n",
      "\t\t- Number of procedures per sample: 4.3769\n",
      "\t\t- Number of unique procedures: 1331\n",
      "\t\t- Distribution of procedures (Top-10): [('3893', 3632), ('9604', 2008), ('966', 1935), ('9904', 1793), ('9671', 1711), ('3995', 1427), ('9672', 1322), ('3891', 983), ('9915', 841), ('8856', 838)]\n",
      "\t- days_since_first_visit:\n",
      "\t\t- Number of days_since_first_visit per sample: 1.0000\n",
      "\t\t- Length of days_since_first_visit: 1\n",
      "\t- label:\n",
      "\t\t- Number of label per sample: 1.0000\n",
      "\t\t- Number of unique label: 2\n",
      "\t\t- Distribution of label (Top-10): [(1, 6065), (0, 4681)]\n"
     ]
    }
   ],
   "source": [
    "# Print dataset statistics\n",
    "\n",
    "print(mimic3_dxtx.stat())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Create the dataloaders\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train, val, test = split_by_patient(mimic3_dxtx, [0.8, 0.1, 0.1])\n",
    "\n",
    "# obtain train/val/test dataloader, they are <torch.data.DataLoader> object\n",
    "train_loader = get_dataloader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = get_dataloader(val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = get_dataloader(test, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the models\n",
    "\n",
    "class MaskDirection(Enum):\n",
    "    FORWARD = 'forward'\n",
    "    BACKWARD = 'backward'\n",
    "    DIAGONAL = 'diagonal'\n",
    "    NONE = 'none'\n",
    "\n",
    "class MaskedLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape: int):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.parameter.Parameter(torch.randn(normalized_shape))\n",
    "        self.beta = nn.parameter.Parameter(torch.randn(normalized_shape))\n",
    "        self.eps = 1e-5\n",
    "        self.normalized_shape = normalized_shape\n",
    "\n",
    "    def forward(self, x: torch.Tensor, key_padding_mask: Optional[torch.Tensor] = None):\n",
    "        print(x.shape)\n",
    "        print(key_padding_mask.shape)\n",
    "        n = torch.sum(torch.ones_like(x) * (~key_padding_mask).int().unsqueeze(-1).expand(-1, -1, self.normalized_shape), dim=-1)\n",
    "        print(n.shape)\n",
    "        print(n)\n",
    "\n",
    "        # expected_val = torch.nanmean(x, dim=1)\n",
    "        # variance = torch.nansum(())\n",
    "\n",
    "# todo: add temporal encoding\n",
    "# todo: implement layer normalization\n",
    "# todo: implement fully connected network structure and linear layer activation functions\n",
    "class MaskEnc(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dim: int,\n",
    "            num_heads: int,\n",
    "            dropout: float = 0.1,\n",
    "            batch_first: bool = True,\n",
    "            temporal_mask_direction: MaskDirection = MaskDirection.NONE\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.temporal_mask_direction = temporal_mask_direction\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=embedding_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=batch_first\n",
    "        )\n",
    "\n",
    "        # self.masked_layer_norm1 = MaskedLayerNorm(embedding_dim)\n",
    "        # self.masked_layer_norm2 = MaskedLayerNorm(embedding_dim)\n",
    "\n",
    "        self.fc = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x: torch.Tensor,\n",
    "            key_padding_mask: Optional[torch.Tensor] = None\n",
    "    ):\n",
    "        attn_mask = self._make_temporal_mask(x.shape[1])\n",
    "\n",
    "        attn_output, attn_output_weights = self.attention(x, x, x, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        # attn_output = torch.nan_to_num(attn_output, nan=-1e9)\n",
    "\n",
    "\n",
    "        # x = self.masked_layer_norm1(x + attn_output, key_padding_mask)\n",
    "        # x = self.masked_layer_norm2(x + self.fc(x), key_padding_mask)\n",
    "\n",
    "        x = self.dropout(self.relu(self.fc(attn_output)))\n",
    "        return x\n",
    "\n",
    "    def _make_temporal_mask(self, n: int) -> Optional[torch.Tensor]:\n",
    "        if self.temporal_mask_direction == MaskDirection.NONE:\n",
    "            return None\n",
    "\n",
    "        mask = torch.ones(n,n)\n",
    "        if self.temporal_mask_direction == MaskDirection.FORWARD:\n",
    "            mask = torch.tril(mask)\n",
    "        if self.temporal_mask_direction == MaskDirection.BACKWARD:\n",
    "            mask = torch.triu(mask)\n",
    "        if self.temporal_mask_direction == MaskDirection.DIAGONAL:\n",
    "            mask = mask.fill_diagonal_(0)\n",
    "\n",
    "        return mask.bool()\n",
    "\n",
    "class BiteNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dim: int,\n",
    "            output_dim: int,\n",
    "            num_heads: int,\n",
    "            dropout: float = 0.1,\n",
    "            batch_first: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        def _make_mask_enc_block(temporal_mask_direction: MaskDirection = MaskDirection.NONE):\n",
    "            return MaskEnc(\n",
    "                embedding_dim = embedding_dim,\n",
    "                num_heads = num_heads,\n",
    "                dropout = dropout,\n",
    "                batch_first = batch_first,\n",
    "                temporal_mask_direction = temporal_mask_direction\n",
    "            )\n",
    "\n",
    "        self.code_level_attn = _make_mask_enc_block(MaskDirection.DIAGONAL)\n",
    "        self.visit_level_attn_forward = _make_mask_enc_block(MaskDirection.FORWARD)\n",
    "        self.visit_level_attn_backward = _make_mask_enc_block(MaskDirection.BACKWARD)\n",
    "        self.fc = nn.Linear(2*embedding_dim, output_dim)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x: torch.Tensor,\n",
    "            key_padding_mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        code_attn = self.code_level_attn(x, key_padding_mask)\n",
    "        code_attn = torch.nan_to_num(code_attn)\n",
    "\n",
    "        u_fw = self.visit_level_attn_forward(code_attn, key_padding_mask)\n",
    "        u_fw = u_fw.nansum(dim=1).squeeze()\n",
    "\n",
    "        u_bw = self.visit_level_attn_backward(code_attn, key_padding_mask)\n",
    "        u_bw = u_bw.nansum(dim=1).squeeze()\n",
    "\n",
    "        u_bi = torch.cat([u_fw, u_bw], dim=-1)\n",
    "        s = self.fc(u_bi)\n",
    "        return s\n",
    "\n",
    "class PyHealthBiteNet(BaseModel):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset: BaseDataset,\n",
    "            feature_keys: List[str],\n",
    "            label_key: str,\n",
    "            mode: str,\n",
    "            use_interval_emb: bool = True,\n",
    "            use_procedure_codes = True,\n",
    "            embedding_dim: int = 128,\n",
    "            num_heads: int = 4,\n",
    "            dropout: float = 0.1,\n",
    "            batch_first: bool = True,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__(dataset, feature_keys, label_key, mode)\n",
    "\n",
    "        # Any BaseModel should have these attributes, as functions like add_feature_transform_layer uses them\n",
    "        self.feat_tokenizers = {}\n",
    "        self.embeddings = nn.ModuleDict()\n",
    "        self.linear_layers = nn.ModuleDict()\n",
    "        self.label_tokenizer = self.get_label_tokenizer()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # self.add_feature_transform_layer will create a transformation layer for each feature\n",
    "        for feature_key in self.feature_keys:\n",
    "            input_info = self.dataset.input_info[feature_key]\n",
    "            self.add_feature_transform_layer(\n",
    "                feature_key, input_info\n",
    "            )\n",
    "\n",
    "        # final output layer\n",
    "        output_size = self.get_output_size(self.label_tokenizer)\n",
    "        self.bite_net = BiteNet(\n",
    "            embedding_dim = embedding_dim,\n",
    "            output_dim = output_size,\n",
    "            num_heads = num_heads,\n",
    "            dropout = dropout,\n",
    "            batch_first = batch_first\n",
    "        )\n",
    "\n",
    "        # self.fc = nn.Linear(len(self.feature_keys) * hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:\n",
    "        patient_emb = []\n",
    "        patient_mask = []\n",
    "        for feature_key in self.feature_keys:\n",
    "\n",
    "            feature_vals = kwargs[feature_key]\n",
    "            x = self.feat_tokenizers[feature_key].batch_encode_2d(feature_vals, padding=True, truncation=False)\n",
    "            x = torch.tensor(x, dtype=torch.long, device=self.device)\n",
    "            pad_idx = self.feat_tokenizers[feature_key].vocabulary(\"<pad>\")\n",
    "\n",
    "            #create the mask\n",
    "            mask = (x != pad_idx).bool()\n",
    "\n",
    "            embeds = self.embeddings[feature_key](x)\n",
    "            patient_emb.append(embeds)\n",
    "            patient_mask.append(mask)\n",
    "\n",
    "        # (patient, features * hidden_dim)\n",
    "        patient_emb = torch.cat(patient_emb, dim=1)\n",
    "        patient_mask = ~(torch.cat(patient_mask, dim=1))\n",
    "\n",
    "        # (patient, label_size)\n",
    "        logits = self.bite_net(patient_emb, patient_mask)\n",
    "        # obtain y_true, loss, y_prob\n",
    "        y_true = self.prepare_labels(kwargs[self.label_key], self.label_tokenizer)\n",
    "        loss = self.get_loss_function()(logits, y_true)\n",
    "        y_prob = self.prepare_y_prob(logits)\n",
    "        return {\"loss\": loss, \"y_prob\": y_prob, \"y_true\": y_true}\n",
    "\n",
    "model_dxtx = PyHealthBiteNet(\n",
    "    dataset = mimic3_dxtx,\n",
    "    feature_keys = [\"conditions\", \"procedures\"],\n",
    "    label_key = \"label\",\n",
    "    mode = \"binary\",\n",
    ")\n",
    "\n",
    "data = next(iter(train_loader))\n",
    "model_dxtx(**data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "model_dxtx = PyHealthBiteNet(\n",
    "    dataset = mimic3_dxtx,\n",
    "    feature_keys = [\"conditions\", \"procedures\"],\n",
    "    label_key = \"label\",\n",
    "    mode = \"binary\",\n",
    ")\n",
    "\n",
    "model_dx = PyHealthBiteNet(\n",
    "    dataset = mimic3_dxtx,\n",
    "    feature_keys = [\"conditions\"],\n",
    "    label_key = \"label\",\n",
    "    mode = \"binary\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "{'loss': tensor(0.7363, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n 'y_prob': tensor([[0.4940],\n         [0.5260],\n         [0.5713],\n         [0.6283],\n         [0.4630],\n         [0.4646],\n         [0.5993],\n         [0.5617],\n         [0.5722],\n         [0.5628],\n         [0.5436],\n         [0.4776],\n         [0.6207],\n         [0.5105],\n         [0.6244],\n         [0.4756],\n         [0.5805],\n         [0.5534],\n         [0.6040],\n         [0.5931],\n         [0.5287],\n         [0.5496],\n         [0.5750],\n         [0.4966],\n         [0.5697],\n         [0.4620],\n         [0.5815],\n         [0.5287],\n         [0.6660],\n         [0.5615],\n         [0.5513],\n         [0.5093]], grad_fn=<SigmoidBackward0>),\n 'y_true': tensor([[0.],\n         [0.],\n         [0.],\n         [0.],\n         [1.],\n         [1.],\n         [1.],\n         [1.],\n         [0.],\n         [1.],\n         [0.],\n         [0.],\n         [0.],\n         [1.],\n         [1.],\n         [1.],\n         [1.],\n         [1.],\n         [1.],\n         [0.],\n         [0.],\n         [0.],\n         [0.],\n         [1.],\n         [0.],\n         [1.],\n         [0.],\n         [1.],\n         [0.],\n         [0.],\n         [1.],\n         [1.]])}"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(train_loader))\n",
    "model_dx(**data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyHealthBiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (conditions): Embedding(3329, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1333, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): BiteNet(\n",
      "    (code_level_attn): MaskEnc(\n",
      "      (attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (visit_level_attn_forward): MaskEnc(\n",
      "      (attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (visit_level_attn_backward): MaskEnc(\n",
      "      (attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Metrics: None\n",
      "Device: cpu\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x2aa8319a0>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 5\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 0 / 5:   0%|          | 0/268 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3ad8290dbee942e5842be0389e26a6ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-268 ---\n",
      "loss: 0.6895\n",
      "Evaluation:   0%|          | 0/35 [00:00<?, ?it/s]/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Evaluation: 100%|██████████| 35/35 [00:00<00:00, 123.56it/s]\n",
      "--- Eval epoch-0, step-268 ---\n",
      "pr_auc: 0.5514\n",
      "roc_auc: 0.5000\n",
      "f1: 0.7109\n",
      "loss: 0.6888\n",
      "New best pr_auc score (0.5514) at epoch-0, step-268\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 1 / 5:   0%|          | 0/268 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8cdef895a7d54afb9aad2a0fd8b55838"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "--- Train epoch-1, step-536 ---\n",
      "loss: 0.6861\n",
      "Evaluation:   0%|          | 0/35 [00:00<?, ?it/s]/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Evaluation: 100%|██████████| 35/35 [00:00<00:00, 122.29it/s]\n",
      "--- Eval epoch-1, step-536 ---\n",
      "pr_auc: 0.5514\n",
      "roc_auc: 0.5000\n",
      "f1: 0.7109\n",
      "loss: 0.6882\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 2 / 5:   0%|          | 0/268 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a34d8b4c182b4eac90dd9129b3def1f7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "--- Train epoch-2, step-804 ---\n",
      "loss: 0.6853\n",
      "Evaluation:   0%|          | 0/35 [00:00<?, ?it/s]/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Evaluation: 100%|██████████| 35/35 [00:00<00:00, 122.83it/s]\n",
      "--- Eval epoch-2, step-804 ---\n",
      "pr_auc: 0.5514\n",
      "roc_auc: 0.5000\n",
      "f1: 0.7109\n",
      "loss: 0.6883\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 3 / 5:   0%|          | 0/268 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "971a9ae8a7c146e688e6114550445ca5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m trainer_dxtx \u001B[38;5;241m=\u001B[39m Trainer(model\u001B[38;5;241m=\u001B[39mmodel_dxtx)\n\u001B[0;32m----> 2\u001B[0m \u001B[43mtrainer_dxtx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader_dxtx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_loader_dxtx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmonitor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpr_auc\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/pyhealth/trainer.py:202\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, train_dataloader, val_dataloader, test_dataloader, epochs, optimizer_class, optimizer_params, weight_decay, max_grad_norm, monitor, monitor_criterion, load_best_model_at_last)\u001B[0m\n\u001B[1;32m    200\u001B[0m loss \u001B[38;5;241m=\u001B[39m output[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    201\u001B[0m \u001B[38;5;66;03m# backward\u001B[39;00m\n\u001B[0;32m--> 202\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m max_grad_norm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    204\u001B[0m     torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(\n\u001B[1;32m    205\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mparameters(), max_grad_norm\n\u001B[1;32m    206\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trainer_dxtx = Trainer(model=model_dxtx)\n",
    "trainer_dxtx.train(\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    epochs=5,\n",
    "    monitor=\"pr_auc\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 33/33 [00:00<00:00, 367.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pr_auc': 0.726784508178901, 'roc_auc': 0.685080036129632, 'f1': 0.7090909090909092, 'loss': 0.6331828191424861}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 33/33 [00:00<00:00, 363.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'pr_auc': 0.726784508178901}"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# option 1: use our built-in evaluation metric\n",
    "score_dxtx = trainer_dxtx.evaluate(test_loader)\n",
    "print (score_dxtx)\n",
    "\n",
    "# option 2: use our pyhealth.metrics to evaluate\n",
    "y_true_dxtx, y_prob_dxtx, loss_dxtx = trainer_dxtx.inference(test_loader)\n",
    "binary_metrics_fn(y_true_dxtx, y_prob_dxtx, metrics=[\"pr_auc\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
