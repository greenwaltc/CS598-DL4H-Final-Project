{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Google Cloud authentication and data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login\n",
    "# ! gcloud auth application-default login\n",
    "# ! gcloud config set project dl4h-final-project-383605"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pCHvVcifGcdr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gzip: *.gz: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# ! pip install --upgrade google-api-python-client google-cloud-storage\n",
    "from google.cloud import storage\n",
    "\n",
    "# Replace these values with your project and bucket as needed\n",
    "project_id = \"dl4h-final-project-383605\"\n",
    "mimic3_bucket = \"mimiciii-1.4.physionet.org\"\n",
    "\n",
    "storage_client = storage.Client(project=project_id)\n",
    "bucket = storage_client.bucket(mimic3_bucket)\n",
    "data_folder = \"./data\"\n",
    "for blob in bucket.list_blobs():\n",
    "  if \"CHARTEVENTS\" in blob.name:\n",
    "    continue\n",
    "  blob.download_to_filename(f\"{data_folder}/{blob.name}\")\n",
    "\n",
    "# Extract all the files\n",
    "! gunzip {data_folder}/*.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gunzip {data_folder}/*.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Library imports and data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T13:41:30.986868Z",
     "end_time": "2023-04-20T13:41:32.520292Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/pyhealth/trainer.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    }
   ],
   "source": [
    "# ! pip install pyhealth\n",
    "from pyhealth.datasets import MIMIC3Dataset\n",
    "from pyhealth.data import Patient, Visit\n",
    "import pandas as pd\n",
    "from pyhealth.datasets import split_by_patient, get_dataloader, BaseDataset\n",
    "from pyhealth.models import BaseModel\n",
    "from pyhealth.trainer import Trainer\n",
    "from pyhealth.metrics.binary import binary_metrics_fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import List, Dict, Optional\n",
    "from enum import Enum\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "\n",
    "# Set this to the directory with all MIMIC-3 dataset files\n",
    "data_root = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s-VgeMdJDYYK",
    "outputId": "9cb9c08a-9003-4836-a3b2-a23e7cd45b55",
    "ExecuteTime": {
     "start_time": "2023-04-20T13:41:32.520520Z",
     "end_time": "2023-04-20T13:41:32.591193Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "mimic3_ds = MIMIC3Dataset(\n",
    "        root=data_root,\n",
    "        tables=[\"DIAGNOSES_ICD\", \"PROCEDURES_ICD\"],\n",
    "        dev=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "HQdeU1TLU6m_",
    "outputId": "590623d6-2917-4155-bf0a-5d75a3979c25",
    "ExecuteTime": {
     "start_time": "2023-04-20T13:41:32.595827Z",
     "end_time": "2023-04-20T13:41:32.598340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics of base dataset (dev=True):\n",
      "\t- Dataset: MIMIC3Dataset\n",
      "\t- Number of patients: 1000\n",
      "\t- Number of visits: 1295\n",
      "\t- Number of visits per patient: 1.2950\n",
      "\t- Number of events per visit in DIAGNOSES_ICD: 9.3544\n",
      "\t- Number of events per visit in PROCEDURES_ICD: 4.3351\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nStatistics of base dataset (dev=True):\\n\\t- Dataset: MIMIC3Dataset\\n\\t- Number of patients: 1000\\n\\t- Number of visits: 1295\\n\\t- Number of visits per patient: 1.2950\\n\\t- Number of events per visit in DIAGNOSES_ICD: 9.3544\\n\\t- Number of events per visit in PROCEDURES_ICD: 4.3351\\n'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print dataset statistics\n",
    "\n",
    "mimic3_ds.stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2n_iXyWSWE-H",
    "outputId": "4f9561b5-c85e-48a7-b9e2-365f8afc241a",
    "ExecuteTime": {
     "start_time": "2023-04-20T13:41:32.605056Z",
     "end_time": "2023-04-20T13:41:32.608195Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find all diagnoses codes\n",
    "# Find all procedure codes\n",
    "# Remove diagnoses codes with fewer than 5 occurences in the dataset\n",
    "\n",
    "all_diag_codes = []\n",
    "all_proc_codes = []\n",
    "for patient_id, patient in mimic3_ds.patients.items():\n",
    "  for i in range(len(patient)):\n",
    "    visit: Visit = patient[i]\n",
    "    conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "    procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "    all_diag_codes.extend(conditions)\n",
    "    all_proc_codes.extend(procedures)\n",
    "\n",
    "codes = pd.Series(all_diag_codes)\n",
    "diag_code_counts = codes.value_counts()\n",
    "filtered_diag_codes = diag_code_counts[diag_code_counts > 4].index.values\n",
    "num_unique_diag_codes = len(filtered_diag_codes)\n",
    "\n",
    "unique_proc_codes = list(set(all_proc_codes))\n",
    "num_unique_proc_codes = len(unique_proc_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T13:41:32.610580Z",
     "end_time": "2023-04-20T13:41:32.611767Z"
    }
   },
   "outputs": [],
   "source": [
    "proc_code_to_index_map = {}\n",
    "diag_code_to_index_map = {}\n",
    "\n",
    "index = 0\n",
    "for proc_code in unique_proc_codes:\n",
    "    proc_code_to_index_map[proc_code] = index\n",
    "    index += 1\n",
    "\n",
    "index = 0\n",
    "for diag_code in filtered_diag_codes:\n",
    "    diag_code_to_index_map[diag_code] = index\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fgrnO7KkWDBY",
    "ExecuteTime": {
     "start_time": "2023-04-20T13:41:32.611865Z",
     "end_time": "2023-04-20T13:41:32.643055Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the tasks\n",
    "\n",
    "DIAGNOSES_KEY = \"conditions\"\n",
    "PROCEDURES_KEY = \"procedures\"\n",
    "INTERVAL_DAYS_KEY = \"days_since_first_visit\"\n",
    "\n",
    "def flatten(l: List):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def patient_level_readmission_prediction(patient, time_window=30):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    # if the patient only has one visit, we drop it\n",
    "    if len(patient) == 1:\n",
    "        return []\n",
    "\n",
    "    sorted_visits = sorted(patient, key=lambda visit: visit.encounter_time)\n",
    "\n",
    "    # step 1: define label\n",
    "    idx_last_visit = len(sorted_visits)-1\n",
    "    last_visit: Visit = sorted_visits[idx_last_visit]\n",
    "    second_to_last_visit: Visit = sorted_visits[idx_last_visit - 1]\n",
    "    first_visit: Visit = sorted_visits[0]\n",
    "\n",
    "    time_diff = (last_visit.encounter_time - second_to_last_visit.encounter_time).days\n",
    "    readmission_label = 1 if time_diff < time_window else 0\n",
    "\n",
    "    # step 2: obtain features\n",
    "    visits_conditions = []\n",
    "    visits_procedures = []\n",
    "    visits_intervals = []\n",
    "    for idx, visit in enumerate(sorted_visits):\n",
    "        if idx == len(sorted_visits) - 1: break\n",
    "        conditions = [c for c in visit.get_code_list(table=\"DIAGNOSES_ICD\") if c in filtered_diag_codes]\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        time_diff_from_first_visit = (visit.encounter_time - first_visit.encounter_time).days\n",
    "\n",
    "        if len(conditions) * len(procedures) == 0:\n",
    "            continue\n",
    "\n",
    "        visits_conditions.append(conditions)\n",
    "        visits_procedures.append(procedures)\n",
    "        visits_intervals.append([time_diff_from_first_visit])\n",
    "\n",
    "    unique_conditions = list(set(flatten(visits_conditions)))\n",
    "    unique_procedures = list(set(flatten(visits_procedures)))\n",
    "\n",
    "    # step 3: exclusion criteria\n",
    "    if len(unique_conditions) * len(unique_procedures) == 0:\n",
    "        return []\n",
    "\n",
    "    # step 4: assemble the sample\n",
    "    samples.append(\n",
    "        {\n",
    "            \"patient_id\": patient.patient_id,\n",
    "            \"visit_id\": visit.visit_id,\n",
    "            \"conditions\": visits_conditions,\n",
    "            \"procedures\": visits_procedures,\n",
    "            \"intervals\": visits_intervals,\n",
    "            \"label\": readmission_label,\n",
    "        }\n",
    "    )\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "IMD0MvgA8e8Z",
    "outputId": "11b7f21f-9f3b-419c-ee21-69ec1e3e02fe",
    "ExecuteTime": {
     "start_time": "2023-04-20T13:41:32.621375Z",
     "end_time": "2023-04-20T13:41:32.671889Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples for patient_level_readmission_prediction: 100%|██████████| 1000/1000 [00:00<00:00, 30153.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create the task datasets\n",
    "mimic3_dxtx = mimic3_ds.set_task(task_fn=patient_level_readmission_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T13:41:32.672453Z",
     "end_time": "2023-04-20T13:41:32.673537Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the unique visit intervals\n",
    "all_intervals = []\n",
    "for sample in mimic3_dxtx:\n",
    "    intervals = flatten(sample['intervals'])\n",
    "    all_intervals.extend(intervals)\n",
    "\n",
    "unique_intervals = list(set(all_intervals))\n",
    "num_unique_intervals = len(unique_intervals)\n",
    "\n",
    "visit_interval_to_index_map = {}\n",
    "\n",
    "index = 0\n",
    "for visit_interval in unique_intervals:\n",
    "    visit_interval_to_index_map[visit_interval] = index\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T13:41:32.679847Z",
     "end_time": "2023-04-20T13:41:32.683342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[258, 499,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
      "\n",
      "        [[118, 174, 520, 383, 200,  70, 126, 317, 274, 143],\n",
      "         [448, 480, 381, 274,   0,   0,   0,   0,   0,   0]]])\n"
     ]
    }
   ],
   "source": [
    "def collate_codes(batch_codes, i_patient, patient, feature_key, code_to_index_map, mask_tensor):\n",
    "    for i_visit, visit_codes in enumerate(patient[feature_key]):\n",
    "        for i_code, code in enumerate(visit_codes):\n",
    "            batch_codes[i_patient][i_visit][i_code] = code_to_index_map[code]\n",
    "\n",
    "        # Set the mask for the visit codes\n",
    "        num_codes = len(visit_codes)\n",
    "        mask_tensor[i_patient][i_visit][:num_codes] = 1\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    max_num_visits = 0\n",
    "    max_num_diagnosis_codes_per_visit = 0\n",
    "    max_num_procedure_codes_per_visit = 0\n",
    "\n",
    "    for patient in batch:\n",
    "        patient_num_visits = len(patient['conditions'])\n",
    "        if patient_num_visits > max_num_visits:\n",
    "            max_num_visits = patient_num_visits\n",
    "        for visit_conditions in patient['conditions']:\n",
    "            num_visit_diagnosis_codes = len(visit_conditions)\n",
    "            if num_visit_diagnosis_codes > max_num_diagnosis_codes_per_visit:\n",
    "                max_num_diagnosis_codes_per_visit = num_visit_diagnosis_codes\n",
    "        for visit_procedures in patient[\"procedures\"]:\n",
    "            num_visit_procedure_codes = len(visit_procedures)\n",
    "            if num_visit_procedure_codes > max_num_procedure_codes_per_visit:\n",
    "                max_num_procedure_codes_per_visit = num_visit_procedure_codes\n",
    "\n",
    "    batch_procedures = torch.zeros(batch_size, max_num_visits, max_num_procedure_codes_per_visit).long()\n",
    "    batch_conditions = torch.zeros(batch_size, max_num_visits, max_num_diagnosis_codes_per_visit).long()\n",
    "    batch_intervals = torch.zeros(batch_size, max_num_visits).long()\n",
    "    batch_labels = torch.zeros(batch_size).long()\n",
    "\n",
    "    batch_visits_masks = torch.zeros(batch_size, max_num_visits).long()\n",
    "    batch_procedures_masks = torch.zeros(batch_size, max_num_visits, max_num_procedure_codes_per_visit).long()\n",
    "    batch_conditions_masks = torch.zeros(batch_size, max_num_visits, max_num_diagnosis_codes_per_visit).long()\n",
    "\n",
    "    for i_patient, patient in enumerate(batch):\n",
    "        # Collate diagnosis and procedure codes\n",
    "        collate_codes(batch_procedures, i_patient, patient, \"procedures\", proc_code_to_index_map, batch_procedures_masks)\n",
    "        collate_codes(batch_conditions, i_patient, patient, \"conditions\", diag_code_to_index_map, batch_conditions_masks)\n",
    "\n",
    "        # Get the number of visits this patient has\n",
    "        visit_intervals = flatten(patient[\"intervals\"])\n",
    "        num_visits = len(visit_intervals)\n",
    "\n",
    "        # Set the visits mask for the patient\n",
    "        batch_visits_masks[i_patient][:num_visits] = 1\n",
    "\n",
    "        # Get the visit intervals (num of days since first visit)\n",
    "\n",
    "        # Collate the visit intervals by setting the onehot encoding\n",
    "        visit_interval_onehot_indices = [visit_interval_to_index_map[interval] for interval in visit_intervals]\n",
    "        batch_intervals[i_patient][:num_visits] = 1\n",
    "\n",
    "        # Set the label\n",
    "        batch_labels[i_patient] = patient[\"label\"]\n",
    "\n",
    "    return (\n",
    "        batch_procedures,\n",
    "        batch_conditions,\n",
    "        batch_intervals,\n",
    "        batch_visits_masks,\n",
    "        batch_procedures_masks,\n",
    "        batch_conditions_masks,\n",
    "        batch_labels\n",
    "    )\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "train, val, test = split_by_patient(mimic3_dxtx, [0.8, 0.1, 0.1])\n",
    "\n",
    "# obtain train/val/test dataloader, they are <torch.data.DataLoader> object\n",
    "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "_ = next(iter(train_loader))\n",
    "print(_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T14:34:51.595826Z",
     "end_time": "2023-04-20T14:34:51.607830Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PyHealthBiteNet object argument after ** must be a mapping, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 307\u001B[0m\n\u001B[1;32m    299\u001B[0m model_dxtx \u001B[38;5;241m=\u001B[39m PyHealthBiteNet(\n\u001B[1;32m    300\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m mimic3_dxtx,\n\u001B[1;32m    301\u001B[0m     feature_keys \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprocedures\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconditions\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mintervals\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    302\u001B[0m     label_key \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    303\u001B[0m     mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinary\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    304\u001B[0m )\n\u001B[1;32m    306\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28miter\u001B[39m(train_loader))\n\u001B[0;32m--> 307\u001B[0m \u001B[43mmodel_dxtx\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;66;03m# model = BiteNet(\u001B[39;00m\n\u001B[1;32m    310\u001B[0m \u001B[38;5;66;03m#     embedding_dim = 4,\u001B[39;00m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;66;03m#     output_dim = 1,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    315\u001B[0m \u001B[38;5;66;03m# procedures, conditions, intervals, visits_masks, procedures_masks, conditions_masks, labels = next(iter(train_loader))\u001B[39;00m\n\u001B[1;32m    316\u001B[0m \u001B[38;5;66;03m# model(procedures, conditions, intervals, visits_masks, procedures_masks, conditions_masks)\u001B[39;00m\n",
      "\u001B[0;31mTypeError\u001B[0m: PyHealthBiteNet object argument after ** must be a mapping, not tuple"
     ]
    }
   ],
   "source": [
    "# Define the models\n",
    "VERY_BIG_NUMBER = 1e30\n",
    "VERY_SMALL_NUMBER = 1e-30\n",
    "VERY_POSITIVE_NUMBER = VERY_BIG_NUMBER\n",
    "VERY_NEGATIVE_NUMBER = -VERY_BIG_NUMBER\n",
    "\n",
    "class MaskDirection(Enum):\n",
    "    FORWARD = 'forward'\n",
    "    BACKWARD = 'backward'\n",
    "    DIAGONAL = 'diagonal'\n",
    "    NONE = 'none'\n",
    "\n",
    "# class MaskedLayerNorm(nn.Module):\n",
    "#     def __init__(self, normalized_shape: int):\n",
    "#         super().__init__()\n",
    "#         self.gamma = nn.parameter.Parameter(torch.randn(normalized_shape))\n",
    "#         self.beta = nn.parameter.Parameter(torch.randn(normalized_shape))\n",
    "#         self.eps = 1e-5\n",
    "#         self.normalized_shape = normalized_shape\n",
    "#\n",
    "#     def forward(self, x: torch.Tensor, key_padding_mask: Optional[torch.Tensor] = None):\n",
    "#         print(x.shape)\n",
    "#         print(key_padding_mask.shape)\n",
    "#         n = torch.sum(torch.ones_like(x) * (~key_padding_mask).int().unsqueeze(-1).expand(-1, -1, self.normalized_shape), dim=-1)\n",
    "#         print(n.shape)\n",
    "#         print(n)\n",
    "#\n",
    "#         # expected_val = torch.nanmean(x, dim=1)\n",
    "#         # variance = torch.nansum(())\n",
    "\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, embedding_size: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_size, embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_size, embedding_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.fc(input)\n",
    "        # x = torch.nan_to_num(x, nan=VERY_NEGATIVE_NUMBER)\n",
    "        soft = F.softmax(x, dim=1)\n",
    "        attn_output = torch.nansum(soft * input, 1)\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "# todo: add temporal encoding\n",
    "# todo: implement layer normalization\n",
    "class MaskEnc(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dim: int,\n",
    "            num_heads: int,\n",
    "            dropout: float = 0.1,\n",
    "            batch_first: bool = True,\n",
    "            temporal_mask_direction: MaskDirection = MaskDirection.NONE,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.temporal_mask_direction = temporal_mask_direction\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "                embed_dim=embedding_dim,\n",
    "                num_heads=num_heads,\n",
    "                dropout=dropout,\n",
    "                batch_first=batch_first\n",
    "            )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embedding_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "        self.attention_pooling = AttentionPooling(embedding_dim)\n",
    "\n",
    "        # self.masked_layer_norm1 = MaskedLayerNorm(embedding_dim)\n",
    "        # self.masked_layer_norm2 = MaskedLayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x: torch.Tensor,\n",
    "            key_padding_mask: Optional[torch.Tensor] = None\n",
    "    ):\n",
    "        attn_mask = self._make_temporal_mask(x.shape[1])\n",
    "\n",
    "        attn_output, attn_output_weights = self.attention(x, x, x, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        out = self.fc(attn_output)\n",
    "        out = self.attention_pooling(out)\n",
    "\n",
    "        # x = self.masked_layer_norm1(x + attn_output, key_padding_mask)\n",
    "        # x = self.masked_layer_norm2(x + self.fc(x), key_padding_mask)\n",
    "        return out\n",
    "\n",
    "    def _make_temporal_mask(self, n: int) -> Optional[torch.Tensor]:\n",
    "        if self.temporal_mask_direction == MaskDirection.NONE:\n",
    "            return None\n",
    "\n",
    "        if self.temporal_mask_direction == MaskDirection.FORWARD:\n",
    "            return torch.tril(torch.ones(n,n))\n",
    "        if self.temporal_mask_direction == MaskDirection.BACKWARD:\n",
    "            return torch.triu(torch.ones(n,n))\n",
    "        if self.temporal_mask_direction == MaskDirection.DIAGONAL:\n",
    "            return torch.zeros(n,n).fill_diagonal_(1)\n",
    "\n",
    "class BiteNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dim: int = 128,\n",
    "            output_dim: int = 1,\n",
    "            num_heads: int = 4,\n",
    "            dropout: float = 0.1,\n",
    "            batch_first: bool = True,\n",
    "            use_procedure_codes: bool = True,\n",
    "            num_diag_codes: int = num_unique_diag_codes,\n",
    "            num_proc_codes: int = num_unique_proc_codes,\n",
    "            num_intervals: int = num_unique_intervals,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_procedure_codes = use_procedure_codes\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.diag_emb = nn.Embedding(num_diag_codes, embedding_dim)\n",
    "        self.proc_emb = nn.Embedding(num_proc_codes, embedding_dim)\n",
    "        self.interval_emb = nn.Embedding(num_intervals, embedding_dim)\n",
    "\n",
    "        def _make_mask_enc_block(temporal_mask_direction: MaskDirection = MaskDirection.NONE):\n",
    "            return MaskEnc(\n",
    "                embedding_dim = embedding_dim,\n",
    "                num_heads = num_heads,\n",
    "                dropout = dropout,\n",
    "                batch_first = batch_first,\n",
    "                temporal_mask_direction = temporal_mask_direction,\n",
    "            )\n",
    "\n",
    "        self.code_level_attn = _make_mask_enc_block(MaskDirection.DIAGONAL)\n",
    "        self.visit_level_attn_forward = _make_mask_enc_block(MaskDirection.FORWARD)\n",
    "        self.visit_level_attn_backward = _make_mask_enc_block(MaskDirection.BACKWARD)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2*embedding_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def flatten(self, x: torch.Tensor, keep: int):\n",
    "        fixed_shape = list(x.size())\n",
    "        start = len(fixed_shape) - keep\n",
    "        left = reduce(mul, [fixed_shape[i] or x.shape[i] for i in range(start)])\n",
    "        out_shape = [left] + [fixed_shape[i] or x.shape[i] for i in range(start, len(fixed_shape))]\n",
    "        return torch.reshape(x, out_shape)\n",
    "\n",
    "    def reshape(self, v, ref, embedding_dim):\n",
    "        batch_size = ref.shape[0]\n",
    "        n_visits = ref.shape[1]\n",
    "        out = torch.reshape(v, [batch_size, n_visits, embedding_dim])\n",
    "        return out\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            procedures: torch.Tensor,\n",
    "            conditions: torch.Tensor,\n",
    "            intervals: torch.Tensor,\n",
    "            visits_mask: torch.Tensor,\n",
    "            procedures_mask: torch.Tensor,\n",
    "            conditions_mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        embedded_conditions = self.diag_emb(conditions)\n",
    "\n",
    "        if self.use_procedure_codes:\n",
    "            procedures_emb = self.proc_emb(procedures)\n",
    "            embedded_codes = torch.cat([embedded_conditions, procedures_emb], dim=2)\n",
    "            codes_mask = torch.cat([conditions_mask, procedures_mask], dim=-1)\n",
    "        else:\n",
    "            embedded_codes = embedded_conditions\n",
    "            codes_mask = conditions_mask\n",
    "\n",
    "        codes_mask = ~(codes_mask.bool())\n",
    "        visits_mask = ~(visits_mask.bool())\n",
    "\n",
    "        # input tensor, reshape 4 dimension to 3\n",
    "        flattened_codes = self.flatten(embedded_codes, 2)\n",
    "\n",
    "        # input mask, reshape 3 dimension to 2\n",
    "        flattened_codes_mask = self.flatten(codes_mask, 1)\n",
    "\n",
    "        code_attn = self.code_level_attn(flattened_codes, flattened_codes_mask)\n",
    "        code_attn = self.reshape(code_attn, embedded_codes, self.embedding_dim)\n",
    "\n",
    "        u_fw = self.visit_level_attn_forward(code_attn, visits_mask)\n",
    "        u_bw = self.visit_level_attn_backward(code_attn, visits_mask)\n",
    "        u_bi = torch.cat([u_fw, u_bw], dim=-1)\n",
    "        s = self.fc(u_bi)\n",
    "        return s\n",
    "\n",
    "class PyHealthBiteNet(BaseModel):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset: BaseDataset,\n",
    "            feature_keys: List[str],\n",
    "            label_key: str,\n",
    "            mode: str,\n",
    "            use_interval_emb: bool = True,\n",
    "            use_procedure_codes: bool = True,\n",
    "            embedding_dim: int = 128,\n",
    "            num_heads: int = 4,\n",
    "            dropout: float = 0.1,\n",
    "            batch_first: bool = True,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__(dataset, feature_keys, label_key, mode)\n",
    "\n",
    "        # Any BaseModel should have these attributes, as functions like add_feature_transform_layer uses them\n",
    "        self.feat_tokenizers = {}\n",
    "        self.embeddings = nn.ModuleDict()\n",
    "        self.linear_layers = nn.ModuleDict()\n",
    "        self.label_tokenizer = self.get_label_tokenizer()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # self.add_feature_transform_layer will create a transformation layer for each feature\n",
    "        for feature_key in self.feature_keys:\n",
    "            input_info = self.dataset.input_info[feature_key]\n",
    "            self.add_feature_transform_layer(\n",
    "                feature_key, input_info\n",
    "            )\n",
    "\n",
    "        # final output layer\n",
    "        output_size = self.get_output_size(self.label_tokenizer)\n",
    "        self.bite_net = BiteNet(\n",
    "            embedding_dim = embedding_dim,\n",
    "            output_dim = output_size,\n",
    "            num_heads = num_heads,\n",
    "            dropout = dropout,\n",
    "            batch_first = batch_first\n",
    "        )\n",
    "\n",
    "        # self.fc = nn.Linear(len(self.feature_keys) * hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:\n",
    "        print(kwargs)\n",
    "        patient_emb = []\n",
    "        patient_mask = []\n",
    "        for feature_key in self.feature_keys:\n",
    "\n",
    "            feature_vals = kwargs[feature_key]\n",
    "            for visits_feature_vals in feature_vals:\n",
    "                x = self.feat_tokenizers[feature_key].batch_encode_2d(visits_feature_vals, padding=True, truncation=False)\n",
    "                print(visits_feature_vals)\n",
    "\n",
    "            x = self.feat_tokenizers[feature_key].batch_encode_2d(feature_vals, padding=True, truncation=False)\n",
    "            x = torch.tensor(x, dtype=torch.long, device=self.device)\n",
    "            pad_idx = self.feat_tokenizers[feature_key].vocabulary(\"<pad>\")\n",
    "\n",
    "            #create the mask\n",
    "            mask = (x != pad_idx)\n",
    "\n",
    "            embeds = self.embeddings[feature_key](x)\n",
    "            patient_emb.append(embeds)\n",
    "            patient_mask.append(mask)\n",
    "\n",
    "        # (patient, features * hidden_dim)\n",
    "        patient_emb = torch.cat(patient_emb, dim=1)\n",
    "        patient_mask = ~(torch.cat(patient_mask, dim=1))\n",
    "\n",
    "        # (patient, label_size)\n",
    "        logits = self.bite_net(patient_emb, patient_mask)\n",
    "        # obtain y_true, loss, y_prob\n",
    "        y_true = self.prepare_labels(kwargs[self.label_key], self.label_tokenizer)\n",
    "        loss = self.get_loss_function()(logits, y_true)\n",
    "        y_prob = self.prepare_y_prob(logits)\n",
    "        return {\"loss\": loss, \"y_prob\": y_prob, \"y_true\": y_true}\n",
    "\n",
    "model_dxtx = PyHealthBiteNet(\n",
    "    dataset = mimic3_dxtx,\n",
    "    feature_keys = ['procedures', 'conditions', 'intervals'],\n",
    "    label_key = \"label\",\n",
    "    mode = \"binary\",\n",
    ")\n",
    "\n",
    "data = next(iter(train_loader))\n",
    "model_dxtx(**data)\n",
    "\n",
    "# model = BiteNet(\n",
    "#     embedding_dim = 4,\n",
    "#     output_dim = 1,\n",
    "#     num_heads = 4,\n",
    "#     dropout = 0\n",
    "# )\n",
    "# procedures, conditions, intervals, visits_masks, procedures_masks, conditions_masks, labels = next(iter(train_loader))\n",
    "# model(procedures, conditions, intervals, visits_masks, procedures_masks, conditions_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_dxtx = PyHealthBiteNet(\n",
    "    dataset = mimic3_dxtx,\n",
    "    feature_keys = [\"conditions\", \"procedures\"],\n",
    "    label_key = \"label\",\n",
    "    mode = \"binary\",\n",
    ")\n",
    "\n",
    "model_dx = PyHealthBiteNet(\n",
    "    dataset = mimic3_dxtx,\n",
    "    feature_keys = [\"conditions\"],\n",
    "    label_key = \"label\",\n",
    "    mode = \"binary\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(0.7363, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       " 'y_prob': tensor([[0.4940],\n",
       "         [0.5260],\n",
       "         [0.5713],\n",
       "         [0.6283],\n",
       "         [0.4630],\n",
       "         [0.4646],\n",
       "         [0.5993],\n",
       "         [0.5617],\n",
       "         [0.5722],\n",
       "         [0.5628],\n",
       "         [0.5436],\n",
       "         [0.4776],\n",
       "         [0.6207],\n",
       "         [0.5105],\n",
       "         [0.6244],\n",
       "         [0.4756],\n",
       "         [0.5805],\n",
       "         [0.5534],\n",
       "         [0.6040],\n",
       "         [0.5931],\n",
       "         [0.5287],\n",
       "         [0.5496],\n",
       "         [0.5750],\n",
       "         [0.4966],\n",
       "         [0.5697],\n",
       "         [0.4620],\n",
       "         [0.5815],\n",
       "         [0.5287],\n",
       "         [0.6660],\n",
       "         [0.5615],\n",
       "         [0.5513],\n",
       "         [0.5093]], grad_fn=<SigmoidBackward0>),\n",
       " 'y_true': tensor([[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(train_loader))\n",
    "model_dx(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyHealthBiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (conditions): Embedding(3329, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1333, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): BiteNet(\n",
      "    (code_level_attn): MaskEnc(\n",
      "      (attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (visit_level_attn_forward): MaskEnc(\n",
      "      (attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (visit_level_attn_backward): MaskEnc(\n",
      "      (attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Metrics: None\n",
      "Device: cpu\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x2aa8319a0>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 5\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad8290dbee942e5842be0389e26a6ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 / 5:   0%|          | 0/268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-268 ---\n",
      "loss: 0.6895\n",
      "Evaluation:   0%|          | 0/35 [00:00<?, ?it/s]/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Evaluation: 100%|██████████| 35/35 [00:00<00:00, 123.56it/s]\n",
      "--- Eval epoch-0, step-268 ---\n",
      "pr_auc: 0.5514\n",
      "roc_auc: 0.5000\n",
      "f1: 0.7109\n",
      "loss: 0.6888\n",
      "New best pr_auc score (0.5514) at epoch-0, step-268\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cdef895a7d54afb9aad2a0fd8b55838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 / 5:   0%|          | 0/268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "--- Train epoch-1, step-536 ---\n",
      "loss: 0.6861\n",
      "Evaluation:   0%|          | 0/35 [00:00<?, ?it/s]/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Evaluation: 100%|██████████| 35/35 [00:00<00:00, 122.29it/s]\n",
      "--- Eval epoch-1, step-536 ---\n",
      "pr_auc: 0.5514\n",
      "roc_auc: 0.5000\n",
      "f1: 0.7109\n",
      "loss: 0.6882\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34d8b4c182b4eac90dd9129b3def1f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 / 5:   0%|          | 0/268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "--- Train epoch-2, step-804 ---\n",
      "loss: 0.6853\n",
      "Evaluation:   0%|          | 0/35 [00:00<?, ?it/s]/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Evaluation: 100%|██████████| 35/35 [00:00<00:00, 122.83it/s]\n",
      "--- Eval epoch-2, step-804 ---\n",
      "pr_auc: 0.5514\n",
      "roc_auc: 0.5000\n",
      "f1: 0.7109\n",
      "loss: 0.6883\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971a9ae8a7c146e688e6114550445ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 / 5:   0%|          | 0/268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m trainer_dxtx \u001B[38;5;241m=\u001B[39m Trainer(model\u001B[38;5;241m=\u001B[39mmodel_dxtx)\n\u001B[0;32m----> 2\u001B[0m \u001B[43mtrainer_dxtx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader_dxtx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_loader_dxtx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmonitor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpr_auc\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/pyhealth/trainer.py:202\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, train_dataloader, val_dataloader, test_dataloader, epochs, optimizer_class, optimizer_params, weight_decay, max_grad_norm, monitor, monitor_criterion, load_best_model_at_last)\u001B[0m\n\u001B[1;32m    200\u001B[0m loss \u001B[38;5;241m=\u001B[39m output[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    201\u001B[0m \u001B[38;5;66;03m# backward\u001B[39;00m\n\u001B[0;32m--> 202\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m max_grad_norm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    204\u001B[0m     torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(\n\u001B[1;32m    205\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mparameters(), max_grad_norm\n\u001B[1;32m    206\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trainer_dxtx = Trainer(model=model_dxtx)\n",
    "trainer_dxtx.train(\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    epochs=5,\n",
    "    monitor=\"pr_auc\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 33/33 [00:00<00:00, 367.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pr_auc': 0.726784508178901, 'roc_auc': 0.685080036129632, 'f1': 0.7090909090909092, 'loss': 0.6331828191424861}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 33/33 [00:00<00:00, 363.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pr_auc': 0.726784508178901}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# option 1: use our built-in evaluation metric\n",
    "score_dxtx = trainer_dxtx.evaluate(test_loader)\n",
    "print (score_dxtx)\n",
    "\n",
    "# option 2: use our pyhealth.metrics to evaluate\n",
    "y_true_dxtx, y_prob_dxtx, loss_dxtx = trainer_dxtx.inference(test_loader)\n",
    "binary_metrics_fn(y_true_dxtx, y_prob_dxtx, metrics=[\"pr_auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
