{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Google Cloud authentication and data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login\n",
    "# ! gcloud auth application-default login\n",
    "# ! gcloud config set project dl4h-final-project-383605"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pCHvVcifGcdr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gzip: *.gz: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# ! pip install --upgrade google-api-python-client google-cloud-storage\n",
    "from google.cloud import storage\n",
    "\n",
    "# Replace these values with your project and bucket as needed\n",
    "project_id = \"dl4h-final-project-383605\"\n",
    "mimic3_bucket = \"mimiciii-1.4.physionet.org\"\n",
    "\n",
    "storage_client = storage.Client(project=project_id)\n",
    "bucket = storage_client.bucket(mimic3_bucket)\n",
    "data_folder = \"./data\"\n",
    "for blob in bucket.list_blobs():\n",
    "  if \"CHARTEVENTS\" in blob.name:\n",
    "    continue\n",
    "  blob.download_to_filename(f\"{data_folder}/{blob.name}\")\n",
    "\n",
    "# Extract all the files\n",
    "! gunzip {data_folder}/*.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gunzip {data_folder}/*.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Library imports and data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T18:10:43.315000Z",
     "end_time": "2023-04-22T18:10:43.318219Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip install pyhealth\n",
    "from pyhealth.datasets import MIMIC3Dataset, SampleDataset\n",
    "from pyhealth.data import Visit\n",
    "import pandas as pd\n",
    "from pyhealth.datasets import split_by_patient, get_dataloader\n",
    "from pyhealth.models import BaseModel\n",
    "from pyhealth.trainer import Trainer\n",
    "from pyhealth.metrics.binary import binary_metrics_fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Dict, Optional\n",
    "from enum import Enum\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "\n",
    "# Set this to the directory with all MIMIC-3 dataset files\n",
    "data_root = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s-VgeMdJDYYK",
    "outputId": "9cb9c08a-9003-4836-a3b2-a23e7cd45b55",
    "ExecuteTime": {
     "start_time": "2023-04-22T18:11:57.435144Z",
     "end_time": "2023-04-22T18:12:00.244064Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "mimic3_ds = MIMIC3Dataset(\n",
    "        root=data_root,\n",
    "        tables=[\"DIAGNOSES_ICD\", \"PROCEDURES_ICD\"],\n",
    "        dev=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "HQdeU1TLU6m_",
    "outputId": "590623d6-2917-4155-bf0a-5d75a3979c25",
    "ExecuteTime": {
     "start_time": "2023-04-22T18:12:02.232818Z",
     "end_time": "2023-04-22T18:12:02.326272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics of base dataset (dev=False):\n",
      "\t- Dataset: MIMIC3Dataset\n",
      "\t- Number of patients: 46520\n",
      "\t- Number of visits: 58976\n",
      "\t- Number of visits per patient: 1.2678\n",
      "\t- Number of events per visit in DIAGNOSES_ICD: 11.0384\n",
      "\t- Number of events per visit in PROCEDURES_ICD: 4.0711\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nStatistics of base dataset (dev=False):\\n\\t- Dataset: MIMIC3Dataset\\n\\t- Number of patients: 46520\\n\\t- Number of visits: 58976\\n\\t- Number of visits per patient: 1.2678\\n\\t- Number of events per visit in DIAGNOSES_ICD: 11.0384\\n\\t- Number of events per visit in PROCEDURES_ICD: 4.0711\\n'"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print dataset statistics\n",
    "\n",
    "mimic3_ds.stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "# Find all diagnoses codes\n",
    "# Find all procedure codes\n",
    "# Remove diagnoses codes with fewer than 5 occurences in the dataset\n",
    "\n",
    "all_diag_codes = []\n",
    "for patient_id, patient in mimic3_ds.patients.items():\n",
    "  for i in range(len(patient)):\n",
    "    visit: Visit = patient[i]\n",
    "    conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "    all_diag_codes.extend(conditions)\n",
    "\n",
    "codes = pd.Series(all_diag_codes)\n",
    "diag_code_counts = codes.value_counts()\n",
    "filtered_diag_codes = diag_code_counts[diag_code_counts > 4].index.values\n",
    "num_unique_diag_codes = len(filtered_diag_codes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T18:12:03.817269Z",
     "end_time": "2023-04-22T18:12:04.048432Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "fgrnO7KkWDBY",
    "ExecuteTime": {
     "start_time": "2023-04-22T18:12:06.202560Z",
     "end_time": "2023-04-22T18:12:06.206518Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the tasks\n",
    "\n",
    "DIAGNOSES_KEY = \"conditions\"\n",
    "PROCEDURES_KEY = \"procedures\"\n",
    "INTERVAL_DAYS_KEY = \"days_since_first_visit\"\n",
    "\n",
    "def flatten(l: List):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def patient_level_readmission_prediction(patient, time_window=30):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    # if the patient only has one visit, we drop it\n",
    "    if len(patient) == 1:\n",
    "        return []\n",
    "\n",
    "    sorted_visits = sorted(patient, key=lambda visit: visit.encounter_time)\n",
    "\n",
    "    # step 1: define label\n",
    "    idx_last_visit = len(sorted_visits)-1\n",
    "    last_visit: Visit = sorted_visits[idx_last_visit]\n",
    "    second_to_last_visit: Visit = sorted_visits[idx_last_visit - 1]\n",
    "    first_visit: Visit = sorted_visits[0]\n",
    "\n",
    "    time_diff = (last_visit.encounter_time - second_to_last_visit.encounter_time).days\n",
    "    readmission_label = 1 if time_diff < time_window else 0\n",
    "\n",
    "    # step 2: obtain features\n",
    "    visits_conditions = []\n",
    "    visits_procedures = []\n",
    "    visits_intervals = []\n",
    "    for idx, visit in enumerate(sorted_visits):\n",
    "        if idx == len(sorted_visits) - 1: break\n",
    "        conditions = [c for c in visit.get_code_list(table=\"DIAGNOSES_ICD\") if c in filtered_diag_codes]\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        time_diff_from_first_visit = (visit.encounter_time - first_visit.encounter_time).days\n",
    "\n",
    "        if len(conditions) * len(procedures) == 0:\n",
    "            continue\n",
    "\n",
    "        visits_conditions.append(conditions)\n",
    "        visits_procedures.append(procedures)\n",
    "        visits_intervals.append([str(time_diff_from_first_visit)])\n",
    "\n",
    "    unique_conditions = list(set(flatten(visits_conditions)))\n",
    "    unique_procedures = list(set(flatten(visits_procedures)))\n",
    "\n",
    "    # step 3: exclusion criteria\n",
    "    if len(unique_conditions) * len(unique_procedures) == 0:\n",
    "        return []\n",
    "\n",
    "    # step 4: assemble the sample\n",
    "    samples.append(\n",
    "        {\n",
    "            \"patient_id\": patient.patient_id,\n",
    "            \"visit_id\": visit.visit_id,\n",
    "            \"conditions\": visits_conditions,\n",
    "            \"procedures\": visits_procedures,\n",
    "            \"intervals\": visits_intervals,\n",
    "            \"label\": readmission_label,\n",
    "        }\n",
    "    )\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "IMD0MvgA8e8Z",
    "outputId": "11b7f21f-9f3b-419c-ee21-69ec1e3e02fe",
    "ExecuteTime": {
     "start_time": "2023-04-22T18:12:09.731248Z",
     "end_time": "2023-04-22T18:12:20.494753Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples for patient_level_readmission_prediction: 100%|██████████| 46520/46520 [00:10<00:00, 4367.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create the task datasets\n",
    "mimic3_dxtx = mimic3_ds.set_task(task_fn=patient_level_readmission_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "train, val, test = split_by_patient(mimic3_dxtx, [0.8, 0.1, 0.1])\n",
    "\n",
    "train_loader = get_dataloader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = get_dataloader(val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = get_dataloader(test, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T12:24:27.986146Z",
     "end_time": "2023-04-23T12:24:27.987899Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T12:35:46.547183Z",
     "end_time": "2023-04-23T12:35:46.573841Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'loss': tensor(0.7028, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n 'y_prob': tensor([[0.5760],\n         [0.5742]], grad_fn=<SigmoidBackward0>),\n 'y_true': tensor([[1.],\n         [0.]])}"
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the models\n",
    "VERY_BIG_NUMBER = 1e30\n",
    "VERY_SMALL_NUMBER = 1e-30\n",
    "VERY_POSITIVE_NUMBER = VERY_BIG_NUMBER\n",
    "VERY_NEGATIVE_NUMBER = -VERY_BIG_NUMBER\n",
    "\n",
    "class MaskDirection(Enum):\n",
    "    FORWARD = 'forward'\n",
    "    BACKWARD = 'backward'\n",
    "    DIAGONAL = 'diagonal'\n",
    "    NONE = 'none'\n",
    "\n",
    "# class MaskedLayerNorm(nn.Module):\n",
    "#     def __init__(self, normalized_shape: int):\n",
    "#         super().__init__()\n",
    "#         self.gamma = nn.parameter.Parameter(torch.randn(normalized_shape))\n",
    "#         self.beta = nn.parameter.Parameter(torch.randn(normalized_shape))\n",
    "#         self.eps = 1e-5\n",
    "#         self.normalized_shape = normalized_shape\n",
    "#\n",
    "#     def forward(self, x: torch.Tensor, key_padding_mask: Optional[torch.Tensor] = None):\n",
    "#         print(x.shape)\n",
    "#         print(key_padding_mask.shape)\n",
    "#         n = torch.sum(torch.ones_like(x) * (~key_padding_mask).int().unsqueeze(-1).expand(-1, -1, self.normalized_shape), dim=-1)\n",
    "#         print(n.shape)\n",
    "#         print(n)\n",
    "#\n",
    "#         # expected_val = torch.nanmean(x, dim=1)\n",
    "#         # variance = torch.nansum(())\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, keep: int):\n",
    "        fixed_shape = list(x.size())\n",
    "        start = len(fixed_shape) - keep\n",
    "        left = reduce(mul, [fixed_shape[i] or x.shape[i] for i in range(start)])\n",
    "        out_shape = [left] + [fixed_shape[i] or x.shape[i] for i in range(start, len(fixed_shape))]\n",
    "        return torch.reshape(x, out_shape)\n",
    "\n",
    "\n",
    "class Unflatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, v: torch.Tensor, ref: torch.Tensor, embedding_dim):\n",
    "        batch_size = ref.shape[0]\n",
    "        n_visits = ref.shape[1]\n",
    "        out = torch.reshape(v, [batch_size, n_visits, embedding_dim])\n",
    "        return out\n",
    "\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, embedding_size: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_size, embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_size, embedding_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.fc(input)\n",
    "        x = torch.nan_to_num(x, nan=VERY_NEGATIVE_NUMBER)\n",
    "        soft = F.softmax(x, dim=1)\n",
    "        attn_output = torch.nansum(soft * input, 1)\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "# todo: implement layer normalization\n",
    "class MaskEnc(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dim: int,\n",
    "            num_heads: int,\n",
    "            dropout: float = 0.1,\n",
    "            batch_first: bool = True,\n",
    "            temporal_mask_direction: MaskDirection = MaskDirection.NONE,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.temporal_mask_direction = temporal_mask_direction\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "                embed_dim=embedding_dim,\n",
    "                num_heads=num_heads,\n",
    "                dropout=dropout,\n",
    "                batch_first=batch_first\n",
    "            )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embedding_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "        # self.masked_layer_norm1 = MaskedLayerNorm(embedding_dim)\n",
    "        # self.masked_layer_norm2 = MaskedLayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x, key_padding_mask = inputs\n",
    "        attn_mask = self._make_temporal_mask(x.shape[1])\n",
    "\n",
    "        attn_output, attn_output_weights = self.attention(x, x, x, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        out = self.fc(attn_output)\n",
    "\n",
    "        # x = self.masked_layer_norm1(x + attn_output, key_padding_mask)\n",
    "        # x = self.masked_layer_norm2(x + self.fc(x), key_padding_mask)\n",
    "        return out, key_padding_mask\n",
    "\n",
    "    def _make_temporal_mask(self, n: int) -> Optional[torch.Tensor]:\n",
    "        if self.temporal_mask_direction == MaskDirection.NONE:\n",
    "            return None\n",
    "        if self.temporal_mask_direction == MaskDirection.FORWARD:\n",
    "            return torch.tril(torch.ones(n,n))\n",
    "        if self.temporal_mask_direction == MaskDirection.BACKWARD:\n",
    "            return torch.triu(torch.ones(n,n))\n",
    "        if self.temporal_mask_direction == MaskDirection.DIAGONAL:\n",
    "            return torch.zeros(n,n).fill_diagonal_(1)\n",
    "\n",
    "\n",
    "class BiteNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dim: int = 128,\n",
    "            num_heads: int = 4,\n",
    "            dropout: float = 0.1,\n",
    "            batch_first: bool = True,\n",
    "            n_mask_enc_layers: int = 2,\n",
    "            use_procedures: bool = True,\n",
    "            use_intervals: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_intervals = use_intervals\n",
    "        self.use_procedures = use_procedures\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.flatten = Flatten()\n",
    "        self.unflatten = Unflatten()\n",
    "\n",
    "        def _make_mask_enc_block(temporal_mask_direction: MaskDirection = MaskDirection.NONE):\n",
    "            return MaskEnc(\n",
    "                embedding_dim = embedding_dim,\n",
    "                num_heads = num_heads,\n",
    "                dropout = dropout,\n",
    "                batch_first = batch_first,\n",
    "                temporal_mask_direction = temporal_mask_direction,\n",
    "            )\n",
    "\n",
    "        self.code_attn = nn.Sequential()\n",
    "        self.visit_attn_fw = nn.Sequential()\n",
    "        self.visit_attn_bw = nn.Sequential()\n",
    "        for _ in range(n_mask_enc_layers):\n",
    "            self.code_attn.append(_make_mask_enc_block(MaskDirection.DIAGONAL))\n",
    "            self.visit_attn_fw.append(_make_mask_enc_block(MaskDirection.FORWARD))\n",
    "            self.visit_attn_bw.append(_make_mask_enc_block(MaskDirection.BACKWARD))\n",
    "\n",
    "        # Attention pooling layers\n",
    "        self.code_attn_pooling = AttentionPooling(embedding_dim)\n",
    "        self.visit_attn_bw_pooling = AttentionPooling(embedding_dim)\n",
    "        self.visit_attn_fw_pooling = AttentionPooling(embedding_dim)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2*embedding_dim, embedding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            embedded_codes: torch.Tensor,\n",
    "            embedded_intervals: torch.Tensor,\n",
    "            codes_mask: torch.Tensor,\n",
    "            visits_mask: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        codes_mask = ~(codes_mask.bool())\n",
    "\n",
    "        # input tensor, reshape 4 dimension to 3\n",
    "        flattened_codes = self.flatten(embedded_codes, 2)\n",
    "\n",
    "        # input mask, reshape 3 dimension to 2\n",
    "        flattened_codes_mask = self.flatten(codes_mask, 1)\n",
    "\n",
    "        code_attn, _ = self.code_attn((flattened_codes, flattened_codes_mask))\n",
    "        code_attn = self.code_attn_pooling(code_attn)\n",
    "        code_attn = self.unflatten(code_attn, embedded_codes, self.embedding_dim)\n",
    "\n",
    "        if self.use_intervals:\n",
    "            code_attn += embedded_intervals\n",
    "\n",
    "        visits_mask = ~(visits_mask.bool())\n",
    "\n",
    "        u_fw, _ = self.visit_attn_fw((code_attn, visits_mask))\n",
    "        u_fw = self.visit_attn_fw_pooling(u_fw)\n",
    "\n",
    "        u_bw, _ = self.visit_attn_bw((code_attn, visits_mask))\n",
    "        u_bw = self.visit_attn_bw_pooling(u_bw)\n",
    "\n",
    "        u_bi = torch.cat([u_fw, u_bw], dim=-1)\n",
    "        s = self.fc(u_bi)\n",
    "        return s\n",
    "\n",
    "class PyHealthBiteNet(BaseModel):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset: SampleDataset,\n",
    "            feature_keys: List[str],\n",
    "            label_key: str,\n",
    "            mode: str,\n",
    "            embedding_dim: int = 128,\n",
    "            n_mask_enc_layers: int = 2,\n",
    "            use_intervals: bool = True,\n",
    "            use_procedures: bool = True,\n",
    "            num_heads: int = 4,\n",
    "            dropout: float = 0.1,\n",
    "            batch_first: bool = True,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__(dataset, feature_keys, label_key, mode)\n",
    "\n",
    "        self.use_intervals = use_intervals\n",
    "        self.use_procedures = use_procedures\n",
    "\n",
    "        # Any BaseModel should have these attributes, as functions like add_feature_transform_layer uses them\n",
    "        self.feat_tokenizers = {}\n",
    "        self.embeddings = nn.ModuleDict()\n",
    "        self.linear_layers = nn.ModuleDict()\n",
    "        self.label_tokenizer = self.get_label_tokenizer()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # self.add_feature_transform_layer will create a transformation layer for each feature\n",
    "        for feature_key in self.feature_keys:\n",
    "            input_info = self.dataset.input_info[feature_key]\n",
    "            self.add_feature_transform_layer(\n",
    "                feature_key, input_info, special_tokens=[\"<pad>\", \"<unk>\"]\n",
    "            )\n",
    "\n",
    "        # final output layer\n",
    "        output_size = self.get_output_size(self.label_tokenizer)\n",
    "        self.bite_net = BiteNet(\n",
    "            embedding_dim = embedding_dim,\n",
    "            num_heads = num_heads,\n",
    "            dropout = dropout,\n",
    "            batch_first = batch_first,\n",
    "            use_intervals=use_intervals,\n",
    "            use_procedures=use_procedures,\n",
    "            n_mask_enc_layers=n_mask_enc_layers\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(self.embedding_dim, output_size)\n",
    "\n",
    "    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        embeddings = {}\n",
    "        masks = {}\n",
    "        for feature_key in self.feature_keys:\n",
    "            input_info = self.dataset.input_info[feature_key]\n",
    "\n",
    "            # each patient's feature is represented by [[code1, code2],[code3]]\n",
    "            assert input_info[\"dim\"] == 3 and input_info[\"type\"] == str\n",
    "            feature_vals = kwargs[feature_key]\n",
    "\n",
    "            x = self.feat_tokenizers[feature_key].batch_encode_3d(feature_vals, truncation=(False, False))\n",
    "            x = torch.tensor(x, dtype=torch.long, device=self.device)\n",
    "            pad_idx = self.feat_tokenizers[feature_key].vocabulary(\"<pad>\")\n",
    "            #create the mask\n",
    "            mask = (x != pad_idx).long()\n",
    "            embeds = self.embeddings[feature_key](x)\n",
    "            embeddings[feature_key] = embeds\n",
    "            masks[feature_key] = mask\n",
    "\n",
    "        embedded_codes = embeddings['conditions']\n",
    "        codes_mask = masks['conditions']\n",
    "        if self.use_procedures:\n",
    "            embedded_codes = torch.cat((embedded_codes, embeddings['procedures']), dim=2)\n",
    "            codes_mask = torch.cat((codes_mask, masks['procedures']), dim=2)\n",
    "\n",
    "        output = self.bite_net(embedded_codes, embeddings['intervals'].squeeze(2), codes_mask, masks['intervals'].squeeze(-1))\n",
    "        logits = self.fc(output)\n",
    "\n",
    "        # obtain y_true, loss, y_prob\n",
    "        y_true = self.prepare_labels(kwargs[self.label_key], self.label_tokenizer)\n",
    "        loss = self.get_loss_function()(logits, y_true)\n",
    "        y_prob = self.prepare_y_prob(logits)\n",
    "        return {\"loss\": loss, \"y_prob\": y_prob, \"y_true\": y_true}\n",
    "\n",
    "model_dxtx = PyHealthBiteNet(\n",
    "    dataset = mimic3_dxtx,\n",
    "    feature_keys = ['procedures', 'conditions', 'intervals'],\n",
    "    label_key = \"label\",\n",
    "    mode = \"binary\",\n",
    "    embedding_dim=4\n",
    ")\n",
    "\n",
    "data = next(iter(train_loader))\n",
    "model_dxtx(**data)\n",
    "\n",
    "# model = BiteNet(\n",
    "#     embedding_dim = 4,\n",
    "#     output_dim = 1,\n",
    "#     num_heads = 4,\n",
    "#     dropout = 0\n",
    "# )\n",
    "# procedures, conditions, intervals, visits_masks, procedures_masks, conditions_masks, labels = next(iter(train_loader))\n",
    "# model(procedures, conditions, intervals, visits_masks, procedures_masks, conditions_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T12:35:38.010576Z",
     "end_time": "2023-04-23T12:35:38.057981Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dxtx = PyHealthBiteNet(\n",
    "    dataset = mimic3_dxtx,\n",
    "    feature_keys = [\"conditions\", \"procedures\", \"intervals\"],\n",
    "    label_key = \"label\",\n",
    "    mode = \"binary\",\n",
    ")\n",
    "\n",
    "model_dx = PyHealthBiteNet(\n",
    "    dataset = mimic3_dxtx,\n",
    "    feature_keys = [\"conditions\", \"intervals\"],\n",
    "    label_key = \"label\",\n",
    "    mode = \"binary\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T18:13:09.904206Z",
     "end_time": "2023-04-22T18:15:27.925345Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyHealthBiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (conditions): Embedding(3383, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1366, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1758, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (1): MaskEnc(\n",
      "        (attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (1): MaskEnc(\n",
      "        (attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (1): MaskEnc(\n",
      "        (attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (code_attn_pooling): AttentionPooling(\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw_pooling): AttentionPooling(\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw_pooling): AttentionPooling(\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Metrics: None\n",
      "Device: cpu\n",
      "\n",
      "Training:\n",
      "Batch size: 2\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x2aaa9e100>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 5\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 0 / 5:   0%|          | 0/2786 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1cac74d241024127aac0e2e0f90ac075"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1192],\n",
      "        [-0.1162]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0864],\n",
      "        [-0.0869]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.1368],\n",
      "        [-0.1319]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.1771],\n",
      "        [-0.1860]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2398],\n",
      "        [-0.2601]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3476],\n",
      "        [-0.3470]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4296],\n",
      "        [-0.3722]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4397],\n",
      "        [-0.4377]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4015],\n",
      "        [-0.4265]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5426],\n",
      "        [-0.5348]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5387],\n",
      "        [-0.5388]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9607],\n",
      "        [-0.8535]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9307],\n",
      "        [-0.8161]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.9437],\n",
      "        [-1.1268]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.2985],\n",
      "        [-1.9040]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-4.4641],\n",
      "        [-3.9192]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.6981],\n",
      "        [-1.7069]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.6610],\n",
      "        [-2.4436]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7395],\n",
      "        [-1.5742]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7247],\n",
      "        [-1.2538]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4354],\n",
      "        [-1.2838]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7557],\n",
      "        [-0.8464]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7123],\n",
      "        [-0.6449]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6563],\n",
      "        [-0.5313]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5088],\n",
      "        [-0.5905]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4715],\n",
      "        [-0.5216]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4266],\n",
      "        [-0.4684]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4102],\n",
      "        [-0.4302]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4048],\n",
      "        [-0.4092]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3930],\n",
      "        [-0.3901]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4021],\n",
      "        [-0.3886]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4085],\n",
      "        [-0.4028]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3978],\n",
      "        [-0.3944]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4105],\n",
      "        [-0.4084]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4211],\n",
      "        [-0.3977]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4305],\n",
      "        [-0.4243]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4252],\n",
      "        [-0.4198]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4186],\n",
      "        [-0.4206]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4247],\n",
      "        [-0.4279]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4426],\n",
      "        [-0.4426]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4546],\n",
      "        [-0.4367]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4553],\n",
      "        [-0.4593]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4638],\n",
      "        [-0.4711]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4773],\n",
      "        [-0.4604]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4917],\n",
      "        [-0.5225]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4907],\n",
      "        [-0.5385]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5078],\n",
      "        [-0.5634]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6393],\n",
      "        [-0.6531]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7278],\n",
      "        [-0.7434]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6780],\n",
      "        [-0.8748]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0250],\n",
      "        [-1.0828]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4847],\n",
      "        [-0.9245]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4426],\n",
      "        [-2.3468]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.8245],\n",
      "        [-4.1766]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-4.9517],\n",
      "        [-7.4325]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-4.6120],\n",
      "        [-3.1740]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.9782],\n",
      "        [-1.3688]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.1992],\n",
      "        [-2.0493]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4172],\n",
      "        [-1.4399]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9733],\n",
      "        [-0.9492]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7679],\n",
      "        [-0.6779]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6861],\n",
      "        [-0.6275]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6170],\n",
      "        [-0.5606]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5119],\n",
      "        [-0.5146]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4831],\n",
      "        [-0.5031]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4708],\n",
      "        [-0.4764]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4743],\n",
      "        [-0.4694]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4669],\n",
      "        [-0.4614]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4689],\n",
      "        [-0.4581]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4661],\n",
      "        [-0.4792]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4812],\n",
      "        [-0.4715]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4684],\n",
      "        [-0.4736]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4784],\n",
      "        [-0.4925]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4892],\n",
      "        [-0.4952]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5064],\n",
      "        [-0.5121]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5174],\n",
      "        [-0.5110]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5299],\n",
      "        [-0.5228]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5555],\n",
      "        [-0.5401]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5594],\n",
      "        [-0.5639]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5515],\n",
      "        [-0.5792]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5763],\n",
      "        [-0.5959]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6299],\n",
      "        [-0.6033]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5829],\n",
      "        [-0.6168]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6203],\n",
      "        [-0.6421]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6349],\n",
      "        [-0.6351]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6438],\n",
      "        [-0.6506]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6706],\n",
      "        [-0.6153]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6686],\n",
      "        [-0.6143]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7068],\n",
      "        [-0.6289]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7025],\n",
      "        [-0.7131]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7117],\n",
      "        [-0.8044]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7460],\n",
      "        [-0.7555]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8635],\n",
      "        [-0.7735]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8276],\n",
      "        [-1.0184]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1550],\n",
      "        [-1.0997]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0595],\n",
      "        [-1.2057]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1617],\n",
      "        [-1.5219]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.1882],\n",
      "        [-1.5307]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.7451],\n",
      "        [-3.3074]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.3073],\n",
      "        [-2.3567]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.1098],\n",
      "        [-3.1538]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.4160],\n",
      "        [-3.3639]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.0081],\n",
      "        [-2.0521]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.0311],\n",
      "        [-2.0892]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8461],\n",
      "        [-0.9985]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3809],\n",
      "        [-1.8784]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8704],\n",
      "        [-1.4188]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.0065],\n",
      "        [-1.8322]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.1805],\n",
      "        [-1.9033]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0468],\n",
      "        [-1.3467]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0615],\n",
      "        [-0.9221]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1831],\n",
      "        [-0.9748]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0354],\n",
      "        [-1.0396]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9298],\n",
      "        [-0.8697]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9025],\n",
      "        [-0.8441]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7527],\n",
      "        [-0.7845]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6384],\n",
      "        [-0.7235]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6554],\n",
      "        [-0.6973]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6020],\n",
      "        [-0.6803]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5875],\n",
      "        [-0.6231]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6219],\n",
      "        [-0.6094]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6171],\n",
      "        [-0.6000]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6165],\n",
      "        [-0.6315]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6048],\n",
      "        [-0.6554]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6392],\n",
      "        [-0.6210]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6291],\n",
      "        [-0.6350]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6338],\n",
      "        [-0.6575]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6343],\n",
      "        [-0.6421]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6359],\n",
      "        [-0.6547]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6708],\n",
      "        [-0.6588]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6530],\n",
      "        [-0.6529]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6318],\n",
      "        [-0.6657]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6696],\n",
      "        [-0.6760]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6571],\n",
      "        [-0.6904]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6776],\n",
      "        [-0.6704]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6290],\n",
      "        [-0.6942]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7203],\n",
      "        [-0.6955]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6826],\n",
      "        [-0.7320]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7739],\n",
      "        [-0.7674]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8004],\n",
      "        [-0.7364]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8399],\n",
      "        [-0.8513]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7659],\n",
      "        [-0.8661]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9382],\n",
      "        [-0.9656]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0087],\n",
      "        [-0.8944]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2435],\n",
      "        [-1.1959]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1901],\n",
      "        [-1.2705]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0176],\n",
      "        [-1.2957]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2426],\n",
      "        [-1.3924]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4612],\n",
      "        [-1.6500]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2640],\n",
      "        [-1.4862]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8469],\n",
      "        [-1.8354]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.9548],\n",
      "        [-1.9439]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4046],\n",
      "        [-2.0404]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2482],\n",
      "        [-1.5243]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3541],\n",
      "        [-1.3449]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1954],\n",
      "        [-1.3386]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1130],\n",
      "        [-1.0690]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1459],\n",
      "        [-1.0730]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1247],\n",
      "        [-1.0101]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9768],\n",
      "        [-1.0919]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0100],\n",
      "        [-1.0710]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0361],\n",
      "        [-1.0835]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1997],\n",
      "        [-1.1078]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2176],\n",
      "        [-1.2014]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2361],\n",
      "        [-1.2000]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0708],\n",
      "        [-1.2572]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0901],\n",
      "        [-1.2499]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1839],\n",
      "        [-1.1844]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0521],\n",
      "        [-1.1363]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0591],\n",
      "        [-1.0543]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1051],\n",
      "        [-1.0443]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0397],\n",
      "        [-1.1555]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0898],\n",
      "        [-1.0703]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0580],\n",
      "        [-1.0860]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0761],\n",
      "        [-1.0844]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0011],\n",
      "        [-1.0509]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9922],\n",
      "        [-1.0072]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9409],\n",
      "        [-0.9626]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0395],\n",
      "        [-1.0006]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0151],\n",
      "        [-0.9624]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9173],\n",
      "        [-0.9676]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9840],\n",
      "        [-0.9857]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8830],\n",
      "        [-0.9341]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8890],\n",
      "        [-0.9025]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8637],\n",
      "        [-0.8662]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8367],\n",
      "        [-0.8373]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8028],\n",
      "        [-0.7754]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7837],\n",
      "        [-0.7820]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7624],\n",
      "        [-0.7736]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7531],\n",
      "        [-0.7692]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7896],\n",
      "        [-0.8039]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7810],\n",
      "        [-0.7924]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7603],\n",
      "        [-0.7655]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8006],\n",
      "        [-0.8067]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7781],\n",
      "        [-0.8190]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7981],\n",
      "        [-0.8018]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7961],\n",
      "        [-0.7635]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7727],\n",
      "        [-0.7414]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7318],\n",
      "        [-0.7721]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7461],\n",
      "        [-0.7516]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7598],\n",
      "        [-0.7803]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7651],\n",
      "        [-0.7581]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7381],\n",
      "        [-0.7504]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7547],\n",
      "        [-0.7416]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7663],\n",
      "        [-0.7689]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7864],\n",
      "        [-0.7901]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7490],\n",
      "        [-0.7918]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8241],\n",
      "        [-0.7699]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8439],\n",
      "        [-0.7615]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8334],\n",
      "        [-0.8171]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8239],\n",
      "        [-0.8806]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8319],\n",
      "        [-0.8949]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8903],\n",
      "        [-0.8264]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9982],\n",
      "        [-0.9268]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9870],\n",
      "        [-0.8562]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8890],\n",
      "        [-0.9238]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9471],\n",
      "        [-0.9009]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9653],\n",
      "        [-0.9089]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8831],\n",
      "        [-0.9359]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9029],\n",
      "        [-0.9723]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9596],\n",
      "        [-0.8363]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8510],\n",
      "        [-1.0086]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0480],\n",
      "        [-1.0525]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0726],\n",
      "        [-1.1666]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1195],\n",
      "        [-1.1983]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2115],\n",
      "        [-1.3184]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4458],\n",
      "        [-1.4461]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2959],\n",
      "        [-1.6420]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.9279],\n",
      "        [-1.4233]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5206],\n",
      "        [-1.3247]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.6011],\n",
      "        [-1.7875]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.4613],\n",
      "        [-2.7380]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.1840],\n",
      "        [-2.3368]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2752],\n",
      "        [-1.7768]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8578],\n",
      "        [-1.8723]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2362],\n",
      "        [-1.4442]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1660],\n",
      "        [-1.1520]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1747],\n",
      "        [-1.2077]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0433],\n",
      "        [-1.0138]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9360],\n",
      "        [-0.9611]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9114],\n",
      "        [-0.8991]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8560],\n",
      "        [-0.8720]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8022],\n",
      "        [-0.8127]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7770],\n",
      "        [-0.8251]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7998],\n",
      "        [-0.7917]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7909],\n",
      "        [-0.7895]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7842],\n",
      "        [-0.7758]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7847],\n",
      "        [-0.8117]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8035],\n",
      "        [-0.7966]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7999],\n",
      "        [-0.8222]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8225],\n",
      "        [-0.8261]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8424],\n",
      "        [-0.8521]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8450],\n",
      "        [-0.8412]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8859],\n",
      "        [-0.8596]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9380],\n",
      "        [-0.9130]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9048],\n",
      "        [-0.9173]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9311],\n",
      "        [-0.9120]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9451],\n",
      "        [-0.9144]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9985],\n",
      "        [-0.9934]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0348],\n",
      "        [-1.0690]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0450],\n",
      "        [-1.0743]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0728],\n",
      "        [-1.1214]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2484],\n",
      "        [-1.2561]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2312],\n",
      "        [-1.3028]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3739],\n",
      "        [-1.4686]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5893],\n",
      "        [-1.7221]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.9009],\n",
      "        [-1.8447]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7587],\n",
      "        [-2.2171]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-3.1772],\n",
      "        [-2.6566]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8913],\n",
      "        [-2.3136]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-4.7799],\n",
      "        [-6.0123]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-4.3362],\n",
      "        [-5.2266]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-8.0276],\n",
      "        [-7.7777]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-11.7745],\n",
      "        [-12.0228]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-10.3251],\n",
      "        [ -6.0430]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ -3.9105],\n",
      "        [-15.0892]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-11.0329],\n",
      "        [-11.6847]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-6.6968],\n",
      "        [-7.3910]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.4164],\n",
      "        [-2.2482]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.6469],\n",
      "        [-1.8417]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2120],\n",
      "        [-1.4791]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1948],\n",
      "        [-1.1587]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0659],\n",
      "        [-1.1011]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0313],\n",
      "        [-0.9690]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9315],\n",
      "        [-0.9109]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8992],\n",
      "        [-0.8978]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8505],\n",
      "        [-0.8670]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8562],\n",
      "        [-0.8438]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8147],\n",
      "        [-0.8510]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8278],\n",
      "        [-0.8413]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8099],\n",
      "        [-0.8203]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8146],\n",
      "        [-0.8030]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8186],\n",
      "        [-0.8425]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8272],\n",
      "        [-0.8248]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8047],\n",
      "        [-0.8306]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8128],\n",
      "        [-0.8465]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8535],\n",
      "        [-0.7964]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8239],\n",
      "        [-0.8352]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8066],\n",
      "        [-0.8293]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8115],\n",
      "        [-0.7885]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8040],\n",
      "        [-0.7793]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7604],\n",
      "        [-0.7636]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7391],\n",
      "        [-0.7445]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7120],\n",
      "        [-0.7237]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7052],\n",
      "        [-0.7303]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7176],\n",
      "        [-0.6974]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7225],\n",
      "        [-0.7181]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6940],\n",
      "        [-0.7072]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7066],\n",
      "        [-0.7348]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.6936],\n",
      "        [-0.7302]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7184],\n",
      "        [-0.7108]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7067],\n",
      "        [-0.7135]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7359],\n",
      "        [-0.7370]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7598],\n",
      "        [-0.7295]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7552],\n",
      "        [-0.7408]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7539],\n",
      "        [-0.7514]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7758],\n",
      "        [-0.7820]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.7787],\n",
      "        [-0.7580]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8186],\n",
      "        [-0.7994]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8601],\n",
      "        [-0.8137]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8593],\n",
      "        [-0.8286]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8384],\n",
      "        [-0.9305]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9000],\n",
      "        [-0.9646]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9457],\n",
      "        [-0.9548]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0359],\n",
      "        [-1.1564]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1773],\n",
      "        [-1.1139]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0681],\n",
      "        [-1.0493]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0396],\n",
      "        [-0.9931]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0047],\n",
      "        [-0.9355]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0530],\n",
      "        [-0.9295]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9940],\n",
      "        [-0.9769]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8954],\n",
      "        [-1.0018]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0000],\n",
      "        [-1.0297]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0709],\n",
      "        [-1.0515]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1016],\n",
      "        [-1.0586]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1170],\n",
      "        [-1.1079]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0613],\n",
      "        [-1.0762]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1257],\n",
      "        [-1.0365]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0010],\n",
      "        [-1.0562]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0107],\n",
      "        [-1.0177]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0146],\n",
      "        [-0.9388]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0294],\n",
      "        [-1.0549]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0394],\n",
      "        [-0.9399]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0002],\n",
      "        [-0.9870]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0788],\n",
      "        [-0.9932]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0685],\n",
      "        [-1.1065]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0425],\n",
      "        [-1.0221]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1342],\n",
      "        [-1.1385]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1534],\n",
      "        [-1.2042]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2214],\n",
      "        [-1.1899]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2870],\n",
      "        [-1.2120]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4213],\n",
      "        [-1.4552]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5223],\n",
      "        [-1.3587]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4495],\n",
      "        [-1.5307]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.6342],\n",
      "        [-1.6410]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.0331],\n",
      "        [-1.5857]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7264],\n",
      "        [-1.8301]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8619],\n",
      "        [-1.9365]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.2251],\n",
      "        [-2.2692]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8931],\n",
      "        [-1.8847]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8077],\n",
      "        [-1.7541]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.6806],\n",
      "        [-1.5375]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5900],\n",
      "        [-1.6227]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5370],\n",
      "        [-1.5215]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4758],\n",
      "        [-1.4565]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4164],\n",
      "        [-1.4086]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2273],\n",
      "        [-1.4091]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2783],\n",
      "        [-1.3190]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3368],\n",
      "        [-1.2865]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2530],\n",
      "        [-1.2453]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3031],\n",
      "        [-1.3304]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3405],\n",
      "        [-1.4252]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4488],\n",
      "        [-1.3311]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4733],\n",
      "        [-1.3556]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4408],\n",
      "        [-1.3739]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4453],\n",
      "        [-1.4103]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4709],\n",
      "        [-1.4779]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3844],\n",
      "        [-1.4306]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5288],\n",
      "        [-1.3443]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.6357],\n",
      "        [-1.5617]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5429],\n",
      "        [-1.4258]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4523],\n",
      "        [-1.5836]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3524],\n",
      "        [-1.6090]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4606],\n",
      "        [-1.4155]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5842],\n",
      "        [-1.4869]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5023],\n",
      "        [-1.3219]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2970],\n",
      "        [-1.7070]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3428],\n",
      "        [-1.3129]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2423],\n",
      "        [-1.3108]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2410],\n",
      "        [-1.2565]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1432],\n",
      "        [-1.1970]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2660],\n",
      "        [-1.2756]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2607],\n",
      "        [-1.1432]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1542],\n",
      "        [-1.1794]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1281],\n",
      "        [-1.1393]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1388],\n",
      "        [-1.1077]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0858],\n",
      "        [-1.1160]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0920],\n",
      "        [-1.0473]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0171],\n",
      "        [-1.0733]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0861],\n",
      "        [-1.1277]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1165],\n",
      "        [-1.1150]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1145],\n",
      "        [-1.2390]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1314],\n",
      "        [-1.1237]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1386],\n",
      "        [-1.1336]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1954],\n",
      "        [-1.1566]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2071],\n",
      "        [-1.1266]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1714],\n",
      "        [-1.1687]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1393],\n",
      "        [-1.2293]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1455],\n",
      "        [-1.0778]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0911],\n",
      "        [-1.1332]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1838],\n",
      "        [-1.1100]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0583],\n",
      "        [-1.1934]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0999],\n",
      "        [-1.1504]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1421],\n",
      "        [-1.1358]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1316],\n",
      "        [-1.1761]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2062],\n",
      "        [-1.1919]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1314],\n",
      "        [-1.1859]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1953],\n",
      "        [-1.2184]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1300],\n",
      "        [-1.2253]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1397],\n",
      "        [-1.1971]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2071],\n",
      "        [-1.1732]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1145],\n",
      "        [-1.1722]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2414],\n",
      "        [-1.1790]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2527],\n",
      "        [-1.1888]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1708],\n",
      "        [-1.1758]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1188],\n",
      "        [-1.0728]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1589],\n",
      "        [-1.0518]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0818],\n",
      "        [-1.1353]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1207],\n",
      "        [-1.1037]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1186],\n",
      "        [-1.1400]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0669],\n",
      "        [-1.1631]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1840],\n",
      "        [-1.2139]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1652],\n",
      "        [-1.1346]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2539],\n",
      "        [-1.2183]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2806],\n",
      "        [-1.0645]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2678],\n",
      "        [-1.2802]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2521],\n",
      "        [-1.2688]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2778],\n",
      "        [-1.3314]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2981],\n",
      "        [-1.2203]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2575],\n",
      "        [-1.1956]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4573],\n",
      "        [-1.2480]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2933],\n",
      "        [-1.3165]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3882],\n",
      "        [-1.3626]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4449],\n",
      "        [-1.4987]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3119],\n",
      "        [-1.5600]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8059],\n",
      "        [-1.7846]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.9217],\n",
      "        [-1.3540]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.2299],\n",
      "        [-1.9663]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.5856],\n",
      "        [-2.0258]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.2015],\n",
      "        [-2.8190]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8843],\n",
      "        [-1.9116]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.6179],\n",
      "        [-1.7289]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7342],\n",
      "        [-1.5730]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3990],\n",
      "        [-1.5795]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4633],\n",
      "        [-1.4907]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3633],\n",
      "        [-1.3582]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3117],\n",
      "        [-1.2808]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3348],\n",
      "        [-1.3186]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2758],\n",
      "        [-1.2882]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2513],\n",
      "        [-1.2280]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2679],\n",
      "        [-1.2700]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3037],\n",
      "        [-1.3316]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2557],\n",
      "        [-1.3160]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2775],\n",
      "        [-1.2534]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3127],\n",
      "        [-1.3255]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2411],\n",
      "        [-1.3403]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3553],\n",
      "        [-1.2742]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2386],\n",
      "        [-1.2648]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2050],\n",
      "        [-1.1560]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1070],\n",
      "        [-1.1485]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0992],\n",
      "        [-1.0734]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0448],\n",
      "        [-1.0870]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0315],\n",
      "        [-1.0374]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0361],\n",
      "        [-1.0291]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0267],\n",
      "        [-1.0565]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9896],\n",
      "        [-0.9965]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0027],\n",
      "        [-1.0015]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9829],\n",
      "        [-0.9792]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9665],\n",
      "        [-0.9997]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9974],\n",
      "        [-1.0010]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9927],\n",
      "        [-1.0113]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9886],\n",
      "        [-0.9687]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9374],\n",
      "        [-0.9450]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9432],\n",
      "        [-0.9603]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9576],\n",
      "        [-0.9362]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9496],\n",
      "        [-0.9514]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9932],\n",
      "        [-0.9489]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9382],\n",
      "        [-0.9787]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9395],\n",
      "        [-0.9869]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9687],\n",
      "        [-0.9820]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9906],\n",
      "        [-0.9756]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9501],\n",
      "        [-0.9958]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9641],\n",
      "        [-0.9655]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9526],\n",
      "        [-0.9642]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9572],\n",
      "        [-0.9640]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9971],\n",
      "        [-0.9744]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9913],\n",
      "        [-0.9630]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9427],\n",
      "        [-0.9803]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9575],\n",
      "        [-0.9544]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9914],\n",
      "        [-0.9832]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0178],\n",
      "        [-1.0229]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9975],\n",
      "        [-1.0064]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0354],\n",
      "        [-1.0139]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0188],\n",
      "        [-0.9982]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0018],\n",
      "        [-0.9787]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9656],\n",
      "        [-0.9539]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0001],\n",
      "        [-0.9827]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9893],\n",
      "        [-0.9816]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9797],\n",
      "        [-0.9635]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9759],\n",
      "        [-0.9687]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9492],\n",
      "        [-0.9688]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9508],\n",
      "        [-0.9616]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9841],\n",
      "        [-0.9364]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9517],\n",
      "        [-0.9630]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9637],\n",
      "        [-0.9249]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9489],\n",
      "        [-0.9251]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9319],\n",
      "        [-0.9907]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9225],\n",
      "        [-0.8991]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9336],\n",
      "        [-0.9315]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9312],\n",
      "        [-0.9116]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.8870],\n",
      "        [-0.9085]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9086],\n",
      "        [-0.9396]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9045],\n",
      "        [-0.9588]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9319],\n",
      "        [-0.9083]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9645],\n",
      "        [-0.9115]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9826],\n",
      "        [-0.9128]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9882],\n",
      "        [-0.9011]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9249],\n",
      "        [-0.9028]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9098],\n",
      "        [-0.9542]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9484],\n",
      "        [-0.9461]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9282],\n",
      "        [-0.9555]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9238],\n",
      "        [-0.9543]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9144],\n",
      "        [-0.9940]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9594],\n",
      "        [-0.9175]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9523],\n",
      "        [-0.9632]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9924],\n",
      "        [-0.9753]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9749],\n",
      "        [-0.9734]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9895],\n",
      "        [-0.9652]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0532],\n",
      "        [-1.0794]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9679],\n",
      "        [-0.9721]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.9755],\n",
      "        [-1.1825]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0176],\n",
      "        [-0.9999]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0299],\n",
      "        [-0.9777]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0310],\n",
      "        [-1.0077]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0702],\n",
      "        [-1.0889]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1552],\n",
      "        [-1.0854]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1089],\n",
      "        [-1.1182]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1387],\n",
      "        [-1.1547]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1253],\n",
      "        [-1.1436]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1478],\n",
      "        [-1.1596]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1926],\n",
      "        [-1.1581]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1545],\n",
      "        [-1.1218]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1881],\n",
      "        [-1.1192]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1525],\n",
      "        [-1.1229]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1959],\n",
      "        [-1.1410]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1786],\n",
      "        [-1.2207]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1860],\n",
      "        [-1.2465]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2406],\n",
      "        [-1.2905]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3548],\n",
      "        [-1.2598]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3165],\n",
      "        [-1.3045]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4541],\n",
      "        [-1.4276]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4000],\n",
      "        [-1.5576]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7587],\n",
      "        [-1.7273]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5927],\n",
      "        [-1.6591]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.9261],\n",
      "        [-1.7808]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7007],\n",
      "        [-1.9127]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.2051],\n",
      "        [-2.1054]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.5890],\n",
      "        [-1.7498]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.8470],\n",
      "        [-2.1564]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.7837],\n",
      "        [-3.4180]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.6133],\n",
      "        [-4.8225]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.1116],\n",
      "        [-2.3115]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.9874],\n",
      "        [-2.2305]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.9526],\n",
      "        [-1.9332]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.6403],\n",
      "        [-1.6594]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5023],\n",
      "        [-1.4475]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3818],\n",
      "        [-1.4752]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2663],\n",
      "        [-1.3441]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3794],\n",
      "        [-1.3562]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3401],\n",
      "        [-1.2677]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3080],\n",
      "        [-1.3120]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2482],\n",
      "        [-1.3125]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2868],\n",
      "        [-1.2822]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2351],\n",
      "        [-1.3037]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2735],\n",
      "        [-1.2397]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2741],\n",
      "        [-1.2317]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3480],\n",
      "        [-1.3305]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1185],\n",
      "        [-1.3832]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1975],\n",
      "        [-1.2536]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2645],\n",
      "        [-1.2461]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2368],\n",
      "        [-1.1469]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1352],\n",
      "        [-1.1975]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2970],\n",
      "        [-1.2766]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2944],\n",
      "        [-1.2480]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4499],\n",
      "        [-1.4556]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5256],\n",
      "        [-1.6283]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8369],\n",
      "        [-1.8322]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2258],\n",
      "        [-1.5340]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5302],\n",
      "        [-1.3697]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2299],\n",
      "        [-1.2940]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2613],\n",
      "        [-1.3353]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3067],\n",
      "        [-1.2119]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2408],\n",
      "        [-1.1938]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2220],\n",
      "        [-1.2782]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2144],\n",
      "        [-1.2464]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2584],\n",
      "        [-1.3555]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3183],\n",
      "        [-1.3362]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3540],\n",
      "        [-1.3458]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3565],\n",
      "        [-1.4699]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3730],\n",
      "        [-1.3924]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3752],\n",
      "        [-1.3695]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4157],\n",
      "        [-1.4487]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3450],\n",
      "        [-1.3417]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3465],\n",
      "        [-1.3547]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3619],\n",
      "        [-1.3411]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3692],\n",
      "        [-1.3469]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3807],\n",
      "        [-1.3644]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3956],\n",
      "        [-1.3870]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3958],\n",
      "        [-1.4099]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4374],\n",
      "        [-1.4816]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4403],\n",
      "        [-1.4915]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4649],\n",
      "        [-1.4426]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4912],\n",
      "        [-1.5037]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5674],\n",
      "        [-1.5388]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5004],\n",
      "        [-1.5287]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.6512],\n",
      "        [-1.5746]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.6177],\n",
      "        [-1.6089]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.6568],\n",
      "        [-1.6305]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.6248],\n",
      "        [-1.5421]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.6247],\n",
      "        [-1.5722]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.6702],\n",
      "        [-1.6433]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.6519],\n",
      "        [-1.6486]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7313],\n",
      "        [-1.6276]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7991],\n",
      "        [-1.7506]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8919],\n",
      "        [-1.6930]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7785],\n",
      "        [-1.8054]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.1212],\n",
      "        [-1.9121]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.1877],\n",
      "        [-2.1358]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.9596],\n",
      "        [-2.0904]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7176],\n",
      "        [-1.9358]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.6704],\n",
      "        [-1.6867]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.6398],\n",
      "        [-1.5993]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5525],\n",
      "        [-1.5312]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5484],\n",
      "        [-1.5121]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4855],\n",
      "        [-1.4535]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4320],\n",
      "        [-1.4512]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3971],\n",
      "        [-1.3739]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3584],\n",
      "        [-1.3492]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3452],\n",
      "        [-1.3309]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3319],\n",
      "        [-1.3508]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3121],\n",
      "        [-1.3052]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3053],\n",
      "        [-1.3019]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2856],\n",
      "        [-1.2701]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2409],\n",
      "        [-1.2860]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2499],\n",
      "        [-1.1968]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2137],\n",
      "        [-1.1819]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1800],\n",
      "        [-1.1807]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1324],\n",
      "        [-1.1498]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1078],\n",
      "        [-1.1242]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0873],\n",
      "        [-1.1064]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0572],\n",
      "        [-1.0897]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0582],\n",
      "        [-1.0550]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0501],\n",
      "        [-1.0678]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0523],\n",
      "        [-1.0449]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0464],\n",
      "        [-1.0181]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0609],\n",
      "        [-1.0498]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0663],\n",
      "        [-1.0498]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0480],\n",
      "        [-1.0774]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0893],\n",
      "        [-1.0998]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0717],\n",
      "        [-1.0725]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1142],\n",
      "        [-1.1160]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1103],\n",
      "        [-1.1256]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0969],\n",
      "        [-1.1052]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1421],\n",
      "        [-1.1268]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1387],\n",
      "        [-1.1468]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1485],\n",
      "        [-1.1545]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1499],\n",
      "        [-1.1630]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2029],\n",
      "        [-1.1793]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2042],\n",
      "        [-1.2045]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2172],\n",
      "        [-1.2127]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2304],\n",
      "        [-1.2487]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2624],\n",
      "        [-1.2669]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2819],\n",
      "        [-1.3256]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3188],\n",
      "        [-1.3507]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3407],\n",
      "        [-1.3118]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3413],\n",
      "        [-1.3475]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3084],\n",
      "        [-1.3436]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3087],\n",
      "        [-1.3687]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3706],\n",
      "        [-1.3510]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3584],\n",
      "        [-1.3330]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3173],\n",
      "        [-1.3329]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2913],\n",
      "        [-1.2815]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3167],\n",
      "        [-1.2756]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2749],\n",
      "        [-1.3051]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2854],\n",
      "        [-1.2607]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2854],\n",
      "        [-1.2688]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2509],\n",
      "        [-1.2747]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2674],\n",
      "        [-1.2692]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3027],\n",
      "        [-1.2912]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2691],\n",
      "        [-1.2860]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3097],\n",
      "        [-1.2947]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3042],\n",
      "        [-1.2974]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3358],\n",
      "        [-1.3387]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3246],\n",
      "        [-1.3602]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3539],\n",
      "        [-1.3745]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4056],\n",
      "        [-1.3765]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4564],\n",
      "        [-1.4427]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4654],\n",
      "        [-1.5184]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5641],\n",
      "        [-1.5677]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.6163],\n",
      "        [-1.5580]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.6027],\n",
      "        [-1.5963]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.6775],\n",
      "        [-1.6700]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7067],\n",
      "        [-1.7043]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7284],\n",
      "        [-1.7450]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7821],\n",
      "        [-1.7587]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8158],\n",
      "        [-1.8267]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8782],\n",
      "        [-1.8581]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8896],\n",
      "        [-1.8979]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8606],\n",
      "        [-1.9670]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.0670],\n",
      "        [-1.8547]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.9885],\n",
      "        [-2.0873]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.0714],\n",
      "        [-1.9878]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.9549],\n",
      "        [-2.1094]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.0099],\n",
      "        [-2.1060]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.0248],\n",
      "        [-2.0706]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8846],\n",
      "        [-1.9499]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8463],\n",
      "        [-1.8605]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8382],\n",
      "        [-1.7393]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7110],\n",
      "        [-1.7013]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.6450],\n",
      "        [-1.6377]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5994],\n",
      "        [-1.6325]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5492],\n",
      "        [-1.5298]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5149],\n",
      "        [-1.5109]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4709],\n",
      "        [-1.4928]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4426],\n",
      "        [-1.4248]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4279],\n",
      "        [-1.4341]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3918],\n",
      "        [-1.4009]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4077],\n",
      "        [-1.3669]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3653],\n",
      "        [-1.3656]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3489],\n",
      "        [-1.3576]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3354],\n",
      "        [-1.3392]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2996],\n",
      "        [-1.3039]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3048],\n",
      "        [-1.2890]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3177],\n",
      "        [-1.2593]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2494],\n",
      "        [-1.3149]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2660],\n",
      "        [-1.3043]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2738],\n",
      "        [-1.2852]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3165],\n",
      "        [-1.2546]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2730],\n",
      "        [-1.3108]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2897],\n",
      "        [-1.2247]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2405],\n",
      "        [-1.2326]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2355],\n",
      "        [-1.2285]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2065],\n",
      "        [-1.2045]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2025],\n",
      "        [-1.1534]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2098],\n",
      "        [-1.2071]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2276],\n",
      "        [-1.2150]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1862],\n",
      "        [-1.2000]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2069],\n",
      "        [-1.2232]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1997],\n",
      "        [-1.2002]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1907],\n",
      "        [-1.2038]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2103],\n",
      "        [-1.1889]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2377],\n",
      "        [-1.2351]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2373],\n",
      "        [-1.2158]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2032],\n",
      "        [-1.1958]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2533],\n",
      "        [-1.2590]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2259],\n",
      "        [-1.2424]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2421],\n",
      "        [-1.2215]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2411],\n",
      "        [-1.2367]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2882],\n",
      "        [-1.2871]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2874],\n",
      "        [-1.2584]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2896],\n",
      "        [-1.2572]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2754],\n",
      "        [-1.2924]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2375],\n",
      "        [-1.2707]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1959],\n",
      "        [-1.2398]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1938],\n",
      "        [-1.1787]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1788],\n",
      "        [-1.1639]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1690],\n",
      "        [-1.1673]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1610],\n",
      "        [-1.1127]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1355],\n",
      "        [-1.1289]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1616],\n",
      "        [-1.1650]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1710],\n",
      "        [-1.1341]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1317],\n",
      "        [-1.1046]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0459],\n",
      "        [-1.1213]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1008],\n",
      "        [-1.1077]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0550],\n",
      "        [-1.0767]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0400],\n",
      "        [-1.0748]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0674],\n",
      "        [-1.0607]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0642],\n",
      "        [-1.0562]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0926],\n",
      "        [-1.0773]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0635],\n",
      "        [-1.0797]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1023],\n",
      "        [-1.0919]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1044],\n",
      "        [-1.1170]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1055],\n",
      "        [-1.0995]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1088],\n",
      "        [-1.1460]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1855],\n",
      "        [-1.1699]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1404],\n",
      "        [-1.1634]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1912],\n",
      "        [-1.1692]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1815],\n",
      "        [-1.1826]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1585],\n",
      "        [-1.1517]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1891],\n",
      "        [-1.1718]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1809],\n",
      "        [-1.2224]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1755],\n",
      "        [-1.2141]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2418],\n",
      "        [-1.2021]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2524],\n",
      "        [-1.2547]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2542],\n",
      "        [-1.2637]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2354],\n",
      "        [-1.2845]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2835],\n",
      "        [-1.2630]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2636],\n",
      "        [-1.2660]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2156],\n",
      "        [-1.1943]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2338],\n",
      "        [-1.2257]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1658],\n",
      "        [-1.1845]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1543],\n",
      "        [-1.1601]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1232],\n",
      "        [-1.1450]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1270],\n",
      "        [-1.1247]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1206],\n",
      "        [-1.1080]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1139],\n",
      "        [-1.1196]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1132],\n",
      "        [-1.1183]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1115],\n",
      "        [-1.1248]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1098],\n",
      "        [-1.1019]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1115],\n",
      "        [-1.1157]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1159],\n",
      "        [-1.0976]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1085],\n",
      "        [-1.1417]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.0968],\n",
      "        [-1.1304]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1550],\n",
      "        [-1.1439]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1736],\n",
      "        [-1.1623]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.1980],\n",
      "        [-1.1963]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2080],\n",
      "        [-1.2518]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2536],\n",
      "        [-1.1969]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2073],\n",
      "        [-1.2566]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2617],\n",
      "        [-1.2322]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2467],\n",
      "        [-1.2723]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2804],\n",
      "        [-1.2463]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2357],\n",
      "        [-1.2058]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2510],\n",
      "        [-1.2526]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2613],\n",
      "        [-1.2566]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2344],\n",
      "        [-1.2750]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2572],\n",
      "        [-1.2553]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2952],\n",
      "        [-1.2927]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3007],\n",
      "        [-1.3166]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.2992],\n",
      "        [-1.3377]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3551],\n",
      "        [-1.3482]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3569],\n",
      "        [-1.3452]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3470],\n",
      "        [-1.3737]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3814],\n",
      "        [-1.3737]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3784],\n",
      "        [-1.3932]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4282],\n",
      "        [-1.4502]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4790],\n",
      "        [-1.4631]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4909],\n",
      "        [-1.4943]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5547],\n",
      "        [-1.5332]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5730],\n",
      "        [-1.5407]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.6107],\n",
      "        [-1.6018]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5934],\n",
      "        [-1.6206]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.6598],\n",
      "        [-1.6289]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.6839],\n",
      "        [-1.6191]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.6011],\n",
      "        [-1.7058]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7383],\n",
      "        [-1.7770]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7127],\n",
      "        [-1.7108]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8342],\n",
      "        [-1.7664]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8188],\n",
      "        [-1.7468]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8136],\n",
      "        [-1.8479]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7961],\n",
      "        [-1.7602]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7409],\n",
      "        [-1.7494]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7915],\n",
      "        [-1.8064]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7855],\n",
      "        [-1.7704]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7945],\n",
      "        [-1.8103]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7231],\n",
      "        [-1.7321]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7862],\n",
      "        [-1.8234]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7730],\n",
      "        [-1.7895]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7578],\n",
      "        [-1.7709]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8341],\n",
      "        [-1.8240]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8007],\n",
      "        [-1.7587]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8371],\n",
      "        [-2.0847]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8515],\n",
      "        [-1.8482]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.9142],\n",
      "        [-1.8127]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8806],\n",
      "        [-1.8711]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.9847],\n",
      "        [-1.9570]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.9746],\n",
      "        [-1.9702]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.0837],\n",
      "        [-2.0542]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.1549],\n",
      "        [-2.1779]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.1720],\n",
      "        [-2.1707]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.1231],\n",
      "        [-2.1393]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.2167],\n",
      "        [-2.2202]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.1883],\n",
      "        [-2.3793]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.3520],\n",
      "        [-2.2042]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.2372],\n",
      "        [-2.2885]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.2789],\n",
      "        [-2.1702]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.1951],\n",
      "        [-2.1288]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.1588],\n",
      "        [-2.1521]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.1101],\n",
      "        [-1.9693]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-2.0238],\n",
      "        [-1.9505]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8927],\n",
      "        [-1.9222]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8114],\n",
      "        [-1.8762]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.8083],\n",
      "        [-1.7943]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7567],\n",
      "        [-1.7288]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.7259],\n",
      "        [-1.7173]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.6997],\n",
      "        [-1.6265]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.6825],\n",
      "        [-1.6301]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.6349],\n",
      "        [-1.5595]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5565],\n",
      "        [-1.5126]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5577],\n",
      "        [-1.5363]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4903],\n",
      "        [-1.5074]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.5605],\n",
      "        [-1.4859]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.4047],\n",
      "        [-1.4149]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-1.3482],\n",
      "        [-1.3279]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[113], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m trainer_dxtx \u001B[38;5;241m=\u001B[39m Trainer(model\u001B[38;5;241m=\u001B[39mmodel_dxtx)\n\u001B[0;32m----> 2\u001B[0m \u001B[43mtrainer_dxtx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmonitor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpr_auc\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/pyhealth/trainer.py:208\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, train_dataloader, val_dataloader, test_dataloader, epochs, optimizer_class, optimizer_params, weight_decay, max_grad_norm, monitor, monitor_criterion, load_best_model_at_last)\u001B[0m\n\u001B[1;32m    204\u001B[0m     torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(\n\u001B[1;32m    205\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mparameters(), max_grad_norm\n\u001B[1;32m    206\u001B[0m     )\n\u001B[1;32m    207\u001B[0m \u001B[38;5;66;03m# update\u001B[39;00m\n\u001B[0;32m--> 208\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    209\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m    210\u001B[0m training_loss\u001B[38;5;241m.\u001B[39mappend(loss\u001B[38;5;241m.\u001B[39mitem())\n",
      "File \u001B[0;32m~/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/optim/optimizer.py:280\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    276\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    277\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    278\u001B[0m                                \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 280\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    281\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    283\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/optim/optimizer.py:33\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     32\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m---> 33\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     35\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(prev_grad)\n",
      "File \u001B[0;32m~/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/optim/adam.py:141\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    130\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m    132\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[1;32m    133\u001B[0m         group,\n\u001B[1;32m    134\u001B[0m         params_with_grad,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    138\u001B[0m         max_exp_avg_sqs,\n\u001B[1;32m    139\u001B[0m         state_steps)\n\u001B[0;32m--> 141\u001B[0m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    142\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    143\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    144\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    145\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    146\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    147\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    148\u001B[0m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    149\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    150\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    152\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    153\u001B[0m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    155\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    156\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    157\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    159\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrad_scale\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m~/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/optim/adam.py:281\u001B[0m, in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    279\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[0;32m--> 281\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    282\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    283\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    284\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    286\u001B[0m \u001B[43m     \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    287\u001B[0m \u001B[43m     \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    288\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    289\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    290\u001B[0m \u001B[43m     \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    291\u001B[0m \u001B[43m     \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    292\u001B[0m \u001B[43m     \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    293\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    294\u001B[0m \u001B[43m     \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    295\u001B[0m \u001B[43m     \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    296\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    297\u001B[0m \u001B[43m     \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/optim/adam.py:344\u001B[0m, in \u001B[0;36m_single_tensor_adam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[1;32m    341\u001B[0m     param \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mview_as_real(param)\n\u001B[1;32m    343\u001B[0m \u001B[38;5;66;03m# Decay the first and second moment running average coefficient\u001B[39;00m\n\u001B[0;32m--> 344\u001B[0m \u001B[43mexp_avg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmul_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malpha\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    345\u001B[0m exp_avg_sq\u001B[38;5;241m.\u001B[39mmul_(beta2)\u001B[38;5;241m.\u001B[39maddcmul_(grad, grad\u001B[38;5;241m.\u001B[39mconj(), value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta2)\n\u001B[1;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m capturable \u001B[38;5;129;01mor\u001B[39;00m differentiable:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trainer_dxtx = Trainer(model=model_dxtx)\n",
    "trainer_dxtx.train(\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    epochs=5,\n",
    "    monitor=\"pr_auc\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T18:15:41.212971Z",
     "end_time": "2023-04-22T18:15:42.940102Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation:   0%|          | 0/22 [00:00<?, ?it/s]/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Evaluation: 100%|██████████| 22/22 [00:00<00:00, 25.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pr_auc': 0.17073170731707318, 'roc_auc': 0.5, 'f1': 0.0, 'loss': 0.6380227506160736}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation:   0%|          | 0/22 [00:00<?, ?it/s]/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Evaluation: 100%|██████████| 22/22 [00:00<00:00, 25.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'pr_auc': 0.17073170731707318}"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# option 1: use our built-in evaluation metric\n",
    "score_dxtx = trainer_dxtx.evaluate(test_loader)\n",
    "print (score_dxtx)\n",
    "\n",
    "# option 2: use our pyhealth.metrics to evaluate\n",
    "y_true_dxtx, y_prob_dxtx, loss_dxtx = trainer_dxtx.inference(test_loader)\n",
    "binary_metrics_fn(y_true_dxtx, y_prob_dxtx, metrics=[\"pr_auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
