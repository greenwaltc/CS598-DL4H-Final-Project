{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Library imports and data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-04T00:52:58.115437Z",
     "end_time": "2023-05-04T00:53:00.342441Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camer\\PycharmProjects\\CS598-DL4H-Final-Project\\venv\\lib\\site-packages\\pyhealth\\trainer.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    }
   ],
   "source": [
    "# ! pip install pyhealth\n",
    "from pyhealth.datasets import MIMIC3Dataset\n",
    "from pyhealth.data import Patient, Visit, Event\n",
    "from pyhealth.datasets import split_by_patient, get_dataloader\n",
    "from pyhealth.trainer import Trainer\n",
    "from pyhealth.medcode import InnerMap\n",
    "from tqdm import tqdm\n",
    "from pyhealth.metrics.binary import binary_metrics_fn\n",
    "from sklearn.metrics import precision_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from typing import List\n",
    "\n",
    "# Set this to the directory with all MIMIC-3 dataset files\n",
    "data_root = \"data\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s-VgeMdJDYYK",
    "outputId": "9cb9c08a-9003-4836-a3b2-a23e7cd45b55",
    "ExecuteTime": {
     "start_time": "2023-05-04T00:53:00.343440Z",
     "end_time": "2023-05-04T00:53:03.764422Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "mimic3_ds = MIMIC3Dataset(\n",
    "        root=data_root,\n",
    "        tables=[\"DIAGNOSES_ICD\", \"PROCEDURES_ICD\"],\n",
    "        dev=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "HQdeU1TLU6m_",
    "outputId": "590623d6-2917-4155-bf0a-5d75a3979c25",
    "ExecuteTime": {
     "start_time": "2023-05-04T00:53:03.765421Z",
     "end_time": "2023-05-04T00:53:03.969422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics of base dataset (dev=False):\n",
      "\t- Dataset: MIMIC3Dataset\n",
      "\t- Number of patients: 46520\n",
      "\t- Number of visits: 58976\n",
      "\t- Number of visits per patient: 1.2678\n",
      "\t- Number of events per visit in DIAGNOSES_ICD: 11.0384\n",
      "\t- Number of events per visit in PROCEDURES_ICD: 4.0711\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nStatistics of base dataset (dev=False):\\n\\t- Dataset: MIMIC3Dataset\\n\\t- Number of patients: 46520\\n\\t- Number of visits: 58976\\n\\t- Number of visits per patient: 1.2678\\n\\t- Number of events per visit in DIAGNOSES_ICD: 11.0384\\n\\t- Number of events per visit in PROCEDURES_ICD: 4.0711\\n'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print dataset statistics\n",
    "\n",
    "mimic3_ds.stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Find all diagnoses codes\n",
    "# Remove diagnoses codes with fewer than 5 occurences in the dataset\n",
    "\n",
    "all_diagnosis_codes = []\n",
    "for patient_id, patient in mimic3_ds.patients.items():\n",
    "  for i in range(len(patient)):\n",
    "    visit: Visit = patient[i]\n",
    "    conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "    all_diagnosis_codes.extend(conditions)\n",
    "\n",
    "codes = pd.Series(all_diagnosis_codes)\n",
    "diag_code_counts = codes.value_counts()\n",
    "filtered_diag_codes = diag_code_counts[diag_code_counts > 4].index.values\n",
    "n_unique_diag_codes = len(filtered_diag_codes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-04T00:53:03.971422Z",
     "end_time": "2023-05-04T00:53:04.327420Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46520/46520 [01:10<00:00, 655.77it/s]\n"
     ]
    }
   ],
   "source": [
    "MIN_N_VISITS_PER_PATIENT = 2\n",
    "\n",
    "# Filter Dataset to requirements specified in paper\n",
    "\n",
    "filtered_patients = {}\n",
    "for patient_id, patient in tqdm(mimic3_ds.patients.items()):\n",
    "\n",
    "    filtered_patient: Patient = Patient(\n",
    "        patient_id=patient.patient_id,\n",
    "        birth_datetime=patient.birth_datetime,\n",
    "        death_datetime=patient.death_datetime,\n",
    "        gender=patient.gender,\n",
    "        ethnicity=patient.ethnicity\n",
    "    )\n",
    "\n",
    "    for i_visit, visit in enumerate(patient):\n",
    "        filtered_visit: Visit = Visit(\n",
    "            visit_id=visit.visit_id,\n",
    "            patient_id=visit.patient_id,\n",
    "            encounter_time=visit.encounter_time,\n",
    "            discharge_time=visit.discharge_time,\n",
    "            discharge_status=visit.discharge_status\n",
    "        )\n",
    "\n",
    "        diagnoses_codes = visit.get_code_list(\"DIAGNOSES_ICD\")\n",
    "        procedures_codes = visit.get_code_list(\"PROCEDURES_ICD\")\n",
    "        prescriptions_codes = visit.get_code_list(\"PRESCRIPTIONS\")\n",
    "\n",
    "        if len(diagnoses_codes) > 0:\n",
    "            diagnosis_events = visit.event_list_dict[\"DIAGNOSES_ICD\"]\n",
    "            for i_event in range(len(diagnosis_events) - 1, -1, -1):\n",
    "                event: Event = diagnosis_events[i_event]\n",
    "                if event.code not in filtered_diag_codes:\n",
    "                    diagnosis_events.pop(i_event) # Remove the diagnosis code with fewer than the cutoff occurrences\n",
    "\n",
    "            if len(diagnosis_events) == 0: continue # Don't include visits with no diagnoses\n",
    "\n",
    "            filtered_visit.set_event_list(\"DIAGNOSES_ICD\", diagnosis_events)\n",
    "        else:\n",
    "            continue # Don't include visits with no diagnoses\n",
    "\n",
    "        if len(procedures_codes) > 0:\n",
    "           filtered_visit.set_event_list(\"PROCEDURES_ICD\", visit.event_list_dict[\"PROCEDURES_ICD\"])\n",
    "\n",
    "        if len(prescriptions_codes) > 0:\n",
    "            filtered_visit.set_event_list(\"PRESCRIPTIONS\", visit.event_list_dict[\"PRESCRIPTIONS\"])\n",
    "\n",
    "        filtered_patient.add_visit(filtered_visit)\n",
    "\n",
    "    if len(filtered_patient.visits) >= MIN_N_VISITS_PER_PATIENT:\n",
    "        filtered_patients[patient_id] = filtered_patient\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-04T00:53:04.332422Z",
     "end_time": "2023-05-04T00:54:15.280198Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics of base dataset (dev=False):\n",
      "\t- Dataset: MIMIC3Dataset\n",
      "\t- Number of patients: 7496\n",
      "\t- Number of visits: 19905\n",
      "\t- Number of visits per patient: 2.6554\n",
      "\t- Number of events per visit in DIAGNOSES_ICD: 12.9735\n",
      "\t- Number of events per visit in PROCEDURES_ICD: 4.0975\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nStatistics of base dataset (dev=False):\\n\\t- Dataset: MIMIC3Dataset\\n\\t- Number of patients: 7496\\n\\t- Number of visits: 19905\\n\\t- Number of visits per patient: 2.6554\\n\\t- Number of events per visit in DIAGNOSES_ICD: 12.9735\\n\\t- Number of events per visit in PROCEDURES_ICD: 4.0975\\n'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic3_ds.patients = filtered_patients\n",
    "mimic3_ds.stat()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-04T00:54:15.282196Z",
     "end_time": "2023-05-04T00:54:15.484199Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fgrnO7KkWDBY",
    "ExecuteTime": {
     "start_time": "2023-05-04T00:54:15.490200Z",
     "end_time": "2023-05-04T00:54:15.529208Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the tasks\n",
    "\n",
    "DIAGNOSES_KEY = \"conditions\"\n",
    "PROCEDURES_KEY = \"procedures\"\n",
    "INTERVAL_DAYS_KEY = \"days_since_first_visit\"\n",
    "\n",
    "icd9cm = InnerMap.load(\"ICD9CM\")\n",
    "\n",
    "def flatten(l: List):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def patient_level_readmission_prediction(patient, time_window: int = 30, max_length_visits: int = None):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    sorted_visits = sorted(patient, key=lambda visit: visit.encounter_time)\n",
    "\n",
    "    # Clip the patient visits to the most recent max_length_visits + 1 if max_length_visits is not None\n",
    "    if max_length_visits is not None:\n",
    "        n_visits = len(sorted_visits)\n",
    "        if n_visits > max_length_visits + 1:\n",
    "            sorted_visits = sorted_visits[n_visits - (max_length_visits + 1):]\n",
    "\n",
    "    feature_visits: List[Visit] = sorted_visits[:-1]\n",
    "    last_visit: Visit = sorted_visits[-1]\n",
    "    second_to_last_visit: Visit = feature_visits[-1]\n",
    "    first_visit: Visit = feature_visits[0]\n",
    "\n",
    "    # step 1 a: define readmission label\n",
    "    time_diff = (last_visit.encounter_time - second_to_last_visit.encounter_time).days\n",
    "    readmission_label = 1 if time_diff <= time_window else 0\n",
    "\n",
    "    # step 1 b: define diagnosis prediction label\n",
    "    diagnosis_label = list(set([icd9cm.get_ancestors(code)[1] for code in last_visit.get_code_list(\"DIAGNOSES_ICD\")]))\n",
    "\n",
    "    # step 2: obtain features\n",
    "    visits_diagnoses = []\n",
    "    visits_procedures = []\n",
    "    visits_intervals = []\n",
    "    for idx, visit in enumerate(feature_visits):\n",
    "        diagnoses = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        time_diff_from_first_visit = (visit.encounter_time - first_visit.encounter_time).days\n",
    "\n",
    "        # Exclude visits that are missing either diagnoses or procedures.\n",
    "        # BiteNet can handle missing procedures, but other PyHealth models like RNN\n",
    "        # require all features have a length greater than 0.\n",
    "        if len(diagnoses) == 0:\n",
    "            continue\n",
    "\n",
    "        visits_diagnoses.append(diagnoses)\n",
    "        visits_procedures.append(procedures)\n",
    "        visits_intervals.append([str(time_diff_from_first_visit)])\n",
    "\n",
    "    unique_diagnoses = list(set(flatten(visits_diagnoses)))\n",
    "\n",
    "    # step 3: exclusion criteria\n",
    "    if len(unique_diagnoses) == 0:\n",
    "        return []\n",
    "\n",
    "    # step 4: assemble the sample\n",
    "    samples.append(\n",
    "        {\n",
    "            \"patient_id\": patient.patient_id,\n",
    "            \"visit_id\": visit.visit_id,\n",
    "            \"diagnoses\": visits_diagnoses,\n",
    "            \"procedures\": visits_procedures,\n",
    "            \"intervals\": visits_intervals,\n",
    "            \"readmission_label\": readmission_label,\n",
    "            \"diagnosis_label\": diagnosis_label\n",
    "        }\n",
    "    )\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "KS = list(range(5, 31, 5))\n",
    "N_TRIALS = 5\n",
    "SEQ_LENS = list(range(6, 17, 2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-04T00:54:15.516197Z",
     "end_time": "2023-05-04T00:54:15.538208Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def train_and_inference(model, train_loader, val_loader, test_loader, lr=0.001, monitor=\"pr_auc\", optim = torch.optim.RMSprop):\n",
    "    trainer = Trainer(model=model, device=device)\n",
    "    trainer.train(\n",
    "        train_dataloader=train_loader,\n",
    "        val_dataloader=val_loader,\n",
    "        epochs=10,\n",
    "        monitor=monitor,\n",
    "        optimizer_class=optim,\n",
    "        optimizer_params = {\"lr\" : lr},\n",
    "        load_best_model_at_last=True\n",
    "    )\n",
    "\n",
    "    return trainer.inference(test_loader)\n",
    "\n",
    "def precision_at_k(Y_true, Y_prob):\n",
    "\n",
    "    Y_pred = (Y_prob > 0.5).astype(int)\n",
    "    desc_idx = np.flip(np.argsort(Y_prob, axis=-1), axis=-1)\n",
    "\n",
    "    Y_true = np.take(Y_true, desc_idx)\n",
    "    Y_pred = np.take(Y_pred, desc_idx)\n",
    "\n",
    "    precisions = [\n",
    "        [\n",
    "            precision_score(y_true_sample[:k], y_pred_sample[:k])\n",
    "            for y_true_sample, y_pred_sample in zip(Y_true, Y_pred)\n",
    "        ]\n",
    "        for k in KS\n",
    "    ]\n",
    "\n",
    "    precisions = np.asarray(precisions)\n",
    "    precisions = np.mean(precisions, axis=1)\n",
    "    precisions = {\n",
    "        str(k): p for k, p in zip(KS, precisions.tolist())\n",
    "    }\n",
    "    return precisions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-04T00:54:15.534210Z",
     "end_time": "2023-05-04T00:54:15.547209Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-04T00:54:15.550210Z",
     "end_time": "2023-05-04T00:54:15.566208Z"
    }
   },
   "outputs": [],
   "source": [
    "# DxTx with interval embeddings\n",
    "\n",
    "metrics_df = pd.DataFrame(columns=['model_name', 'feature_set', 'seq_len', 'trial', 'pr_auc', 'roc_auc', 'f1', '5', '10', '15', '20', '25', '30'])\n",
    "\n",
    "def train_and_record_metrics(model_readm, model_diag, model_name, feature_set, seq_len, train_loader, val_loader, test_loader):\n",
    "    global metrics_df\n",
    "\n",
    "    for trial in range(N_TRIALS):\n",
    "\n",
    "        y_true, y_prob, _ = train_and_inference(\n",
    "            model_readm,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            test_loader,\n",
    "            lr=0.001\n",
    "        )\n",
    "        binary_metrics = binary_metrics_fn(y_true, y_prob, metrics=[\"pr_auc\", \"roc_auc\", \"f1\"])\n",
    "\n",
    "        y_true, y_prob, _ = train_and_inference(\n",
    "            model_diag,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            test_loader,\n",
    "            lr=0.001,\n",
    "            monitor=\"pr_auc_samples\",\n",
    "            optim=torch.optim.RMSprop\n",
    "        )\n",
    "        precisions = precision_at_k(y_true, y_prob)\n",
    "\n",
    "        row = binary_metrics | precisions | {\n",
    "            \"model_name\": model_name,\n",
    "            \"feature_set\": feature_set,\n",
    "            \"seq_len\": seq_len,\n",
    "            \"trial\": trial\n",
    "        }\n",
    "        row = {\n",
    "            k: [v] for k, v in row.items()\n",
    "        }\n",
    "\n",
    "        metrics_df = pd.concat([metrics_df, pd.DataFrame.from_dict(row)], ignore_index=True)\n",
    "\n",
    "        # Save df for checkpoint\n",
    "        metrics_df.to_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000023BF740CAC0>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5337\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.2573\n",
      "roc_auc: 0.5682\n",
      "f1: 0.0000\n",
      "loss: 0.5161\n",
      "New best pr_auc score (0.2573) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.5053\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3578\n",
      "roc_auc: 0.5626\n",
      "f1: 0.2474\n",
      "loss: 0.5101\n",
      "New best pr_auc score (0.3578) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.4906\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3748\n",
      "roc_auc: 0.5693\n",
      "f1: 0.2591\n",
      "loss: 0.4853\n",
      "New best pr_auc score (0.3748) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.4787\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3722\n",
      "roc_auc: 0.6001\n",
      "f1: 0.2396\n",
      "loss: 0.4965\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.4793\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3819\n",
      "roc_auc: 0.5988\n",
      "f1: 0.2460\n",
      "loss: 0.4830\n",
      "New best pr_auc score (0.3819) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.4715\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3900\n",
      "roc_auc: 0.6069\n",
      "f1: 0.2513\n",
      "loss: 0.4841\n",
      "New best pr_auc score (0.3900) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.4720\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3724\n",
      "roc_auc: 0.5662\n",
      "f1: 0.2500\n",
      "loss: 0.4932\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.4698\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3904\n",
      "roc_auc: 0.6078\n",
      "f1: 0.2487\n",
      "loss: 0.4929\n",
      "New best pr_auc score (0.3904) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.4680\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3907\n",
      "roc_auc: 0.6015\n",
      "f1: 0.2609\n",
      "loss: 0.4767\n",
      "New best pr_auc score (0.3907) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.4663\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3888\n",
      "roc_auc: 0.5985\n",
      "f1: 0.2567\n",
      "loss: 0.5411\n",
      "Loaded best model\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000023BF740CAC0>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.1088\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.3216\n",
      "loss: 0.0895\n",
      "New best pr_auc_samples score (0.3216) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.0872\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.3313\n",
      "loss: 0.0844\n",
      "New best pr_auc_samples score (0.3313) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0848\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.3442\n",
      "loss: 0.0828\n",
      "New best pr_auc_samples score (0.3442) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0830\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.3643\n",
      "loss: 0.0824\n",
      "New best pr_auc_samples score (0.3643) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0813\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.3777\n",
      "loss: 0.0806\n",
      "New best pr_auc_samples score (0.3777) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0805\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.3822\n",
      "loss: 0.0809\n",
      "New best pr_auc_samples score (0.3822) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0796\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.3912\n",
      "loss: 0.0804\n",
      "New best pr_auc_samples score (0.3912) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0785\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.3971\n",
      "loss: 0.0797\n",
      "New best pr_auc_samples score (0.3971) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0776\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.4042\n",
      "loss: 0.0801\n",
      "New best pr_auc_samples score (0.4042) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0767\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.4029\n",
      "loss: 0.0798\n",
      "Loaded best model\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000023BF740CAC0>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.4843\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.3401\n",
      "roc_auc: 0.5373\n",
      "f1: 0.2604\n",
      "loss: 0.4957\n",
      "New best pr_auc score (0.3401) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.4716\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3900\n",
      "roc_auc: 0.6078\n",
      "f1: 0.2618\n",
      "loss: 0.4837\n",
      "New best pr_auc score (0.3900) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.4637\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3899\n",
      "roc_auc: 0.6043\n",
      "f1: 0.2553\n",
      "loss: 0.4842\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.4601\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3690\n",
      "roc_auc: 0.5819\n",
      "f1: 0.2526\n",
      "loss: 0.5300\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.4546\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3504\n",
      "roc_auc: 0.5528\n",
      "f1: 0.2581\n",
      "loss: 0.5328\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.4405\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.4064\n",
      "roc_auc: 0.5989\n",
      "f1: 0.2623\n",
      "loss: 0.5059\n",
      "New best pr_auc score (0.4064) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.4325\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3717\n",
      "roc_auc: 0.5668\n",
      "f1: 0.2786\n",
      "loss: 0.5241\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.4217\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3714\n",
      "roc_auc: 0.5914\n",
      "f1: 0.2500\n",
      "loss: 0.5267\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.3997\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3774\n",
      "roc_auc: 0.5857\n",
      "f1: 0.2567\n",
      "loss: 0.5341\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.3692\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3722\n",
      "roc_auc: 0.5929\n",
      "f1: 0.2963\n",
      "loss: 0.6153\n",
      "Loaded best model\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000023BF740CAC0>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.0796\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.4019\n",
      "loss: 0.0802\n",
      "New best pr_auc_samples score (0.4019) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.0763\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.4110\n",
      "loss: 0.0798\n",
      "New best pr_auc_samples score (0.4110) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0753\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.4143\n",
      "loss: 0.0800\n",
      "New best pr_auc_samples score (0.4143) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0745\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.4078\n",
      "loss: 0.0811\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0738\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.4142\n",
      "loss: 0.0790\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0731\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.4075\n",
      "loss: 0.0809\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0725\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.4137\n",
      "loss: 0.0826\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0717\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.4190\n",
      "loss: 0.0812\n",
      "New best pr_auc_samples score (0.4190) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0710\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.4182\n",
      "loss: 0.0809\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0703\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.4183\n",
      "loss: 0.0821\n",
      "Loaded best model\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000023BF740CAC0>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5182\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.2367\n",
      "roc_auc: 0.5563\n",
      "f1: 0.0000\n",
      "loss: 0.5248\n",
      "New best pr_auc score (0.2367) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.4798\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3646\n",
      "roc_auc: 0.5632\n",
      "f1: 0.0000\n",
      "loss: 0.5215\n",
      "New best pr_auc score (0.3646) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.4462\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3788\n",
      "roc_auc: 0.5751\n",
      "f1: 0.2703\n",
      "loss: 0.5020\n",
      "New best pr_auc score (0.3788) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.4109\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3769\n",
      "roc_auc: 0.5576\n",
      "f1: 0.2685\n",
      "loss: 0.5080\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.3872\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3832\n",
      "roc_auc: 0.5823\n",
      "f1: 0.2888\n",
      "loss: 0.6093\n",
      "New best pr_auc score (0.3832) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.3573\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3802\n",
      "roc_auc: 0.5775\n",
      "f1: 0.2652\n",
      "loss: 0.5566\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.3430\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3823\n",
      "roc_auc: 0.5810\n",
      "f1: 0.2651\n",
      "loss: 0.7756\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.3329\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3618\n",
      "roc_auc: 0.5824\n",
      "f1: 0.2557\n",
      "loss: 0.7760\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.3126\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3656\n",
      "roc_auc: 0.5495\n",
      "f1: 0.2667\n",
      "loss: 0.5729\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.3006\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3662\n",
      "roc_auc: 0.5540\n",
      "f1: 0.2746\n",
      "loss: 0.6963\n",
      "Loaded best model\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000023BF740CAC0>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.0745\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.4182\n",
      "loss: 0.0832\n",
      "New best pr_auc_samples score (0.4182) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.0710\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.4154\n",
      "loss: 0.0820\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0702\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.4164\n",
      "loss: 0.0815\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0695\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.4202\n",
      "loss: 0.0809\n",
      "New best pr_auc_samples score (0.4202) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0689\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.4153\n",
      "loss: 0.0818\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0682\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.4096\n",
      "loss: 0.0836\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0675\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.4098\n",
      "loss: 0.0832\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0669\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.4025\n",
      "loss: 0.0840\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0664\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.4086\n",
      "loss: 0.0838\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0658\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.4122\n",
      "loss: 0.0841\n",
      "Loaded best model\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000023BF740CAC0>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.4176\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.3905\n",
      "roc_auc: 0.6084\n",
      "f1: 0.2520\n",
      "loss: 0.5367\n",
      "New best pr_auc score (0.3905) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.3724\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3694\n",
      "roc_auc: 0.5470\n",
      "f1: 0.2847\n",
      "loss: 0.6257\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.3578\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3723\n",
      "roc_auc: 0.5674\n",
      "f1: 0.2596\n",
      "loss: 0.4941\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.3407\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3852\n",
      "roc_auc: 0.5943\n",
      "f1: 0.2692\n",
      "loss: 0.8075\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.3243\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3886\n",
      "roc_auc: 0.5955\n",
      "f1: 0.2735\n",
      "loss: 0.5527\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.3063\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3904\n",
      "roc_auc: 0.5963\n",
      "f1: 0.2900\n",
      "loss: 0.6822\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.2991\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3782\n",
      "roc_auc: 0.5718\n",
      "f1: 0.2815\n",
      "loss: 0.7601\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.2887\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3926\n",
      "roc_auc: 0.6031\n",
      "f1: 0.2558\n",
      "loss: 0.6949\n",
      "New best pr_auc score (0.3926) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.2839\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3988\n",
      "roc_auc: 0.6000\n",
      "f1: 0.2974\n",
      "loss: 0.6979\n",
      "New best pr_auc score (0.3988) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.2733\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3662\n",
      "roc_auc: 0.5605\n",
      "f1: 0.2837\n",
      "loss: 0.7392\n",
      "Loaded best model\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000023BF740CAC0>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.0723\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.4135\n",
      "loss: 0.0824\n",
      "New best pr_auc_samples score (0.4135) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.0692\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.4110\n",
      "loss: 0.0814\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0683\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.4186\n",
      "loss: 0.0813\n",
      "New best pr_auc_samples score (0.4186) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0675\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.4135\n",
      "loss: 0.0830\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0669\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.4037\n",
      "loss: 0.0831\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0663\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.4101\n",
      "loss: 0.0837\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0658\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.4060\n",
      "loss: 0.0851\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0652\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.4087\n",
      "loss: 0.0835\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0646\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.4040\n",
      "loss: 0.0841\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0642\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.4019\n",
      "loss: 0.0857\n",
      "Loaded best model\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000023BF740CAC0>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.3208\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.3836\n",
      "roc_auc: 0.5930\n",
      "f1: 0.2857\n",
      "loss: 0.6922\n",
      "New best pr_auc score (0.3836) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.2604\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3692\n",
      "roc_auc: 0.5591\n",
      "f1: 0.2761\n",
      "loss: 0.7597\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.2568\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3715\n",
      "roc_auc: 0.5652\n",
      "f1: 0.2824\n",
      "loss: 0.7085\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.2502\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3453\n",
      "roc_auc: 0.5718\n",
      "f1: 0.2902\n",
      "loss: 0.8003\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.2448\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3788\n",
      "roc_auc: 0.5660\n",
      "f1: 0.2623\n",
      "loss: 0.8577\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.2358\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3829\n",
      "roc_auc: 0.5866\n",
      "f1: 0.2620\n",
      "loss: 0.6846\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.2349\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3621\n",
      "roc_auc: 0.5558\n",
      "f1: 0.2500\n",
      "loss: 0.6913\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.2297\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3614\n",
      "roc_auc: 0.5909\n",
      "f1: 0.2481\n",
      "loss: 0.9921\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.2179\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3630\n",
      "roc_auc: 0.5840\n",
      "f1: 0.2387\n",
      "loss: 0.9339\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.2160\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3703\n",
      "roc_auc: 0.5796\n",
      "f1: 0.2594\n",
      "loss: 0.7707\n",
      "Loaded best model\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000023BF740CAC0>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.0727\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.4170\n",
      "loss: 0.0818\n",
      "New best pr_auc_samples score (0.4170) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.0678\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.4143\n",
      "loss: 0.0821\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0670\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.4159\n",
      "loss: 0.0828\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0665\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.4167\n",
      "loss: 0.0830\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0658\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.4090\n",
      "loss: 0.0835\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0654\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.4067\n",
      "loss: 0.0831\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0648\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.4078\n",
      "loss: 0.0855\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0643\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.4071\n",
      "loss: 0.0864\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0637\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.4105\n",
      "loss: 0.0859\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0631\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.3955\n",
      "loss: 0.0865\n",
      "Loaded best model\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000023BF740CAC0>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5366\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.2559\n",
      "roc_auc: 0.5503\n",
      "f1: 0.0000\n",
      "loss: 0.5145\n",
      "New best pr_auc score (0.2559) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.5094\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3833\n",
      "roc_auc: 0.6058\n",
      "f1: 0.2391\n",
      "loss: 0.4911\n",
      "New best pr_auc score (0.3833) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.4893\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3860\n",
      "roc_auc: 0.5933\n",
      "f1: 0.2613\n",
      "loss: 0.5012\n",
      "New best pr_auc score (0.3860) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.4717\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3941\n",
      "roc_auc: 0.5933\n",
      "f1: 0.2623\n",
      "loss: 0.4767\n",
      "New best pr_auc score (0.3941) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.4401\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3909\n",
      "roc_auc: 0.6005\n",
      "f1: 0.2727\n",
      "loss: 0.4818\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.4168\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3773\n",
      "roc_auc: 0.5754\n",
      "f1: 0.2654\n",
      "loss: 0.5069\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.3899\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3899\n",
      "roc_auc: 0.6128\n",
      "f1: 0.2715\n",
      "loss: 0.5402\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.3792\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3881\n",
      "roc_auc: 0.5968\n",
      "f1: 0.2535\n",
      "loss: 0.5755\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.3693\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3770\n",
      "roc_auc: 0.5972\n",
      "f1: 0.2594\n",
      "loss: 0.6149\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.3578\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3727\n",
      "roc_auc: 0.5706\n",
      "f1: 0.2477\n",
      "loss: 0.6382\n",
      "Loaded best model\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000023BF740CAC0>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.1076\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.3258\n",
      "loss: 0.0875\n",
      "New best pr_auc_samples score (0.3258) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.0867\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.3281\n",
      "loss: 0.0841\n",
      "New best pr_auc_samples score (0.3281) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0839\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.3550\n",
      "loss: 0.0824\n",
      "New best pr_auc_samples score (0.3550) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0821\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.3757\n",
      "loss: 0.0818\n",
      "New best pr_auc_samples score (0.3757) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0811\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.3823\n",
      "loss: 0.0808\n",
      "New best pr_auc_samples score (0.3823) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0804\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.3882\n",
      "loss: 0.0801\n",
      "New best pr_auc_samples score (0.3882) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0796\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.3913\n",
      "loss: 0.0831\n",
      "New best pr_auc_samples score (0.3913) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0787\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.3918\n",
      "loss: 0.0810\n",
      "New best pr_auc_samples score (0.3918) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0780\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.3917\n",
      "loss: 0.0807\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0773\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.3976\n",
      "loss: 0.0825\n",
      "New best pr_auc_samples score (0.3976) at epoch-9, step-1880\n",
      "Loaded best model\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000023BF740CAC0>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.4664\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.3862\n",
      "roc_auc: 0.5938\n",
      "f1: 0.2845\n",
      "loss: 0.5117\n",
      "New best pr_auc score (0.3862) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.4214\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3871\n",
      "roc_auc: 0.6033\n",
      "f1: 0.2844\n",
      "loss: 0.5000\n",
      "New best pr_auc score (0.3871) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.4298\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.4041\n",
      "roc_auc: 0.6142\n",
      "f1: 0.2710\n",
      "loss: 0.5188\n",
      "New best pr_auc score (0.4041) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.3846\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3834\n",
      "roc_auc: 0.5969\n",
      "f1: 0.2822\n",
      "loss: 0.5542\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.3688\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3906\n",
      "roc_auc: 0.5930\n",
      "f1: 0.2626\n",
      "loss: 0.5341\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.3615\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3922\n",
      "roc_auc: 0.5979\n",
      "f1: 0.2896\n",
      "loss: 0.5301\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.3636\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3974\n",
      "roc_auc: 0.6046\n",
      "f1: 0.3089\n",
      "loss: 0.5760\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.3449\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3809\n",
      "roc_auc: 0.5836\n",
      "f1: 0.2621\n",
      "loss: 0.5870\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.3466\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3946\n",
      "roc_auc: 0.6088\n",
      "f1: 0.2831\n",
      "loss: 0.5943\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.3366\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3826\n",
      "roc_auc: 0.5890\n",
      "f1: 0.2830\n",
      "loss: 0.5876\n",
      "Loaded best model\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000023BF740CAC0>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.0786\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.4087\n",
      "loss: 0.0797\n",
      "New best pr_auc_samples score (0.4087) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.0758\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.4098\n",
      "loss: 0.0800\n",
      "New best pr_auc_samples score (0.4098) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0750\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.4091\n",
      "loss: 0.0807\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0744\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.4132\n",
      "loss: 0.0814\n",
      "New best pr_auc_samples score (0.4132) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0736\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.4132\n",
      "loss: 0.0807\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0730\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.4124\n",
      "loss: 0.0814\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0722\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.4105\n",
      "loss: 0.0827\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0717\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.4095\n",
      "loss: 0.0822\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0711\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.4104\n",
      "loss: 0.0814\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0703\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.4130\n",
      "loss: 0.0815\n",
      "Loaded best model\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000023BF740CAC0>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.4083\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.3902\n",
      "roc_auc: 0.5938\n",
      "f1: 0.2549\n",
      "loss: 0.5350\n",
      "New best pr_auc score (0.3902) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.3753\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3839\n",
      "roc_auc: 0.6027\n",
      "f1: 0.3460\n",
      "loss: 0.5253\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.3632\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3985\n",
      "roc_auc: 0.6164\n",
      "f1: 0.2162\n",
      "loss: 0.5074\n",
      "New best pr_auc score (0.3985) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.3612\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3791\n",
      "roc_auc: 0.5842\n",
      "f1: 0.3191\n",
      "loss: 0.6575\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.3475\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3995\n",
      "roc_auc: 0.6035\n",
      "f1: 0.3203\n",
      "loss: 0.6869\n",
      "New best pr_auc score (0.3995) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.3444\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3744\n",
      "roc_auc: 0.6029\n",
      "f1: 0.3348\n",
      "loss: 0.5323\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.3421\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3896\n",
      "roc_auc: 0.6020\n",
      "f1: 0.2895\n",
      "loss: 0.6332\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.3286\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.4006\n",
      "roc_auc: 0.6072\n",
      "f1: 0.3187\n",
      "loss: 0.6768\n",
      "New best pr_auc score (0.4006) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.3250\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3965\n",
      "roc_auc: 0.6003\n",
      "f1: 0.2749\n",
      "loss: 0.6907\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.3243\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3606\n",
      "roc_auc: 0.6103\n",
      "f1: 0.3394\n",
      "loss: 0.7466\n",
      "Loaded best model\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000023BF740CAC0>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.0770\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.4114\n",
      "loss: 0.0806\n",
      "New best pr_auc_samples score (0.4114) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.0738\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.4127\n",
      "loss: 0.0804\n",
      "New best pr_auc_samples score (0.4127) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0728\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.4162\n",
      "loss: 0.0810\n",
      "New best pr_auc_samples score (0.4162) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0720\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.4132\n",
      "loss: 0.0818\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0714\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.4174\n",
      "loss: 0.0804\n",
      "New best pr_auc_samples score (0.4174) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0709\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.4144\n",
      "loss: 0.0818\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0703\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.4194\n",
      "loss: 0.0821\n",
      "New best pr_auc_samples score (0.4194) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0697\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.4113\n",
      "loss: 0.0833\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0691\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.4060\n",
      "loss: 0.0830\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0686\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.4126\n",
      "loss: 0.0830\n",
      "Loaded best model\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000023BF740CAC0>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.3981\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.3914\n",
      "roc_auc: 0.6047\n",
      "f1: 0.2476\n",
      "loss: 0.5729\n",
      "New best pr_auc score (0.3914) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.3216\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3739\n",
      "roc_auc: 0.5934\n",
      "f1: 0.2544\n",
      "loss: 0.6828\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.3159\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3590\n",
      "roc_auc: 0.5832\n",
      "f1: 0.2500\n",
      "loss: 0.8124\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.3174\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3462\n",
      "roc_auc: 0.5909\n",
      "f1: 0.2477\n",
      "loss: 0.6016\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.3107\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3303\n",
      "roc_auc: 0.6086\n",
      "f1: 0.2522\n",
      "loss: 0.7000\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.3029\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.2936\n",
      "roc_auc: 0.5938\n",
      "f1: 0.3115\n",
      "loss: 0.7686\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.3099\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3565\n",
      "roc_auc: 0.5693\n",
      "f1: 0.2783\n",
      "loss: 0.6827\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.3022\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3594\n",
      "roc_auc: 0.5911\n",
      "f1: 0.2818\n",
      "loss: 0.6726\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.2956\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3555\n",
      "roc_auc: 0.6006\n",
      "f1: 0.2943\n",
      "loss: 0.9418\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.2924\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3717\n",
      "roc_auc: 0.5851\n",
      "f1: 0.2936\n",
      "loss: 0.7791\n",
      "Loaded best model\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000023BF740CAC0>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.0725\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.4136\n",
      "loss: 0.0830\n",
      "New best pr_auc_samples score (0.4136) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.0699\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.4088\n",
      "loss: 0.0844\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0691\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.4103\n",
      "loss: 0.0820\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0686\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.4105\n",
      "loss: 0.0824\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0681\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.4150\n",
      "loss: 0.0832\n",
      "New best pr_auc_samples score (0.4150) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0673\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.4081\n",
      "loss: 0.0837\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0669\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.4048\n",
      "loss: 0.0844\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0663\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.4061\n",
      "loss: 0.0842\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0659\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.4052\n",
      "loss: 0.0838\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0653\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.4086\n",
      "loss: 0.0878\n",
      "Loaded best model\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000023BF740CAC0>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.3792\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.3632\n",
      "roc_auc: 0.6051\n",
      "f1: 0.2752\n",
      "loss: 0.5766\n",
      "New best pr_auc score (0.3632) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.3318\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3717\n",
      "roc_auc: 0.5881\n",
      "f1: 0.2660\n",
      "loss: 0.5669\n",
      "New best pr_auc score (0.3717) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.3222\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3736\n",
      "roc_auc: 0.6012\n",
      "f1: 0.2845\n",
      "loss: 0.5680\n",
      "New best pr_auc score (0.3736) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.3138\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3872\n",
      "roc_auc: 0.6114\n",
      "f1: 0.3210\n",
      "loss: 0.6333\n",
      "New best pr_auc score (0.3872) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.3035\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3808\n",
      "roc_auc: 0.5876\n",
      "f1: 0.2819\n",
      "loss: 0.6749\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.2955\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.4067\n",
      "roc_auc: 0.5984\n",
      "f1: 0.3320\n",
      "loss: 0.6442\n",
      "New best pr_auc score (0.4067) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.3039\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3615\n",
      "roc_auc: 0.6154\n",
      "f1: 0.3243\n",
      "loss: 0.7270\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.2945\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3658\n",
      "roc_auc: 0.5734\n",
      "f1: 0.2831\n",
      "loss: 0.7629\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.2986\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3897\n",
      "roc_auc: 0.6167\n",
      "f1: 0.2667\n",
      "loss: 0.6438\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.2852\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3961\n",
      "roc_auc: 0.5949\n",
      "f1: 0.3369\n",
      "loss: 0.8398\n",
      "Loaded best model\n",
      "BiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (intervals): Embedding(1649, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): _BiteNet(\n",
      "    (flatten): Flatten()\n",
      "    (unflatten): Unflatten()\n",
      "    (code_attn): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_fw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_attn_bw): Sequential(\n",
      "      (0): MaskEnc(\n",
      "        (attention): PrePostProcessingWrapper(\n",
      "          (module): MultiHeadAttention(\n",
      "            (q_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (k_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (v_linear): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (fc): PrePostProcessingWrapper(\n",
      "          (module): Sequential(\n",
      "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Dropout(p=0.1, inplace=False)\n",
      "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (layer_norm): LayerNorm()\n",
      "        )\n",
      "        (output_normalization): LayerNorm()\n",
      "      )\n",
      "      (1): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000023BF740CAC0>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.0707\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.4133\n",
      "loss: 0.0851\n",
      "New best pr_auc_samples score (0.4133) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.0678\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.4132\n",
      "loss: 0.0837\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0671\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.4090\n",
      "loss: 0.0838\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0664\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.4092\n",
      "loss: 0.0827\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0660\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.4069\n",
      "loss: 0.0860\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0654\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.4098\n",
      "loss: 0.0840\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0648\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.4051\n",
      "loss: 0.0865\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0645\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.3989\n",
      "loss: 0.0858\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0640\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.4032\n",
      "loss: 0.0851\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0635\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.4034\n",
      "loss: 0.0861\n",
      "Loaded best model\n",
      "RNN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (gru): GRU(128, 128, batch_first=True, dropout=0.1)\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000023BF740CAC0>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.5040\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.3705\n",
      "roc_auc: 0.5916\n",
      "f1: 0.2222\n",
      "loss: 0.4897\n",
      "New best pr_auc score (0.3705) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.3961\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3893\n",
      "roc_auc: 0.6047\n",
      "f1: 0.2538\n",
      "loss: 0.4991\n",
      "New best pr_auc score (0.3893) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.2868\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3811\n",
      "roc_auc: 0.6049\n",
      "f1: 0.2571\n",
      "loss: 0.5289\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.1818\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3912\n",
      "roc_auc: 0.6129\n",
      "f1: 0.3017\n",
      "loss: 0.5889\n",
      "New best pr_auc score (0.3912) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.1019\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3907\n",
      "roc_auc: 0.6083\n",
      "f1: 0.3030\n",
      "loss: 0.6661\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0538\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3772\n",
      "roc_auc: 0.5945\n",
      "f1: 0.2893\n",
      "loss: 0.7570\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0265\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3696\n",
      "roc_auc: 0.5863\n",
      "f1: 0.2996\n",
      "loss: 0.8493\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0128\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3802\n",
      "roc_auc: 0.5924\n",
      "f1: 0.3171\n",
      "loss: 0.9881\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0061\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3724\n",
      "roc_auc: 0.5829\n",
      "f1: 0.2925\n",
      "loss: 1.0911\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0030\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3770\n",
      "roc_auc: 0.5939\n",
      "f1: 0.2972\n",
      "loss: 1.1613\n",
      "Loaded best model\n",
      "RNN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (gru): GRU(128, 128, batch_first=True, dropout=0.1)\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000023BF740CAC0>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.1089\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.3628\n",
      "loss: 0.0905\n",
      "New best pr_auc_samples score (0.3628) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.0839\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.3824\n",
      "loss: 0.0870\n",
      "New best pr_auc_samples score (0.3824) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0809\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.3970\n",
      "loss: 0.0851\n",
      "New best pr_auc_samples score (0.3970) at epoch-2, step-564\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0788\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.4053\n",
      "loss: 0.0844\n",
      "New best pr_auc_samples score (0.4053) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0769\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.4216\n",
      "loss: 0.0837\n",
      "New best pr_auc_samples score (0.4216) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0754\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.4230\n",
      "loss: 0.0837\n",
      "New best pr_auc_samples score (0.4230) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0738\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.4291\n",
      "loss: 0.0834\n",
      "New best pr_auc_samples score (0.4291) at epoch-6, step-1316\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0723\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.4319\n",
      "loss: 0.0833\n",
      "New best pr_auc_samples score (0.4319) at epoch-7, step-1504\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0710\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.4335\n",
      "loss: 0.0831\n",
      "New best pr_auc_samples score (0.4335) at epoch-8, step-1692\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0698\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.4370\n",
      "loss: 0.0828\n",
      "New best pr_auc_samples score (0.4370) at epoch-9, step-1880\n",
      "Loaded best model\n",
      "RNN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (gru): GRU(128, 128, batch_first=True, dropout=0.1)\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000023BF740CAC0>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.1409\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.3806\n",
      "roc_auc: 0.6048\n",
      "f1: 0.3089\n",
      "loss: 0.6673\n",
      "New best pr_auc score (0.3806) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.0471\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3791\n",
      "roc_auc: 0.6031\n",
      "f1: 0.2764\n",
      "loss: 0.7847\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0233\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3750\n",
      "roc_auc: 0.6016\n",
      "f1: 0.2915\n",
      "loss: 0.8547\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0115\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3718\n",
      "roc_auc: 0.5979\n",
      "f1: 0.2573\n",
      "loss: 0.9652\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0050\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3781\n",
      "roc_auc: 0.6032\n",
      "f1: 0.2810\n",
      "loss: 1.0811\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0023\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3737\n",
      "roc_auc: 0.6027\n",
      "f1: 0.2694\n",
      "loss: 1.1667\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0019\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3745\n",
      "roc_auc: 0.5962\n",
      "f1: 0.2903\n",
      "loss: 1.2400\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0006\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3729\n",
      "roc_auc: 0.5958\n",
      "f1: 0.2638\n",
      "loss: 1.3857\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0003\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3761\n",
      "roc_auc: 0.5986\n",
      "f1: 0.3077\n",
      "loss: 1.4473\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0001\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3783\n",
      "roc_auc: 0.5965\n",
      "f1: 0.2944\n",
      "loss: 1.5710\n",
      "Loaded best model\n",
      "RNN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (gru): GRU(128, 128, batch_first=True, dropout=0.1)\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000023BF740CAC0>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.0700\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.4394\n",
      "loss: 0.0836\n",
      "New best pr_auc_samples score (0.4394) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.0672\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.4380\n",
      "loss: 0.0834\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0659\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.4392\n",
      "loss: 0.0837\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0648\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.4386\n",
      "loss: 0.0838\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0636\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.4382\n",
      "loss: 0.0844\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0626\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.4386\n",
      "loss: 0.0845\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0615\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.4364\n",
      "loss: 0.0851\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0605\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.4355\n",
      "loss: 0.0855\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0595\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.4369\n",
      "loss: 0.0861\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0586\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.4337\n",
      "loss: 0.0863\n",
      "Loaded best model\n",
      "RNN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (gru): GRU(128, 128, batch_first=True, dropout=0.1)\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000023BF740CAC0>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.0736\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.3936\n",
      "roc_auc: 0.6115\n",
      "f1: 0.2980\n",
      "loss: 0.7566\n",
      "New best pr_auc score (0.3936) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.0187\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3908\n",
      "roc_auc: 0.6078\n",
      "f1: 0.3154\n",
      "loss: 0.8570\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0096\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3825\n",
      "roc_auc: 0.6031\n",
      "f1: 0.3015\n",
      "loss: 0.9273\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0046\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3823\n",
      "roc_auc: 0.5979\n",
      "f1: 0.2893\n",
      "loss: 1.0762\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0021\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3802\n",
      "roc_auc: 0.5974\n",
      "f1: 0.2941\n",
      "loss: 1.1788\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0009\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3849\n",
      "roc_auc: 0.5998\n",
      "f1: 0.3013\n",
      "loss: 1.2968\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0004\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3897\n",
      "roc_auc: 0.6082\n",
      "f1: 0.2966\n",
      "loss: 1.3711\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0005\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3881\n",
      "roc_auc: 0.6084\n",
      "f1: 0.3083\n",
      "loss: 1.4761\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0009\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc: 0.3856\n",
      "roc_auc: 0.6017\n",
      "f1: 0.3058\n",
      "loss: 1.4887\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0001\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc: 0.3850\n",
      "roc_auc: 0.6007\n",
      "f1: 0.3140\n",
      "loss: 1.5271\n",
      "Loaded best model\n",
      "RNN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (gru): GRU(128, 128, batch_first=True, dropout=0.1)\n",
      "  (fc): Linear(in_features=128, out_features=467, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000023BF740CAC0>\n",
      "Monitor: pr_auc_samples\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.0688\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc_samples: 0.4399\n",
      "loss: 0.0840\n",
      "New best pr_auc_samples score (0.4399) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.0659\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc_samples: 0.4389\n",
      "loss: 0.0842\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0645\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc_samples: 0.4384\n",
      "loss: 0.0845\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0633\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc_samples: 0.4384\n",
      "loss: 0.0849\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0622\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc_samples: 0.4405\n",
      "loss: 0.0851\n",
      "New best pr_auc_samples score (0.4405) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0611\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc_samples: 0.4373\n",
      "loss: 0.0857\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0601\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc_samples: 0.4357\n",
      "loss: 0.0862\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0591\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc_samples: 0.4334\n",
      "loss: 0.0871\n",
      "\n",
      "--- Train epoch-8, step-1692 ---\n",
      "loss: 0.0581\n",
      "--- Eval epoch-8, step-1692 ---\n",
      "pr_auc_samples: 0.4334\n",
      "loss: 0.0877\n",
      "\n",
      "--- Train epoch-9, step-1880 ---\n",
      "loss: 0.0571\n",
      "--- Eval epoch-9, step-1880 ---\n",
      "pr_auc_samples: 0.4327\n",
      "loss: 0.0885\n",
      "Loaded best model\n",
      "RNN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (diagnoses): Embedding(3428, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1358, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (gru): GRU(128, 128, batch_first=True, dropout=0.1)\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: None\n",
      "Device: cuda\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.rmsprop.RMSprop'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000023BF740CAC0>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 10\n",
      "\n",
      "--- Train epoch-0, step-188 ---\n",
      "loss: 0.0417\n",
      "--- Eval epoch-0, step-188 ---\n",
      "pr_auc: 0.3822\n",
      "roc_auc: 0.5977\n",
      "f1: 0.2892\n",
      "loss: 0.8972\n",
      "New best pr_auc score (0.3822) at epoch-0, step-188\n",
      "\n",
      "--- Train epoch-1, step-376 ---\n",
      "loss: 0.0084\n",
      "--- Eval epoch-1, step-376 ---\n",
      "pr_auc: 0.3833\n",
      "roc_auc: 0.5954\n",
      "f1: 0.3047\n",
      "loss: 0.9801\n",
      "New best pr_auc score (0.3833) at epoch-1, step-376\n",
      "\n",
      "--- Train epoch-2, step-564 ---\n",
      "loss: 0.0040\n",
      "--- Eval epoch-2, step-564 ---\n",
      "pr_auc: 0.3816\n",
      "roc_auc: 0.5959\n",
      "f1: 0.3013\n",
      "loss: 1.0749\n",
      "\n",
      "--- Train epoch-3, step-752 ---\n",
      "loss: 0.0019\n",
      "--- Eval epoch-3, step-752 ---\n",
      "pr_auc: 0.3846\n",
      "roc_auc: 0.6014\n",
      "f1: 0.2972\n",
      "loss: 1.1542\n",
      "New best pr_auc score (0.3846) at epoch-3, step-752\n",
      "\n",
      "--- Train epoch-4, step-940 ---\n",
      "loss: 0.0009\n",
      "--- Eval epoch-4, step-940 ---\n",
      "pr_auc: 0.3852\n",
      "roc_auc: 0.5980\n",
      "f1: 0.2954\n",
      "loss: 1.2986\n",
      "New best pr_auc score (0.3852) at epoch-4, step-940\n",
      "\n",
      "--- Train epoch-5, step-1128 ---\n",
      "loss: 0.0005\n",
      "--- Eval epoch-5, step-1128 ---\n",
      "pr_auc: 0.3917\n",
      "roc_auc: 0.6043\n",
      "f1: 0.3051\n",
      "loss: 1.3868\n",
      "New best pr_auc score (0.3917) at epoch-5, step-1128\n",
      "\n",
      "--- Train epoch-6, step-1316 ---\n",
      "loss: 0.0016\n",
      "--- Eval epoch-6, step-1316 ---\n",
      "pr_auc: 0.3806\n",
      "roc_auc: 0.5938\n",
      "f1: 0.3047\n",
      "loss: 1.4347\n",
      "\n",
      "--- Train epoch-7, step-1504 ---\n",
      "loss: 0.0004\n",
      "--- Eval epoch-7, step-1504 ---\n",
      "pr_auc: 0.3873\n",
      "roc_auc: 0.5998\n",
      "f1: 0.2954\n",
      "loss: 1.4491\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "from bitenet import BiteNet\n",
    "from rnn import RNN\n",
    "from retain import RETAIN\n",
    "from deepr import Deepr\n",
    "\n",
    "for seq_len in SEQ_LENS:\n",
    "\n",
    "    dataset = mimic3_ds.set_task(\n",
    "        task_fn=lambda p: patient_level_readmission_prediction(p, max_length_visits=seq_len)\n",
    "    )\n",
    "    train, val, test = split_by_patient(dataset, [0.8, 0.1, 0.1])\n",
    "\n",
    "    train_loader = get_dataloader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = get_dataloader(val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = get_dataloader(test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    #################### BITENET ####################\n",
    "    train_and_record_metrics(\n",
    "        model_readm=BiteNet(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\", \"procedures\", \"intervals\"],\n",
    "            label_key = \"readmission_label\",\n",
    "            mode = \"binary\",\n",
    "        ).to(device),\n",
    "        model_diag=BiteNet(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\", \"procedures\", \"intervals\"],\n",
    "            label_key = \"diagnosis_label\",\n",
    "            mode = \"multilabel\",\n",
    "        ).to(device),\n",
    "        model_name=\"bitenet\",\n",
    "        feature_set=\"dxtx\",\n",
    "        seq_len=seq_len,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader\n",
    "    )\n",
    "\n",
    "    train_and_record_metrics(\n",
    "        model_readm=BiteNet(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\", \"intervals\"],\n",
    "            label_key = \"readmission_label\",\n",
    "            mode = \"binary\",\n",
    "        ).to(device),\n",
    "        model_diag=BiteNet(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\", \"intervals\"],\n",
    "            label_key = \"diagnosis_label\",\n",
    "            mode = \"multilabel\",\n",
    "        ).to(device),\n",
    "        model_name=\"bitenet\",\n",
    "        feature_set=\"dx\",\n",
    "        seq_len=seq_len,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader\n",
    "    )\n",
    "\n",
    "    #################### RNN ####################\n",
    "    train_and_record_metrics(\n",
    "            model_readm=RNN(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "            label_key = \"readmission_label\",\n",
    "            mode = \"binary\",\n",
    "        ).to(device),\n",
    "        model_diag=RNN(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "            label_key = \"diagnosis_label\",\n",
    "            mode = \"multilabel\",\n",
    "        ).to(device),\n",
    "        model_name=\"rnn\",\n",
    "        feature_set=\"dxtx\",\n",
    "        seq_len=seq_len,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader\n",
    "    )\n",
    "\n",
    "    train_and_record_metrics(\n",
    "            model_readm=RNN(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\"],\n",
    "            label_key = \"readmission_label\",\n",
    "            mode = \"binary\",\n",
    "        ).to(device),\n",
    "        model_diag=RNN(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\"],\n",
    "            label_key = \"diagnosis_label\",\n",
    "            mode = \"multilabel\",\n",
    "        ).to(device),\n",
    "        model_name=\"rnn\",\n",
    "        feature_set=\"dx\",\n",
    "        seq_len=seq_len,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader\n",
    "    )\n",
    "\n",
    "    #################### BRNN ####################\n",
    "    train_and_record_metrics(\n",
    "            model_readm=RNN(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "            label_key = \"readmission_label\",\n",
    "            mode = \"binary\",\n",
    "            bidirectional=True\n",
    "        ).to(device),\n",
    "        model_diag=RNN(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "            label_key = \"diagnosis_label\",\n",
    "            mode = \"multilabel\",\n",
    "            bidirectional=True\n",
    "        ).to(device),\n",
    "        model_name=\"brnn\",\n",
    "        feature_set=\"dxtx\",\n",
    "        seq_len=seq_len,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader\n",
    "    )\n",
    "\n",
    "    train_and_record_metrics(\n",
    "            model_readm=RNN(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\"],\n",
    "            label_key = \"readmission_label\",\n",
    "            mode = \"binary\",\n",
    "            bidirectional=True\n",
    "        ).to(device),\n",
    "        model_diag=RNN(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\"],\n",
    "            label_key = \"diagnosis_label\",\n",
    "            mode = \"multilabel\",\n",
    "            bidirectional=True\n",
    "        ).to(device),\n",
    "        model_name=\"brnn\",\n",
    "        feature_set=\"dx\",\n",
    "        seq_len=seq_len,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader\n",
    "    )\n",
    "\n",
    "    #################### RETAIN ####################\n",
    "    train_and_record_metrics(\n",
    "            model_readm=RETAIN(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "            label_key = \"readmission_label\",\n",
    "            mode = \"binary\"\n",
    "        ).to(device),\n",
    "        model_diag=RETAIN(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "            label_key = \"diagnosis_label\",\n",
    "            mode = \"multilabel\"\n",
    "        ).to(device),\n",
    "        model_name=\"retain\",\n",
    "        feature_set=\"dxtx\",\n",
    "        seq_len=seq_len,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader\n",
    "    )\n",
    "\n",
    "    train_and_record_metrics(\n",
    "            model_readm=RETAIN(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\"],\n",
    "            label_key = \"readmission_label\",\n",
    "            mode = \"binary\"\n",
    "        ).to(device),\n",
    "        model_diag=RETAIN(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\"],\n",
    "            label_key = \"diagnosis_label\",\n",
    "            mode = \"multilabel\"\n",
    "        ).to(device),\n",
    "        model_name=\"retain\",\n",
    "        feature_set=\"dx\",\n",
    "        seq_len=seq_len,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader\n",
    "    )\n",
    "\n",
    "    #################### Deepr ####################\n",
    "    train_and_record_metrics(\n",
    "            model_readm=Deepr(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "            label_key = \"readmission_label\",\n",
    "            mode = \"binary\"\n",
    "        ).to(device),\n",
    "        model_diag=Deepr(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\", \"procedures\"],\n",
    "            label_key = \"diagnosis_label\",\n",
    "            mode = \"multilabel\"\n",
    "        ).to(device),\n",
    "        model_name=\"deepr\",\n",
    "        feature_set=\"dxtx\",\n",
    "        seq_len=seq_len,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader\n",
    "    )\n",
    "\n",
    "    train_and_record_metrics(\n",
    "            model_readm=Deepr(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\"],\n",
    "            label_key = \"readmission_label\",\n",
    "            mode = \"binary\"\n",
    "        ).to(device),\n",
    "        model_diag=Deepr(\n",
    "            dataset = dataset,\n",
    "            feature_keys = [\"diagnoses\"],\n",
    "            label_key = \"diagnosis_label\",\n",
    "            mode = \"multilabel\"\n",
    "        ).to(device),\n",
    "        model_name=\"deepr\",\n",
    "        feature_set=\"dx\",\n",
    "        seq_len=seq_len,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-04T00:54:15.563209Z",
     "end_time": "2023-05-04T05:37:07.419163Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "print(metrics_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-04T10:05:46.577123Z",
     "end_time": "2023-05-04T10:05:46.617123Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
