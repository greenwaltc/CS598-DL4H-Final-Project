{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Google Cloud authentication and data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login\n",
    "# ! gcloud auth application-default login\n",
    "# ! gcloud config set project dl4h-final-project-383605"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pCHvVcifGcdr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gzip: *.gz: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# ! pip install --upgrade google-api-python-client google-cloud-storage\n",
    "from google.cloud import storage\n",
    "\n",
    "# Replace these values with your project and bucket as needed\n",
    "project_id = \"dl4h-final-project-383605\"\n",
    "mimic3_bucket = \"mimiciii-1.4.physionet.org\"\n",
    "\n",
    "storage_client = storage.Client(project=project_id)\n",
    "bucket = storage_client.bucket(mimic3_bucket)\n",
    "data_folder = \"./data\"\n",
    "for blob in bucket.list_blobs():\n",
    "  if \"CHARTEVENTS\" in blob.name:\n",
    "    continue\n",
    "  blob.download_to_filename(f\"{data_folder}/{blob.name}\")\n",
    "\n",
    "# Extract all the files\n",
    "! gunzip {data_folder}/*.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gunzip {data_folder}/*.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Library imports and data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T16:13:34.052108Z",
     "end_time": "2023-04-20T16:13:34.057255Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip install pyhealth\n",
    "from pyhealth.datasets import MIMIC3Dataset, SampleDataset\n",
    "from pyhealth.data import Patient, Visit\n",
    "import pandas as pd\n",
    "from pyhealth.datasets import split_by_patient, get_dataloader, BaseDataset\n",
    "from pyhealth.models import BaseModel\n",
    "from pyhealth.trainer import Trainer\n",
    "from pyhealth.metrics.binary import binary_metrics_fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import List, Dict, Optional\n",
    "from enum import Enum\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "\n",
    "# Set this to the directory with all MIMIC-3 dataset files\n",
    "data_root = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s-VgeMdJDYYK",
    "outputId": "9cb9c08a-9003-4836-a3b2-a23e7cd45b55",
    "ExecuteTime": {
     "start_time": "2023-04-20T16:56:52.733023Z",
     "end_time": "2023-04-20T16:57:21.961770Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing PATIENTS and ADMISSIONS: 100%|██████████| 46520/46520 [00:19<00:00, 2423.10it/s]\n",
      "Parsing DIAGNOSES_ICD: 100%|██████████| 58929/58929 [00:03<00:00, 17375.21it/s]\n",
      "Parsing PROCEDURES_ICD: 100%|██████████| 52243/52243 [00:01<00:00, 27342.92it/s]\n",
      "Mapping codes: 100%|██████████| 46520/46520 [00:00<00:00, 167922.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "\n",
    "mimic3_ds = MIMIC3Dataset(\n",
    "        root=data_root,\n",
    "        tables=[\"DIAGNOSES_ICD\", \"PROCEDURES_ICD\"],\n",
    "        dev=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "HQdeU1TLU6m_",
    "outputId": "590623d6-2917-4155-bf0a-5d75a3979c25",
    "ExecuteTime": {
     "start_time": "2023-04-20T16:13:34.704224Z",
     "end_time": "2023-04-20T16:13:34.724846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics of base dataset (dev=True):\n",
      "\t- Dataset: MIMIC3Dataset\n",
      "\t- Number of patients: 1000\n",
      "\t- Number of visits: 1295\n",
      "\t- Number of visits per patient: 1.2950\n",
      "\t- Number of events per visit in DIAGNOSES_ICD: 9.3544\n",
      "\t- Number of events per visit in PROCEDURES_ICD: 4.3351\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nStatistics of base dataset (dev=True):\\n\\t- Dataset: MIMIC3Dataset\\n\\t- Number of patients: 1000\\n\\t- Number of visits: 1295\\n\\t- Number of visits per patient: 1.2950\\n\\t- Number of events per visit in DIAGNOSES_ICD: 9.3544\\n\\t- Number of events per visit in PROCEDURES_ICD: 4.3351\\n'"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print dataset statistics\n",
    "\n",
    "mimic3_ds.stat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2n_iXyWSWE-H",
    "outputId": "4f9561b5-c85e-48a7-b9e2-365f8afc241a",
    "ExecuteTime": {
     "start_time": "2023-04-20T16:59:43.971493Z",
     "end_time": "2023-04-20T16:59:44.221541Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find all diagnoses codes\n",
    "# Find all procedure codes\n",
    "# Remove diagnoses codes with fewer than 5 occurences in the dataset\n",
    "\n",
    "all_diag_codes = []\n",
    "all_proc_codes = []\n",
    "for patient_id, patient in mimic3_ds.patients.items():\n",
    "  for i in range(len(patient)):\n",
    "    visit: Visit = patient[i]\n",
    "    conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "    procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "    all_diag_codes.extend(conditions)\n",
    "    all_proc_codes.extend(procedures)\n",
    "\n",
    "codes = pd.Series(all_diag_codes)\n",
    "diag_code_counts = codes.value_counts()\n",
    "filtered_diag_codes = diag_code_counts[diag_code_counts > 4].index.values\n",
    "num_unique_diag_codes = len(filtered_diag_codes)\n",
    "\n",
    "unique_proc_codes = list(set(all_proc_codes))\n",
    "num_unique_proc_codes = len(unique_proc_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T16:59:46.735897Z",
     "end_time": "2023-04-20T16:59:46.738785Z"
    }
   },
   "outputs": [],
   "source": [
    "proc_code_to_index_map = {}\n",
    "diag_code_to_index_map = {}\n",
    "\n",
    "index = 0\n",
    "for proc_code in unique_proc_codes:\n",
    "    proc_code_to_index_map[proc_code] = index\n",
    "    index += 1\n",
    "\n",
    "index = 0\n",
    "for diag_code in filtered_diag_codes:\n",
    "    diag_code_to_index_map[diag_code] = index\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "fgrnO7KkWDBY",
    "ExecuteTime": {
     "start_time": "2023-04-20T16:59:50.366434Z",
     "end_time": "2023-04-20T16:59:50.401787Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the tasks\n",
    "\n",
    "DIAGNOSES_KEY = \"conditions\"\n",
    "PROCEDURES_KEY = \"procedures\"\n",
    "INTERVAL_DAYS_KEY = \"days_since_first_visit\"\n",
    "\n",
    "def flatten(l: List):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def patient_level_readmission_prediction(patient, time_window=30):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    # if the patient only has one visit, we drop it\n",
    "    if len(patient) == 1:\n",
    "        return []\n",
    "\n",
    "    sorted_visits = sorted(patient, key=lambda visit: visit.encounter_time)\n",
    "\n",
    "    # step 1: define label\n",
    "    idx_last_visit = len(sorted_visits)-1\n",
    "    last_visit: Visit = sorted_visits[idx_last_visit]\n",
    "    second_to_last_visit: Visit = sorted_visits[idx_last_visit - 1]\n",
    "    first_visit: Visit = sorted_visits[0]\n",
    "\n",
    "    time_diff = (last_visit.encounter_time - second_to_last_visit.encounter_time).days\n",
    "    readmission_label = 1 if time_diff < time_window else 0\n",
    "\n",
    "    # step 2: obtain features\n",
    "    visits_conditions = []\n",
    "    visits_procedures = []\n",
    "    visits_intervals = []\n",
    "    for idx, visit in enumerate(sorted_visits):\n",
    "        if idx == len(sorted_visits) - 1: break\n",
    "        conditions = [c for c in visit.get_code_list(table=\"DIAGNOSES_ICD\") if c in filtered_diag_codes]\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        time_diff_from_first_visit = (visit.encounter_time - first_visit.encounter_time).days\n",
    "\n",
    "        if len(conditions) * len(procedures) == 0:\n",
    "            continue\n",
    "\n",
    "        visits_conditions.append(conditions)\n",
    "        visits_procedures.append(procedures)\n",
    "        visits_intervals.append([time_diff_from_first_visit])\n",
    "\n",
    "    unique_conditions = list(set(flatten(visits_conditions)))\n",
    "    unique_procedures = list(set(flatten(visits_procedures)))\n",
    "\n",
    "    # step 3: exclusion criteria\n",
    "    if len(unique_conditions) * len(unique_procedures) == 0:\n",
    "        return []\n",
    "\n",
    "    # step 4: assemble the sample\n",
    "    samples.append(\n",
    "        {\n",
    "            \"patient_id\": patient.patient_id,\n",
    "            \"visit_id\": visit.visit_id,\n",
    "            \"conditions\": visits_conditions,\n",
    "            \"procedures\": visits_procedures,\n",
    "            \"intervals\": visits_intervals,\n",
    "            \"label\": readmission_label,\n",
    "        }\n",
    "    )\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "IMD0MvgA8e8Z",
    "outputId": "11b7f21f-9f3b-419c-ee21-69ec1e3e02fe",
    "ExecuteTime": {
     "start_time": "2023-04-20T16:59:51.327796Z",
     "end_time": "2023-04-20T17:00:00.329790Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples for patient_level_readmission_prediction: 100%|██████████| 46520/46520 [00:08<00:00, 5227.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create the task datasets\n",
    "mimic3_dxtx = mimic3_ds.set_task(task_fn=patient_level_readmission_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T17:00:16.459211Z",
     "end_time": "2023-04-20T17:00:16.462977Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the unique visit intervals\n",
    "all_intervals = []\n",
    "for sample in mimic3_dxtx:\n",
    "    intervals = flatten(sample['intervals'])\n",
    "    all_intervals.extend(intervals)\n",
    "\n",
    "unique_intervals = list(set(all_intervals))\n",
    "num_unique_intervals = len(unique_intervals)\n",
    "\n",
    "visit_interval_to_index_map = {}\n",
    "\n",
    "index = 0\n",
    "for visit_interval in unique_intervals:\n",
    "    visit_interval_to_index_map[visit_interval] = index\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T16:13:38.522858Z",
     "end_time": "2023-04-20T16:13:38.528914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[427,  16, 431, 217, 258]],\n",
      "\n",
      "        [[274,   0,   0,   0,   0]]])\n"
     ]
    }
   ],
   "source": [
    "# def collate_codes(batch_codes, i_patient, patient, feature_key, code_to_index_map, mask_tensor):\n",
    "#     for i_visit, visit_codes in enumerate(patient[feature_key]):\n",
    "#         for i_code, code in enumerate(visit_codes):\n",
    "#             batch_codes[i_patient][i_visit][i_code] = code_to_index_map[code]\n",
    "#\n",
    "#         # Set the mask for the visit codes\n",
    "#         num_codes = len(visit_codes)\n",
    "#         mask_tensor[i_patient][i_visit][:num_codes] = 1\n",
    "#\n",
    "# def collate_fn(batch):\n",
    "#     batch_size = len(batch)\n",
    "#\n",
    "#     max_num_visits = 0\n",
    "#     max_num_diagnosis_codes_per_visit = 0\n",
    "#     max_num_procedure_codes_per_visit = 0\n",
    "#\n",
    "#     for patient in batch:\n",
    "#         patient_num_visits = len(patient['conditions'])\n",
    "#         if patient_num_visits > max_num_visits:\n",
    "#             max_num_visits = patient_num_visits\n",
    "#         for visit_conditions in patient['conditions']:\n",
    "#             num_visit_diagnosis_codes = len(visit_conditions)\n",
    "#             if num_visit_diagnosis_codes > max_num_diagnosis_codes_per_visit:\n",
    "#                 max_num_diagnosis_codes_per_visit = num_visit_diagnosis_codes\n",
    "#         for visit_procedures in patient[\"procedures\"]:\n",
    "#             num_visit_procedure_codes = len(visit_procedures)\n",
    "#             if num_visit_procedure_codes > max_num_procedure_codes_per_visit:\n",
    "#                 max_num_procedure_codes_per_visit = num_visit_procedure_codes\n",
    "#\n",
    "#     batch_procedures = torch.zeros(batch_size, max_num_visits, max_num_procedure_codes_per_visit).long()\n",
    "#     batch_conditions = torch.zeros(batch_size, max_num_visits, max_num_diagnosis_codes_per_visit).long()\n",
    "#     batch_intervals = torch.zeros(batch_size, max_num_visits).long()\n",
    "#     batch_labels = torch.zeros(batch_size).long()\n",
    "#\n",
    "#     batch_visits_masks = torch.zeros(batch_size, max_num_visits).long()\n",
    "#     batch_procedures_masks = torch.zeros(batch_size, max_num_visits, max_num_procedure_codes_per_visit).long()\n",
    "#     batch_conditions_masks = torch.zeros(batch_size, max_num_visits, max_num_diagnosis_codes_per_visit).long()\n",
    "#\n",
    "#     for i_patient, patient in enumerate(batch):\n",
    "#         # Collate diagnosis and procedure codes\n",
    "#         collate_codes(batch_procedures, i_patient, patient, \"procedures\", proc_code_to_index_map, batch_procedures_masks)\n",
    "#         collate_codes(batch_conditions, i_patient, patient, \"conditions\", diag_code_to_index_map, batch_conditions_masks)\n",
    "#\n",
    "#         # Get the number of visits this patient has\n",
    "#         visit_intervals = flatten(patient[\"intervals\"])\n",
    "#         num_visits = len(visit_intervals)\n",
    "#\n",
    "#         # Set the visits mask for the patient\n",
    "#         batch_visits_masks[i_patient][:num_visits] = 1\n",
    "#\n",
    "#         # Collate the visit intervals by setting the onehot encoding\n",
    "#         batch_intervals[i_patient][:num_visits] = 1\n",
    "#\n",
    "#         # Set the label\n",
    "#         batch_labels[i_patient] = patient[\"label\"]\n",
    "#\n",
    "#     return (\n",
    "#         batch_procedures,\n",
    "#         batch_conditions,\n",
    "#         batch_intervals,\n",
    "#         batch_visits_masks,\n",
    "#         batch_procedures_masks,\n",
    "#         batch_conditions_masks,\n",
    "#         batch_labels\n",
    "#     )\n",
    "#\n",
    "# BATCH_SIZE = 2\n",
    "# train, val, test = split_by_patient(mimic3_dxtx, [0.8, 0.1, 0.1])\n",
    "#\n",
    "# # obtain train/val/test dataloader, they are <torch.data.DataLoader> object\n",
    "# train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "# val_loader = DataLoader(val, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "# test_loader = DataLoader(test, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "#\n",
    "# _ = next(iter(train_loader))\n",
    "# print(_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "def collate_codes(batch_codes, i_patient, visits, code_to_index_map, mask_tensor: Optional[torch.Tensor] = None):\n",
    "    for i_visit, visit_codes in enumerate(visits):\n",
    "        for i_code, code in enumerate(visit_codes):\n",
    "            batch_codes[i_patient][i_visit][i_code] = code_to_index_map[code]\n",
    "\n",
    "        if mask_tensor is not None:\n",
    "            # Set the mask for the visit codes\n",
    "            num_codes = len(visit_codes)\n",
    "            mask_tensor[i_patient][i_visit][:num_codes] = 1\n",
    "\n",
    "def collate_fn(conditions, procedures, intervals):\n",
    "    batch_size = len(conditions)\n",
    "\n",
    "    max_num_visits = 0\n",
    "    max_num_diagnosis_codes_per_visit = 0\n",
    "    max_num_procedure_codes_per_visit = 0\n",
    "\n",
    "    for i_patient in range(batch_size):\n",
    "        patient_conditions = conditions[i_patient]\n",
    "        patient_procedures = procedures[i_patient]\n",
    "        patient_num_visits = len(patient_conditions)\n",
    "        if patient_num_visits > max_num_visits:\n",
    "            max_num_visits = patient_num_visits\n",
    "        for visit_conditions in patient_conditions:\n",
    "            num_visit_diagnosis_codes = len(visit_conditions)\n",
    "            if num_visit_diagnosis_codes > max_num_diagnosis_codes_per_visit:\n",
    "                max_num_diagnosis_codes_per_visit = num_visit_diagnosis_codes\n",
    "        for visit_procedures in patient_procedures:\n",
    "            num_visit_procedure_codes = len(visit_procedures)\n",
    "            if num_visit_procedure_codes > max_num_procedure_codes_per_visit:\n",
    "                max_num_procedure_codes_per_visit = num_visit_procedure_codes\n",
    "\n",
    "    batch_procedures = torch.zeros(batch_size, max_num_visits, max_num_procedure_codes_per_visit).long()\n",
    "    batch_conditions = torch.zeros(batch_size, max_num_visits, max_num_diagnosis_codes_per_visit).long()\n",
    "    batch_intervals = torch.zeros(batch_size, max_num_visits).long()\n",
    "\n",
    "    batch_visits_masks = torch.zeros(batch_size, max_num_visits).long()\n",
    "    batch_procedures_masks = torch.zeros(batch_size, max_num_visits, max_num_procedure_codes_per_visit).long()\n",
    "    batch_conditions_masks = torch.zeros(batch_size, max_num_visits, max_num_diagnosis_codes_per_visit).long()\n",
    "\n",
    "    for i_patient in range(batch_size):\n",
    "        # Collate diagnosis and procedure codes\n",
    "        collate_codes(batch_procedures, i_patient, procedures[i_patient], proc_code_to_index_map, batch_procedures_masks)\n",
    "        collate_codes(batch_conditions, i_patient, conditions[i_patient], diag_code_to_index_map, batch_conditions_masks)\n",
    "\n",
    "        for i_interval, interval in enumerate(intervals[i_patient]):\n",
    "            interval = interval[0]\n",
    "            batch_intervals[i_patient][i_interval] = visit_interval_to_index_map[interval]\n",
    "            batch_visits_masks[i_patient][i_interval] = 1\n",
    "\n",
    "    return (\n",
    "        batch_procedures,\n",
    "        batch_conditions,\n",
    "        batch_intervals,\n",
    "        batch_visits_masks,\n",
    "        batch_procedures_masks,\n",
    "        batch_conditions_masks\n",
    "    )\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train, val, test = split_by_patient(mimic3_dxtx, [0.8, 0.1, 0.1])\n",
    "\n",
    "train_loader = get_dataloader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = get_dataloader(val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = get_dataloader(test, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T17:41:49.672882Z",
     "end_time": "2023-04-20T17:41:49.686348Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T18:03:24.894802Z",
     "end_time": "2023-04-20T18:03:25.001643Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'loss': tensor(0.7162, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n 'y_prob': tensor([[0.5215],\n         [0.5220],\n         [0.5217],\n         [0.5220],\n         [0.5223],\n         [0.5224],\n         [0.5217],\n         [0.5221],\n         [0.5216],\n         [0.5225],\n         [0.5225],\n         [0.5217],\n         [0.5221],\n         [0.5217],\n         [0.5214],\n         [0.5221],\n         [0.5213],\n         [0.5218],\n         [0.5215],\n         [0.5217],\n         [0.5221],\n         [0.5216],\n         [0.5218],\n         [0.5222],\n         [0.5219],\n         [0.5219],\n         [0.5217],\n         [0.5222],\n         [0.5227],\n         [0.5223],\n         [0.5220],\n         [0.5222]], grad_fn=<SigmoidBackward0>),\n 'y_true': tensor([[0.],\n         [0.],\n         [1.],\n         [1.],\n         [0.],\n         [1.],\n         [0.],\n         [0.],\n         [0.],\n         [0.],\n         [1.],\n         [0.],\n         [0.],\n         [1.],\n         [1.],\n         [0.],\n         [1.],\n         [0.],\n         [0.],\n         [0.],\n         [0.],\n         [0.],\n         [1.],\n         [0.],\n         [0.],\n         [0.],\n         [0.],\n         [0.],\n         [0.],\n         [0.],\n         [0.],\n         [0.]])}"
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the models\n",
    "VERY_BIG_NUMBER = 1e30\n",
    "VERY_SMALL_NUMBER = 1e-30\n",
    "VERY_POSITIVE_NUMBER = VERY_BIG_NUMBER\n",
    "VERY_NEGATIVE_NUMBER = -VERY_BIG_NUMBER\n",
    "\n",
    "class MaskDirection(Enum):\n",
    "    FORWARD = 'forward'\n",
    "    BACKWARD = 'backward'\n",
    "    DIAGONAL = 'diagonal'\n",
    "    NONE = 'none'\n",
    "\n",
    "# class MaskedLayerNorm(nn.Module):\n",
    "#     def __init__(self, normalized_shape: int):\n",
    "#         super().__init__()\n",
    "#         self.gamma = nn.parameter.Parameter(torch.randn(normalized_shape))\n",
    "#         self.beta = nn.parameter.Parameter(torch.randn(normalized_shape))\n",
    "#         self.eps = 1e-5\n",
    "#         self.normalized_shape = normalized_shape\n",
    "#\n",
    "#     def forward(self, x: torch.Tensor, key_padding_mask: Optional[torch.Tensor] = None):\n",
    "#         print(x.shape)\n",
    "#         print(key_padding_mask.shape)\n",
    "#         n = torch.sum(torch.ones_like(x) * (~key_padding_mask).int().unsqueeze(-1).expand(-1, -1, self.normalized_shape), dim=-1)\n",
    "#         print(n.shape)\n",
    "#         print(n)\n",
    "#\n",
    "#         # expected_val = torch.nanmean(x, dim=1)\n",
    "#         # variance = torch.nansum(())\n",
    "\n",
    "def flatten_keep(x: torch.Tensor, keep: int):\n",
    "    fixed_shape = list(x.size())\n",
    "    start = len(fixed_shape) - keep\n",
    "    left = reduce(mul, [fixed_shape[i] or x.shape[i] for i in range(start)])\n",
    "    out_shape = [left] + [fixed_shape[i] or x.shape[i] for i in range(start, len(fixed_shape))]\n",
    "    return torch.reshape(x, out_shape)\n",
    "\n",
    "\n",
    "def reshape(v, ref, embedding_dim):\n",
    "    batch_size = ref.shape[0]\n",
    "    n_visits = ref.shape[1]\n",
    "    out = torch.reshape(v, [batch_size, n_visits, embedding_dim])\n",
    "    return out\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        means = torch.full(\n",
    "            size=(vocab_size, embedding_dim),\n",
    "            fill_value=0.0\n",
    "        )\n",
    "        stds = torch.full(\n",
    "            size=(vocab_size, embedding_dim),\n",
    "            fill_value=embedding_dim ** -0.5\n",
    "        )\n",
    "        self.weights = nn.Parameter(torch.normal(mean=means, std=stds))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask):\n",
    "        embeddings = self.weights[x.reshape(-1)]\n",
    "        embeddings = embeddings.reshape(list(x.shape) + [self.embedding_dim])\n",
    "        embeddings *= torch.unsqueeze(mask, -1)\n",
    "\n",
    "        # Scale embedding by the sqrt of the hidden size\n",
    "        embeddings *= self.embedding_dim ** 0.5\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, embedding_size: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_size, embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_size, embedding_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.fc(input)\n",
    "        # x = torch.nan_to_num(x, nan=VERY_NEGATIVE_NUMBER)\n",
    "        soft = F.softmax(x, dim=1) # Todo: fixme\n",
    "        attn_output = torch.nansum(soft * input, 1)\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "# todo: add temporal encoding\n",
    "# todo: implement layer normalization\n",
    "class MaskEnc(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dim: int,\n",
    "            num_heads: int,\n",
    "            dropout: float = 0.1,\n",
    "            batch_first: bool = True,\n",
    "            temporal_mask_direction: MaskDirection = MaskDirection.NONE,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.temporal_mask_direction = temporal_mask_direction\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "                embed_dim=embedding_dim,\n",
    "                num_heads=num_heads,\n",
    "                dropout=dropout,\n",
    "                batch_first=batch_first\n",
    "            )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embedding_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "        # self.masked_layer_norm1 = MaskedLayerNorm(embedding_dim)\n",
    "        # self.masked_layer_norm2 = MaskedLayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x: torch.Tensor,\n",
    "            key_padding_mask: Optional[torch.Tensor] = None\n",
    "    ):\n",
    "        attn_mask = self._make_temporal_mask(x.shape[1])\n",
    "\n",
    "        attn_output, attn_output_weights = self.attention(x, x, x, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        out = self.fc(attn_output)\n",
    "\n",
    "        # x = self.masked_layer_norm1(x + attn_output, key_padding_mask)\n",
    "        # x = self.masked_layer_norm2(x + self.fc(x), key_padding_mask)\n",
    "        return out\n",
    "\n",
    "    def _make_temporal_mask(self, n: int) -> Optional[torch.Tensor]:\n",
    "        if self.temporal_mask_direction == MaskDirection.NONE:\n",
    "            return None\n",
    "\n",
    "        if self.temporal_mask_direction == MaskDirection.FORWARD:\n",
    "            return torch.tril(torch.ones(n,n))\n",
    "        if self.temporal_mask_direction == MaskDirection.BACKWARD:\n",
    "            return torch.triu(torch.ones(n,n))\n",
    "        if self.temporal_mask_direction == MaskDirection.DIAGONAL:\n",
    "            return torch.zeros(n,n).fill_diagonal_(1)\n",
    "\n",
    "\n",
    "class BiteNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dim: int = 128,\n",
    "            output_dim: int = 1,\n",
    "            num_heads: int = 4,\n",
    "            dropout: float = 0.1,\n",
    "            batch_first: bool = True,\n",
    "            n_mask_enc_layers: int = 2,\n",
    "            use_procedures: bool = True,\n",
    "            use_intervals: bool = True,\n",
    "            num_diag_codes: int = num_unique_diag_codes,\n",
    "            num_proc_codes: int = num_unique_proc_codes,\n",
    "            num_intervals: int = num_unique_intervals,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_intervals = use_intervals\n",
    "        self.use_procedures = use_procedures\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.diag_emb = Embedding(num_diag_codes, embedding_dim)\n",
    "        self.proc_emb = Embedding(num_proc_codes, embedding_dim)\n",
    "        self.interval_emb = Embedding(num_intervals, embedding_dim)\n",
    "\n",
    "        def _make_mask_enc_block(temporal_mask_direction: MaskDirection = MaskDirection.NONE):\n",
    "            return MaskEnc(\n",
    "                embedding_dim = embedding_dim,\n",
    "                num_heads = num_heads,\n",
    "                dropout = dropout,\n",
    "                batch_first = batch_first,\n",
    "                temporal_mask_direction = temporal_mask_direction,\n",
    "            )\n",
    "\n",
    "        self.code_level_attn = nn.ModuleList()\n",
    "        self.visit_level_attn_forward = nn.ModuleList()\n",
    "        self.visit_level_attn_backward = nn.ModuleList()\n",
    "        for _ in range(n_mask_enc_layers):\n",
    "            self.code_level_attn.append(_make_mask_enc_block(MaskDirection.DIAGONAL))\n",
    "            self.visit_level_attn_forward.append(_make_mask_enc_block(MaskDirection.FORWARD))\n",
    "            self.visit_level_attn_backward.append(_make_mask_enc_block(MaskDirection.BACKWARD))\n",
    "        self.code_level_attn.append(AttentionPooling(embedding_dim))\n",
    "        self.visit_level_attn_forward.append(AttentionPooling(embedding_dim))\n",
    "        self.visit_level_attn_backward.append(AttentionPooling(embedding_dim))\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2*embedding_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            procedures: torch.Tensor,\n",
    "            conditions: torch.Tensor,\n",
    "            intervals: torch.Tensor,\n",
    "            visits_mask: torch.Tensor,\n",
    "            procedures_mask: torch.Tensor,\n",
    "            conditions_mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        embedded_conditions = self.diag_emb(conditions, conditions_mask)\n",
    "\n",
    "        if self.use_procedures:\n",
    "            procedures_emb = self.proc_emb(procedures, procedures_mask)\n",
    "            embedded_codes = torch.cat([embedded_conditions, procedures_emb], dim=2)\n",
    "            codes_mask = torch.cat([conditions_mask, procedures_mask], dim=-1)\n",
    "        else:\n",
    "            embedded_codes = embedded_conditions\n",
    "            codes_mask = conditions_mask\n",
    "\n",
    "        codes_mask = ~(codes_mask.bool())\n",
    "\n",
    "        # input tensor, reshape 4 dimension to 3\n",
    "        flattened_codes = flatten_keep(embedded_codes, 2)\n",
    "\n",
    "        # input mask, reshape 3 dimension to 2\n",
    "        flattened_codes_mask = flatten_keep(codes_mask, 1)\n",
    "\n",
    "        code_attn = flattened_codes\n",
    "        for i, l in enumerate(self.code_level_attn):\n",
    "            if i == len(self.code_level_attn) - 1:\n",
    "                code_attn = l(code_attn)\n",
    "            else:\n",
    "                code_attn = l(code_attn, flattened_codes_mask)\n",
    "        code_attn = reshape(code_attn, embedded_codes, self.embedding_dim)\n",
    "\n",
    "        if self.use_intervals:\n",
    "            embedded_intervals = self.interval_emb(intervals, visits_mask)\n",
    "            code_attn += embedded_intervals\n",
    "\n",
    "        visits_mask = ~(visits_mask.bool())\n",
    "\n",
    "        u_fw = code_attn\n",
    "        for i, l in enumerate(self.visit_level_attn_forward):\n",
    "            if i == len(self.visit_level_attn_forward) - 1:\n",
    "                u_fw = l(u_fw)\n",
    "            else:\n",
    "                u_fw = l(u_fw, visits_mask)\n",
    "\n",
    "        u_bw = code_attn\n",
    "        for i, l in enumerate(self.visit_level_attn_backward):\n",
    "            if i == len(self.visit_level_attn_backward) - 1:\n",
    "                u_bw = l(u_bw)\n",
    "            else:\n",
    "                u_bw = l(u_bw, visits_mask)\n",
    "\n",
    "        u_bi = torch.cat([u_fw, u_bw], dim=-1)\n",
    "        s = self.fc(u_bi)\n",
    "        return s\n",
    "\n",
    "class PyHealthBiteNet(BaseModel):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset: SampleDataset,\n",
    "            feature_keys: List[str],\n",
    "            label_key: str,\n",
    "            mode: str,\n",
    "            embedding_dim: int = 128,\n",
    "            n_mask_enc_layers: int = 2,\n",
    "            use_intervals: bool = True,\n",
    "            use_procedures: bool = True,\n",
    "            num_heads: int = 4,\n",
    "            dropout: float = 0.1,\n",
    "            batch_first: bool = True,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__(dataset, feature_keys, label_key, mode)\n",
    "\n",
    "        # Any BaseModel should have these attributes, as functions like add_feature_transform_layer uses them\n",
    "        self.feat_tokenizers = {}\n",
    "        self.embeddings = nn.ModuleDict()\n",
    "        self.linear_layers = nn.ModuleDict()\n",
    "        self.label_tokenizer = self.get_label_tokenizer()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # self.add_feature_transform_layer will create a transformation layer for each feature\n",
    "        for feature_key in self.feature_keys:\n",
    "            input_info = self.dataset.input_info[feature_key]\n",
    "            self.add_feature_transform_layer(\n",
    "                feature_key, input_info\n",
    "            )\n",
    "\n",
    "        # final output layer\n",
    "        output_size = self.get_output_size(self.label_tokenizer)\n",
    "        self.bite_net = BiteNet(\n",
    "            embedding_dim = embedding_dim,\n",
    "            output_dim = output_size,\n",
    "            num_heads = num_heads,\n",
    "            dropout = dropout,\n",
    "            batch_first = batch_first,\n",
    "            use_intervals=use_intervals,\n",
    "            use_procedures=use_procedures\n",
    "        )\n",
    "\n",
    "        # self.fc = nn.Linear(len(self.feature_keys) * hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, **kwargs) -> Dict[str, torch.Tensor]:\n",
    "        conditions = kwargs['conditions']\n",
    "        procedures = kwargs['procedures']\n",
    "        intervals = kwargs['intervals']\n",
    "\n",
    "        input_procedures, input_conditions, input_intervals, visits_mask, procedures_mask, conditions_mask = \\\n",
    "            collate_fn(conditions, procedures, intervals)\n",
    "\n",
    "        logits = self.bite_net(\n",
    "            input_procedures,\n",
    "            input_conditions,\n",
    "            input_intervals,\n",
    "            visits_mask,\n",
    "            procedures_mask,\n",
    "            conditions_mask\n",
    "        )\n",
    "\n",
    "        # obtain y_true, loss, y_prob\n",
    "        y_true = self.prepare_labels(kwargs[self.label_key], self.label_tokenizer)\n",
    "        loss = self.get_loss_function()(logits, y_true)\n",
    "        y_prob = self.prepare_y_prob(logits)\n",
    "        return {\"loss\": loss, \"y_prob\": y_prob, \"y_true\": y_true}\n",
    "\n",
    "model_dxtx = PyHealthBiteNet(\n",
    "    dataset = mimic3_dxtx,\n",
    "    feature_keys = ['procedures', 'conditions', 'intervals'],\n",
    "    label_key = \"label\",\n",
    "    mode = \"binary\",\n",
    "    embedding_dim=128\n",
    ")\n",
    "\n",
    "data = next(iter(train_loader))\n",
    "model_dxtx(**data)\n",
    "\n",
    "# model = BiteNet(\n",
    "#     embedding_dim = 4,\n",
    "#     output_dim = 1,\n",
    "#     num_heads = 4,\n",
    "#     dropout = 0\n",
    "# )\n",
    "# procedures, conditions, intervals, visits_masks, procedures_masks, conditions_masks, labels = next(iter(train_loader))\n",
    "# model(procedures, conditions, intervals, visits_masks, procedures_masks, conditions_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T18:03:36.382318Z",
     "end_time": "2023-04-20T18:03:36.452533Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dxtx = PyHealthBiteNet(\n",
    "    dataset = mimic3_dxtx,\n",
    "    feature_keys = [\"conditions\", \"procedures\"],\n",
    "    label_key = \"label\",\n",
    "    mode = \"binary\",\n",
    ")\n",
    "\n",
    "model_dx = PyHealthBiteNet(\n",
    "    dataset = mimic3_dxtx,\n",
    "    feature_keys = [\"conditions\"],\n",
    "    label_key = \"label\",\n",
    "    mode = \"binary\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T18:03:40.744817Z",
     "end_time": "2023-04-20T18:06:04.823035Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyHealthBiteNet(\n",
      "  (embeddings): ModuleDict(\n",
      "    (conditions): Embedding(3383, 128, padding_idx=0)\n",
      "    (procedures): Embedding(1366, 128, padding_idx=0)\n",
      "  )\n",
      "  (linear_layers): ModuleDict()\n",
      "  (bite_net): BiteNet(\n",
      "    (diag_emb): Embedding()\n",
      "    (proc_emb): Embedding()\n",
      "    (interval_emb): Embedding()\n",
      "    (code_level_attn): ModuleList(\n",
      "      (0-1): 2 x MaskEnc(\n",
      "        (attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (2): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_level_attn_forward): ModuleList(\n",
      "      (0-1): 2 x MaskEnc(\n",
      "        (attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (2): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (visit_level_attn_backward): ModuleList(\n",
      "      (0-1): 2 x MaskEnc(\n",
      "        (attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (2): AttentionPooling(\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Metrics: None\n",
      "Device: cpu\n",
      "\n",
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x2adabd640>\n",
      "Monitor: pr_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 5\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 0 / 5:   0%|          | 0/175 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c13c501cdbbf4ecd97fb4df242812071"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-175 ---\n",
      "loss: 0.5194\n",
      "Evaluation:   0%|          | 0/22 [00:00<?, ?it/s]/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Evaluation: 100%|██████████| 22/22 [00:00<00:00, 30.41it/s]\n",
      "--- Eval epoch-0, step-175 ---\n",
      "pr_auc: 0.1894\n",
      "roc_auc: 0.5000\n",
      "f1: 0.0000\n",
      "loss: 0.6501\n",
      "New best pr_auc score (0.1894) at epoch-0, step-175\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 1 / 5:   0%|          | 0/175 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad275f7c0c674e31bbf9449cb16f1fc1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "--- Train epoch-1, step-350 ---\n",
      "loss: 0.5139\n",
      "Evaluation:   0%|          | 0/22 [00:00<?, ?it/s]/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Evaluation: 100%|██████████| 22/22 [00:00<00:00, 30.53it/s]\n",
      "--- Eval epoch-1, step-350 ---\n",
      "pr_auc: 0.1894\n",
      "roc_auc: 0.5000\n",
      "f1: 0.0000\n",
      "loss: 0.6390\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 2 / 5:   0%|          | 0/175 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f79c3862cdf4f1881a9410b036374ac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "--- Train epoch-2, step-525 ---\n",
      "loss: 0.4871\n",
      "Evaluation:   0%|          | 0/22 [00:00<?, ?it/s]/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Evaluation: 100%|██████████| 22/22 [00:00<00:00, 29.92it/s]\n",
      "--- Eval epoch-2, step-525 ---\n",
      "pr_auc: 0.1894\n",
      "roc_auc: 0.5000\n",
      "f1: 0.0000\n",
      "loss: 0.6277\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 3 / 5:   0%|          | 0/175 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e8938a5f1e674e209e137bdac2b271b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "--- Train epoch-3, step-700 ---\n",
      "loss: 0.4586\n",
      "Evaluation:   0%|          | 0/22 [00:00<?, ?it/s]/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Evaluation: 100%|██████████| 22/22 [00:00<00:00, 29.97it/s]\n",
      "--- Eval epoch-3, step-700 ---\n",
      "pr_auc: 0.1894\n",
      "roc_auc: 0.5000\n",
      "f1: 0.0000\n",
      "loss: 0.6217\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch 4 / 5:   0%|          | 0/175 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d392e4ddb874a6d82c37b693f7c5570"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "--- Train epoch-4, step-875 ---\n",
      "loss: 0.4457\n",
      "Evaluation:   0%|          | 0/22 [00:00<?, ?it/s]/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Evaluation: 100%|██████████| 22/22 [00:00<00:00, 29.85it/s]\n",
      "--- Eval epoch-4, step-875 ---\n",
      "pr_auc: 0.1894\n",
      "roc_auc: 0.5000\n",
      "f1: 0.0000\n",
      "loss: 0.6183\n",
      "Loaded best model\n"
     ]
    }
   ],
   "source": [
    "trainer_dxtx = Trainer(model=model_dxtx)\n",
    "trainer_dxtx.train(\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    epochs=5,\n",
    "    monitor=\"pr_auc\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T18:06:09.547391Z",
     "end_time": "2023-04-20T18:06:11.127244Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation:   0%|          | 0/22 [00:00<?, ?it/s]/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Evaluation: 100%|██████████| 22/22 [00:00<00:00, 27.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pr_auc': 0.20373027259684362, 'roc_auc': 0.5, 'f1': 0.0, 'loss': 0.6519408307292245}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation:   0%|          | 0/22 [00:00<?, ?it/s]/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/Users/cyg1122/PycharmProjects/dl4h-final-project/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Evaluation: 100%|██████████| 22/22 [00:00<00:00, 28.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'pr_auc': 0.20373027259684362}"
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# option 1: use our built-in evaluation metric\n",
    "score_dxtx = trainer_dxtx.evaluate(test_loader)\n",
    "print (score_dxtx)\n",
    "\n",
    "# option 2: use our pyhealth.metrics to evaluate\n",
    "y_true_dxtx, y_prob_dxtx, loss_dxtx = trainer_dxtx.inference(test_loader)\n",
    "binary_metrics_fn(y_true_dxtx, y_prob_dxtx, metrics=[\"pr_auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
